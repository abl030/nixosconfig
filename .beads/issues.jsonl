{"id":"nixosconfig-0qb","title":"Migrate domain-monitor to mkService pattern","description":"Migrate domain-monitor stack from custom service definition to use mkService abstraction.\n\nCurrent custom behaviors to preserve:\n- Build context copying to /tmp\n- Docker build integration (--build flag)\n- Separate cron service/timer for domain checking\n- Custom directory structure\n\nThis should be done AFTER Phase 2 is stable on both hosts.\n\nFile: stacks/domain-monitor/docker-compose.nix\n\nBlocked by: Phase 2 completion and stabilization","status":"open","priority":3,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-12T21:34:34.409771593+08:00","created_by":"abl030","updated_at":"2026-02-12T21:34:34.409771593+08:00","dependencies":[{"issue_id":"nixosconfig-0qb","depends_on_id":"nixosconfig-5dy","type":"blocks","created_at":"2026-02-12T21:35:12.710346383+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-0sb","title":"Phase 2: Episodic Memory (conversation archive)","description":"Deploy obra/episodic-memory for cross-session conversation recall. Archives Claude Code conversation JSONL, indexes with local vector embeddings, exposes semantic search via MCP. BLOCKED by upstream bugs: #47 (MCP stdout corruption), #53 (orphaned processes). See docs/agentic-memory-options-comparison.md.","notes":"2026-02-11: PAUSED. Episodic-memory plugin disabled in base.nix. npm deps require manual install which breaks Nix store, and no credits to justify the plumbing. Flake input kept. Re-evaluate when upstream plugin dep management improves (anthropics/claude-code#13505) or we have credits.","status":"closed","priority":2,"issue_type":"epic","owner":"abl030@gmail.com","created_at":"2026-02-11T12:21:15.426852585+08:00","created_by":"abl030","updated_at":"2026-02-22T14:25:49.039722665+08:00","closed_at":"2026-02-11T20:42:44.643062411+08:00","dependencies":[{"issue_id":"nixosconfig-0sb","depends_on_id":"nixosconfig-sep","type":"parent-child","created_at":"2026-02-11T12:26:41.829428299+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-0sb.1","title":"Research episodic memory options (3+ candidates)","description":"Compare at least 3 episodic memory solutions for Claude Code. Produce docs/episodic-memory-comparison.md with architecture, pros/cons, NixOS packaging difficulty, and recommendation. Converge on one to implement.","status":"closed","priority":1,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T12:39:46.074521081+08:00","created_by":"abl030","updated_at":"2026-02-11T15:35:25.063536442+08:00","closed_at":"2026-02-11T15:32:01.427453677+08:00","dependencies":[{"issue_id":"nixosconfig-0sb.1","depends_on_id":"nixosconfig-0sb","type":"parent-child","created_at":"2026-02-11T12:39:46.075245663+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-0sb.2","title":"Merge fork back to upstream episodic-memory when PRs land","description":"We forked obra/episodic-memory to abl030/episodic-memory and merged PRs #56 (stdout fix, orphan fix, similarity score fix, cross-platform build) and #51 (clear session matcher). Periodically check if upstream merges these PRs. When they do, switch back to upstream and archive the fork. Check: gh pr view 56 --repo obra/episodic-memory --json state \u0026\u0026 gh pr view 51 --repo obra/episodic-memory --json state","status":"closed","priority":4,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T12:57:04.352620135+08:00","created_by":"abl030","updated_at":"2026-02-22T14:25:49.040533097+08:00","closed_at":"2026-02-11T20:42:43.514027419+08:00","dependencies":[{"issue_id":"nixosconfig-0sb.2","depends_on_id":"nixosconfig-0sb","type":"parent-child","created_at":"2026-02-11T12:57:04.353581767+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-0yk","title":"Research agentic memory landscape","description":"Evaluate beads, claude-mem, episodic-memory. Document findings in docs/agentic-memory-landscape.md and docs/agentic-memory-options-comparison.md.","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T12:20:36.085183334+08:00","created_by":"abl030","updated_at":"2026-02-11T12:21:01.801032293+08:00","closed_at":"2026-02-11T12:21:01.801032293+08:00","close_reason":"Deployed and verified on WSL. See docs/beads-rollout-plan.md.","dependencies":[{"issue_id":"nixosconfig-0yk","depends_on_id":"nixosconfig-af3","type":"parent-child","created_at":"2026-02-11T12:20:52.291636815+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-25q","title":"Migrate beads overlay to flake input","description":"Upstream flake (github:steveyegge/beads) doesn't build due to Go \u003e= 1.25.6 requirement. Check periodically: nix build github:steveyegge/beads#default --no-link. When it succeeds, add as flake input and remove fetchurl from overlay.","notes":"Backlog: check periodically with 'nix build github:steveyegge/beads#default --no-link'. Current fetchurl overlay works fine.","status":"closed","priority":4,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T12:21:04.065954008+08:00","created_by":"abl030","updated_at":"2026-02-22T14:25:49.041233371+08:00","closed_at":"2026-02-11T20:42:44.656771142+08:00","dependencies":[{"issue_id":"nixosconfig-25q","depends_on_id":"nixosconfig-sep","type":"parent-child","created_at":"2026-02-11T12:30:35.928631111+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-2ie","title":"Reference: Debug Session Notes \u0026 Fast Debug Checklist","description":"Reference documentation for debug sessions and the doc1 fast debug checklist.\n\n## Debug Session Notes\n\n- Record: host, stack, URL, expected status, current status, last change.\n- Verify upstream directly with `--resolve` before changing nginx/Cloudflare.\n- For Uptime Kuma issues: check `/metrics` and confirm `monitor_status` before assuming DNS or proxy issues.\n\n### Fast Debug Checklist (Doc1)\n\n- `systemctl --user list-units 'podman-compose@*' --no-legend`\n- `podman ps --format 'table {{.Names}}\\t{{.Status}}'`\n- `curl -k --resolve \u003chost\u003e:443:\u003cip\u003e https://\u003chost\u003e/\u003chealth\u003e`\n- `key=$(rg -m1 '^KUMA_API_KEY=' ${KUMA_API_KEY_FILE:-/run/secrets/uptime-kuma/api} | cut -d= -f2-); curl -fsS --user \":$key\" https://status.ablz.au/metrics | rg 'monitor_status' | rg '\u003cdomain\u003e'`","status":"closed","priority":4,"issue_type":"chore","created_at":"2026-02-11T20:35:34.184065622+08:00","updated_at":"2026-02-11T20:36:03.488456951+08:00","closed_at":"2026-02-11T20:36:03.488456951+08:00","close_reason":"Reference doc — content moved from CLAUDE.md","labels":["debugging","doc1","reference"]}
{"id":"nixosconfig-2o9","title":"Podman Rootless Operations Tracker","description":"Track current rootless Podman compose operating model, accepted residual risks, and future migration options (Quadlet and native Nix orchestration).","notes":"2026-02-24 incident: music stack went down after a compose file change (slskd/soularr commented out) triggered a service restart. compose stop left containers in Exited state; compose up then failed because the old network-holder pod still had dependent containers registered against it, so docker-compose couldn't recreate the pod. The ExecStartPre recreate-if-label-mismatch script only removed containers with a PODMAN_SYSTEMD_UNIT label mismatch — correctly-labelled Exited containers were untouched. Fix (untested): extended the script to also rm -f --depend all exited/created/dead containers for the project before compose up, giving it a clean slate. All persistent state is in volumes so this is safe.","status":"open","priority":2,"issue_type":"epic","owner":"abl030@gmail.com","created_at":"2026-02-15T10:34:50.767136377+08:00","created_by":"abl030","updated_at":"2026-02-24T06:57:10.551139559+08:00","labels":["homelab","operations","podman"]}
{"id":"nixosconfig-2o9.1","title":"Evaluate Quadlet migration path for high-value stacks","description":"Define incremental canary-based migration approach from compose-managed stacks to Quadlet with explicit rollback criteria.","status":"open","priority":3,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T10:34:58.833030756+08:00","created_by":"abl030","updated_at":"2026-02-15T10:34:58.833030756+08:00","labels":["backlog","podman","quadlet"],"dependencies":[{"issue_id":"nixosconfig-2o9.1","depends_on_id":"nixosconfig-2o9","type":"parent-child","created_at":"2026-02-15T10:34:58.835913925+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-2o9.2","title":"Evaluate native Nix container orchestration options","description":"Survey viable Nix-native orchestration patterns for homelab stacks and compare operational tradeoffs against current compose model.","status":"open","priority":3,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T10:34:58.882608616+08:00","created_by":"abl030","updated_at":"2026-02-15T10:34:58.882608616+08:00","labels":["backlog","nix","podman"],"dependencies":[{"issue_id":"nixosconfig-2o9.2","depends_on_id":"nixosconfig-2o9","type":"parent-child","created_at":"2026-02-15T10:34:58.883382454+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-2o9.3","title":"Research: Container lifecycle analysis (podman compose)","description":"# Container Lifecycle Analysis: Rebuild vs Auto-Update\n\n**Research Date:** 2026-02-12\n**Related Beads:** nixosconfig-cm5 (research task), nixosconfig-hbz (stale container bug)\n**Status:** Complete (implemented and hardened through 2026-02-13; ownership follow-up moved to Phase 2.5)\n**Related empirical test:** [2026-02-13-compose-change-propagation-test.md](../incidents/2026-02-13-compose-change-propagation-test.md)\n\n## Planned Trial Direction (2026-02)\n\nThe next iteration will trial a simpler deployment model while preserving health visibility:\n- Keep container health checks active in compose definitions.\n- Remove `--wait` from stack deploy path so host activation is not blocked by runtime health convergence.\n- Retain strict auto-update ownership invariant (`PODMAN_SYSTEMD_UNIT` required when autoupdate is enabled).\n- Push runtime failure handling to user service status and monitoring/alerting, rather than rebuild gating.\n\nTrial outcomes to validate:\n- `nixos-rebuild switch` does not stall on persistent `health=starting` containers.\n- Auto-update behavior and rollback signals remain clear.\n- Preflight script surface can be reduced without losing hard-fail safety where invariants are violated.\n\nImplementation note:\n- Phase 1 is now in place: stack deploy uses `compose up -d --remove-orphans` (no compose `--wait` gating).\n- Phase 2 is complete: user service is the sole lifecycle owner with native `sops.secrets` wiring and compatibility fallback window.\n\n## Post-Implementation Notes (2026-02-13)\n\nThis research is still valid, but the stack implementation has since moved to the simplified Phase 2 model:\n\n1. Startup is bounded to avoid rebuild deadlocks:\n   - user compose unit startup is bounded by `startupTimeoutSeconds` (default 300s / 5m).\n   - `podman compose` deploy path now uses `up -d --remove-orphans` (no `--wait` gating).\n2. Stale-health detection now covers both compose label families:\n   - `io.podman.compose.project`\n   - `com.docker.compose.project`\n3. `StartedAt` parsing was normalized to handle Podman timestamps with zone names (for GNU `date` compatibility).\n4. Label-mismatch handling is intentionally hard-fail via container removal before restart so auto-update and systemd ownership stay consistent.\n5. User compose unit naming is `${stackName}.service`; `PODMAN_SYSTEMD_UNIT` points there.\n6. Legacy `*-stack-secrets.service` orchestration was removed; env files are resolved from native `sops.secrets` paths with one-release fallback support.\n7. Ownership follow-up (Phase 2.5) is now tracked separately:\n   - `docs/podman/decisions/2026-02-13-home-manager-user-unit-ownership.md`\n   - `docs/podman/current/phase2.5-home-manager-migration-plan.md`\n   - `docs/podman/research/home-manager-user-service-migration-research-2026-02.md`\n\n### Empirical Update (2026-02-13, `igpu`)\n\nAn explicit compose-change propagation test is documented in:\n- [2026-02-13-compose-change-propagation-test.md](../incidents/2026-02-13-compose-change-propagation-test.md)\n\nObserved in that test:\n- The rebuilt NixOS generation contained the updated user unit and updated compose wrapper path.\n- The running user manager continued using a stale unit path from `~/.config/systemd/user`, and restart behavior followed that stale unit definition.\n- Once stale home-level unit artifacts were removed, the active unit path switched to `/etc/systemd/user/...` and the updated compose command was applied.\n\n### Incident Confirmation (2026-02-13, AWST)\n\nProduction logs later confirmed the predicted failure mode when label invariants are violated:\n\n- `doc1` (`proxmox-vm`) auto-update run: `52` missing-label errors\n- `igpu` auto-update run: `13` missing-label errors\n- Shared error signature: `no PODMAN_SYSTEMD_UNIT label found`\n\nThis validates the need for strict preflight invariant enforcement:\n\n- `io.containers.autoupdate=registry` must always be paired with `PODMAN_SYSTEMD_UNIT`.\n- Violations should fail stack startup early, instead of failing during timer-driven auto-update.\n\nExecution test matrix is tracked in:\n- `docs/podman/decisions/2026-02-12-container-lifecycle-strategy.md` (`Test Matrix (Invariant Enforcement)`).\n\n## Executive Summary\n\nThis document answers critical questions about how `docker-compose --wait` interacts with container reuse, health checks, and the dual service architecture in our podman-compose-based container stack management system. The research reveals that:\n\n1. **Container reuse with `--wait` DOES cause stale health check issues** - this is a real, ongoing risk\n2. **Podman auto-update operates independently of compose files** - it works directly with containers via podman API\n3. **Control-plane simplification completed in Phase 2** - user service now owns stack lifecycle for both rebuild and auto-update restart targets\n4. **Different scenarios require different strategies** - rebuild should optimize for speed, auto-update should prioritize reliability\n\n## Research Questions Answered\n\n### 1. How `--wait` Works: Container Reuse and Health Checks\n\n**Finding:** Docker Compose's `--wait` flag monitors health status but does NOT trigger fresh health checks on reused containers.\n\n#### Key Behavior\n\nFrom [Docker Compose documentation](https://docs.docker.com/reference/cli/docker/compose/up/):\n\u003e \"Wait for services to be running|healthy. Implies detached mode.\"\n\nWhen `docker-compose up -d --wait` runs:\n\n1. **Container Recreation Decision:** Compose compares the `com.docker.compose.config-hash` label on existing containers against the current compose file configuration ([source](https://deepwiki.com/docker/compose/5.2-config-command))\n2. **If Config Unchanged:** Container is reused (not recreated)\n3. **Wait Behavior:** `--wait` monitors the CURRENT health status of all services\n4. **Critical Issue:** If a reused container has a stuck health check from before, `--wait` waits for that old status to change\n\n#### What Triggers Container Recreation?\n\nFrom [Docker Compose up documentation](https://docs.docker.com/reference/cli/docker/compose/up/):\n\u003e \"If there are existing containers for a service, and the service's configuration or image was changed after the container's creation, docker compose up picks up the changes by stopping and recreating the containers (preserving mounted volumes).\"\n\nConfiguration changes include:\n- Image digest (new image version)\n- Environment variables\n- Port mappings\n- Volume mounts\n- Network configuration\n- Health check definition itself\n\n**Key Finding:** Changing health check parameters in the compose file DOES trigger recreation, but deploying the same config twice (rebuild scenario) reuses containers.\n\n### 2. Stale Health Check Problem: Confirmed Real Risk\n\n**Evidence:** This is a documented issue across the Docker ecosystem.\n\n#### How Containers Get Stuck\n\nFrom [Last9's Docker Status Unhealthy guide](https://last9.io/blog/docker-status-unhealthy-how-to-fix-it/):\n\u003e \"The health status is initially 'starting'. Whenever a health check passes, it becomes 'healthy'. After a certain number of consecutive failures, it becomes 'unhealthy'.\"\n\nFrom [GitHub issue about stuck health checks](https://github.com/caprover/caprover/issues/844):\n\u003e \"Container stuck in status (health: starting)\" - containers can remain perpetually in \"starting\" state if the initial health check fails and subsequent restarts reuse the container.\n\n#### The Deadlock Scenario\n\n1. Container starts, application takes too long to initialize\n2. First health check runs before app is ready → fails\n3. Container shows \"Up X minutes (starting)\" with failed health log entry\n4. Service restarts (nixos-rebuild, manual restart)\n5. Docker-compose reuses existing container (config unchanged)\n6. `--wait` blocks waiting for health to become \"healthy\"\n7. **Health check system doesn't re-run** (only has the old failed attempt)\n8. Deployment hangs indefinitely\n\nFrom [Docker Community Forums](https://forums.docker.com/t/unhealthy-container-does-not-restart/105822):\n\u003e \"If the health check command works manually but fails in the health check, the issue is usually timing (runs too early) or environment (missing variables).\"\n\n#### Why This Happens\n\nThe health check mechanism continues running ONLY if:\n- Container was freshly created with health check config\n- Health check interval is still actively scheduled\n\nWhen a container is reused:\n- Old health check state persists\n- If stuck in \"starting\" with one failed attempt, no new attempts are scheduled\n- Container never transitions to \"healthy\"\n- `--wait` waits forever\n\n### 3. `--force-recreate` Flag: Performance vs Reliability\n\n**Documentation:** From [Docker Compose up command](https://docs.docker.com/reference/cli/docker/compose/up/):\n\u003e \"To prevent Compose from picking up changes, use the --no-recreate flag.\"\n\u003e \"If you want to force Compose to stop and recreate all containers, use the --force-recreate flag.\"\n\n#### What `--force-recreate` Does\n\n- Stops and removes ALL containers in the stack\n- Creates fresh containers even if config hasn't changed\n- Resets all health check state\n- Similar to Watchtower's always-recreate approach\n\n#### Performance Implications\n\n**For `up -d` (smart reuse):**\n- Only recreates changed containers\n- Fast for no-op rebuilds\n- Preserves container state when possible\n- **Risk:** Stale health checks, stuck states\n\n**For `up -d --force-recreate` (always fresh):**\n- Recreates everything every time\n- Slower (stop + remove + create overhead ~2-5 seconds per container)\n- Guaranteed clean slate\n- **Safe:** No stale state possible\n\nFrom community discussions on [force-recreate performance](https://www.howtogeek.com/devops/how-to-make-docker-rebuild-an-image-without-its-cache/):\n\u003e \"Use `--force-recreate` for situations where you specifically need to ensure a complete rebuild regardless of changes.\"\n\n### 4. Podman Auto-Update Mechanics: Independent of Compose\n\n**Critical Finding:** Podman auto-update does NOT use compose files. It operates directly on containers via the Podman API.\n\n#### How Podman Auto-Update Works\n\nFrom [Podman auto-update documentation](https://docs.podman.io/en/latest/markdown/podman-auto-update.1.html):\n\u003e \"To make use of auto updates, the container or Kubernetes workloads must run inside a systemd unit.\"\n\u003e\n\u003e \"After a successful update of an image, the containers using the image get updated by restarting the systemd units they run in.\"\n\nProcess:\n1. Checks registry for image updates (containers with `io.containers.autoupdate=registry` label)\n2. Pulls new images if available\n3. **Restarts the systemd unit** (not the container directly)\n4. Systemd unit recreates containers using the new image\n\n#### The PODMAN_SYSTEMD_UNIT Label\n\nFrom [GitHub issue #534](https://github.com/containers/podman-compose/issues/534):\n\u003e \"At container-creation time, Podman looks up the 'PODMAN_SYSTEMD_UNIT' environment variables and stores it verbatim in the container's label.\"\n\u003e\n\u003e \"A necessary requirement for podman auto-update is that systemd units CREATE the podman container at startup. The systemd units are expected to be generated with podman-generate-systemd --new, or similar units that create new containers in order to run the updated images.\"\n\n**Key Insight:** Auto-update requires systemd units that **create** containers, not just start/stop them. This is why we have user services.\n\n#### Rollback on Failure\n\nFrom [Red Hat's Podman auto-update blog](https://www.redhat.com/en/blog/podman-auto-updates-rollbacks):\n\u003e \"When restarting a systemd unit after updating the image has failed, Podman can rollback to using the previous image and restart the unit. This is enabled by default.\"\n\nFrom [GitHub discussion #16098](https://github.com/containers/podman/discussions/16098):\n\u003e \"With the --sdnotify=container implementation, Podman can support simple rollbacks. Health-checks that fail immediately after start will presumably cause the RestartUnit call to return failed, and therefore have a rollback take effect.\"\n\n**Rollback Detection:** Podman watches for:\n- Systemd unit restart failure\n- Container exits with non-zero code\n- Health check fails immediately after update (when `on-failure` action is set)\n\n### 5. Dual Service Architecture: Purpose and Relationship\n\n**From our implementation** (`stacks/lib/podman-compose.nix` and `modules/nixos/homelab/containers/default.nix`):\n\n#### System Service: `\u003cstack-name\u003e-stack.service`\n\n**Purpose:** NixOS-rebuild integration (apply configuration changes)\n\nLocation: Lines 192-232 in `stacks/lib/podman-compose.nix`\n\nKey characteristics:\n- Type: `oneshot` with `RemainAfterExit=true`\n- Runs as: `${user}` (rootless)\n- Environment: Connects to user's podman socket via `CONTAINER_HOST=unix:///run/user/${userUid}/podman/podman.sock`\n- ExecStart: `docker-compose up -d --wait --remove-orphans`\n- RestartTriggers: Compose file, sops secrets, custom triggers\n- **Used by:** `nixos-rebuild switch` (via systemd activation)\n\nPre-start checks (line 217):\n1. Prune legacy pod_ containers\n2. Recreate containers if `PODMAN_SYSTEMD_UNIT` label mismatches (prevents auto-update confusion)\n3. Decrypt secrets, set permissions\n\n#### User Service: `podman-compose@\u003cprojectName\u003e.service`\n\n**Purpose:** Podman auto-update integration (pull new images, recreate containers)\n\nLocation: Lines 234-247 in `stacks/lib/podman-compose.nix`\n\nKey characteristics:\n- Type: `oneshot` with `RemainAfterExit=true`\n- Runs as: Current user (user service context)\n- Environment: Same as system service, includes `PODMAN_SYSTEMD_UNIT=podman-compose@\u003cprojectName\u003e.service`\n- ExecStart: `docker-compose up -d --wait --remove-orphans`\n- RestartIfChanged: `false` (doesn't restart on config changes)\n- **Used by:** `podman auto-update` command (systemd restarts this unit when images update)\n\n#### The Relationship\n\n```\nnixos-rebuild switch → system service → docker-compose up (smart reuse, fast)\n                          ↓\n                    Sets PODMAN_SYSTEMD_UNIT=podman-compose@X.service on containers\n                          ↓\npodman auto-update → Checks registry for new images\n                          ↓\n                    Finds containers labeled: io.containers.autoupdate=registry\n                                            + PODMAN_SYSTEMD_UNIT=podman-compose@X.service\n                          ↓\n                    Restarts user service → docker-compose up (creates fresh containers with new image)\n```\n\n**Key Insight:** Both services use the same compose file and same flags, but they're triggered by different mechanisms:\n- System service: Triggered by NixOS activation (config changes)\n- User service: Triggered by podman auto-update (image updates)\n\n### 6. Current Implementation Analysis\n\nFrom `modules/nixos/homelab/containers/default.nix` (lines 249-265):\n\n```nix\npodman-auto-update = lib.mkIf cfg.autoUpdate.enable {\n  description = \"Podman auto-update (rootless)\";\n  serviceConfig = {\n    Type = \"oneshot\";\n    ExecStart = [\"\" autoUpdateScript];  # Overrides base service\n    User = user;\n    Environment = [...];\n  };\n};\n```\n\nThe `autoUpdateScript` (lines 33-100):\n1. Runs `podman auto-update` (NOT docker-compose)\n2. Parses output to find updated containers\n3. Waits 30 seconds for containers to settle\n4. Checks for failures:\n   - Container not running (crashed)\n   - Container rolled back (dry-run shows still pending update)\n5. Sends Gotify notification on failure\n\n**Critical Finding:** Our auto-update does NOT call docker-compose directly. It uses `podman auto-update`, which then triggers systemd to restart the user services, which then run docker-compose.\n\n### 7. Container Reuse: Summary\n\n**Intended behavior** (see section 1 for details):\n- Reuse: Config hash matches + same image digest\n- Recreate: Config changes, new image, or `--force-recreate`\n\n**Known issue:** Docker Compose v2 sometimes recreates unnecessarily ([GitHub #9600](https://github.com/docker/compose/issues/9600)), but generally respects config hash comparison.\n\n### 8. Best Practices from the Field\n\n#### Health Check Configuration\n\nFrom [Tom Vaidyan's 2025 guide](https://www.tvaidyan.com/2025/02/13/health-checks-in-docker-compose-a-practical-guide/):\n\nBest practices:\n1. **Always add health checks to databases** - They take time to initialize\n2. **Use start_period** - Give services time to initialize before checking (default 0s is often too aggressive)\n3. **Make health checks lightweight** - They run frequently (default every 30s)\n4. **Check actual readiness, not just process existence**\n\nExample configuration:\n```yaml\nhealthcheck:\n  test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/health\"]\n  interval: 30s\n  timeout: 10s\n  retries: 3\n  start_period: 40s  # Critical for slow-starting apps\n```\n\n#### Watchtower's Pattern\n\nFrom [Watchtower usage overview](https://containrrr.dev/watchtower/usage-overview/):\n\u003e \"If watchtower detects that an image has changed, it will automatically restart the container using the new image.\"\n\u003e\n\u003e \"Watchtower downloads all the latest images locally and gracefully shuts down all the corresponding containers that need the update, then starts them back up using the new Docker base image.\"\n\n**Key insight:** Watchtower ALWAYS recreates containers on update (no reuse logic). This approach:\n- Simpler logic (no convergence complexity)\n- Guaranteed clean state\n- Slightly slower but more reliable\n- Worked reliably for years across thousands of deployments\n\n## Synthesis: Answering the Core Questions\n\n### Q1: Does Container Reuse Cause Stale Health Checks with `--wait`?\n\n**Answer:** YES, definitively.\n\nWhen `docker-compose up -d --wait` reuses an existing container:\n1. The container's health check state persists from before\n2. If that state is stuck (e.g., perpetual \"starting\"), it never changes\n3. `--wait` blocks indefinitely waiting for \"healthy\" status\n4. No new health checks are triggered (old state persists)\n\n**Evidence:**\n- [Docker community forums](https://forums.docker.com/t/unhealthy-container-does-not-restart/105822) document this pattern\n- [GitHub issues](https://github.com/caprover/caprover/issues/844) show containers stuck in \"starting\" state\n- Our own experience (bead nixosconfig-hbz) confirms this happens in production\n\n### Q2: Should We Use Different Flags for Rebuild vs Auto-Update?\n\n**Answer:** NO - the dual service architecture already handles this correctly (see section 5 for details).\n\n**For Rebuild (system service):**\n- Smart reuse via `docker-compose up -d --wait` (fast, incremental)\n- Mitigation: `recreateIfLabelMismatch` + proposed stale health detection\n\n**For Auto-Update (user service):**\n- Full recreation via systemd restart cycle (ExecStop → ExecStart)\n- Built-in rollback protection from podman auto-update\n\n### Q3: What Is the Root Cause of Stale Container Issues?\n\n**Answer:** Container reuse during nixos-rebuild when health checks are in a bad state.\n\n**Scenario:**\n1. Service deployed initially, containers healthy\n2. Something breaks (network issue, dependency failure, resource exhaustion)\n3. Container health check fails, stuck in \"starting\" or \"unhealthy\"\n4. User runs `nixos-rebuild switch` (config unchanged, maybe just updating other hosts)\n5. System service runs: `docker-compose up -d --wait`\n6. Container reused (config hash matches)\n7. `--wait` blocks on stale health check\n8. Deployment hangs\n\n**Current Mitigations:**\n1. `stackCleanup` script (line 40-64) prunes stopped containers after each operation\n2. `recreateIfLabelMismatch` (line 173) removes containers with wrong unit labels\n3. Manual intervention: `podman rm -f \u003cname\u003e` when stuck\n\n**Gap:** No automatic detection of stuck health checks BEFORE attempting reuse.\n\n## Recommendations\n\n### Recommendation 1: Add Stale Health Check Detection (HIGH PRIORITY)\n\n**Problem:** Container reuse can inherit stuck health check state, causing indefinite hangs.\n\n**Solution:** Add pre-start check in system service to detect and remove containers with stale health:\n\n```nix\n# In stacks/lib/podman-compose.nix, add parameter:\nhealthCheckTimeout ? 90  # Default 90 seconds, configurable per-stack\n\n# Add to ExecStartPre:\ndetectStaleHealth = [\n  ''\n    /run/current-system/sw/bin/sh -c '\n      ids=$(${podmanBin} ps -a --filter label=io.podman.compose.project=${projectName} --format \"{{.ID}}\")\n      for id in $ids; do\n        health=$(${podmanBin} inspect -f \"{{.State.Health.Status}}\" $id 2\u003e/dev/null || echo \"none\")\n        started=$(${podmanBin} inspect -f \"{{.State.StartedAt}}\" $id 2\u003e/dev/null)\n\n        # Only remove if unhealthy/starting AND running for \u003ethreshold\n        if [ \"$health\" = \"starting\" ] || [ \"$health\" = \"unhealthy\" ]; then\n          age_seconds=$(( $(date +%s) - $(date -d \"$started\" +%s) ))\n          if [ $age_seconds -gt ${toString healthCheckTimeout} ]; then\n            echo \"Removing container $id with stale health ($health) - running for ${age_seconds}s (threshold: ${toString healthCheckTimeout}s)\"\n            ${podmanBin} rm -f $id\n          else\n            echo \"Container $id is $health but only ${age_seconds}s old - allowing more time (threshold: ${toString healthCheckTimeout}s)\"\n          fi\n        fi\n      done\n    '\n  ''\n];\n```\n\nAdd this to line 217 (before recreateIfLabelMismatch).\n\n**Edge Case Handling:**\n- **Default 90 seconds:** Covers most services (2-3x typical 30-45s startup time)\n- **Rapid rebuilds safe:** Won't get stuck in multi-minute loops during development\n- **Configurable per-stack:** Slow services can override (e.g., `healthCheckTimeout = 300` for database migrations)\n- **Formula:** Set to 2-3x expected startup time for the slowest container in the stack\n\n**Benefits:**\n- Prevents indefinite hangs during rebuild (90s max wait for stuck containers)\n- Safe for rapid rebuilds during development (won't loop for 5+ minutes)\n- Maintains fast reuse for healthy containers\n- Automatic remediation (no manual intervention)\n- Low overhead (quick inspect check)\n- Configurable per-stack for edge cases\n\n**Testing Strategy:** See Testing section below.\n\n### Recommendation 2: Improve Health Check Configuration Guidance (MEDIUM PRIORITY)\n\n**Problem:** Many containers lack proper `start_period` configuration, leading to premature health check failures.\n\n**Solution:** Document health check best practices in stack templates:\n\n```yaml\n# Template in docs/stack-template.yaml\nhealthcheck:\n  test: [\"CMD\", \"curl\", \"-f\", \"http://localhost/health\"]\n  interval: 30s        # How often to check after start_period\n  timeout: 10s         # How long to wait for response\n  retries: 3           # Consecutive failures before \"unhealthy\"\n  start_period: 60s    # Grace period for slow startup (CRITICAL)\n```\n\n**Best Practices to Document:**\n- Always include `start_period` for apps with initialization time\n- Use 2-3x expected startup time for `start_period`\n- Keep interval reasonable (30s default is good)\n- Avoid expensive health checks (they run every interval)\n- Test health check command manually before deploying\n\n### Recommendation 3: Keep Current Dual Service Architecture (NO CHANGE)\n\n**Problem:** Initial concern about dual services being confusing or redundant.\n\n**Finding:** The architecture is actually elegant and serves distinct purposes:\n\n```\nSystem Service (oneshot):\n  - Triggered by: nixos-rebuild switch\n  - Purpose: Apply config changes incrementally\n  - Strategy: Smart reuse (fast, only restart changed containers)\n  - Protection: Stale health detection (recommendation 1)\n\nUser Service (oneshot):\n  - Triggered by: podman auto-update (via systemd restart)\n  - Purpose: Pull new images, deploy updates\n  - Strategy: Full recreation (systemd stop → start cycle)\n  - Protection: Built-in rollback (podman auto-update feature)\n```\n\n**Recommendation:** Keep both services as-is. They already implement the right strategies for their use cases.\n\n### Recommendation 4: Do NOT Add `--force-recreate` to System Service (AVOID)\n\n**Temptation:** Always use `--force-recreate` to avoid stale state issues.\n\n**Why NOT to do this:**\n- Defeats purpose of incremental config changes\n- Unnecessarily restarts ALL containers on every rebuild\n- Slower deployments (2-5s overhead per container)\n- Causes downtime when only secrets/firewall rules changed\n- Loses NixOS activation optimization\n\n**Better approach:** Fix the root cause (recommendation 1) rather than work around it.\n\n### Recommendation 5: Monitor Health Check Performance (LOW PRIORITY)\n\n**Problem:** We don't have visibility into health check timing and failure patterns.\n\n**Solution:** Add logging to track health check state transitions during deployments:\n\n```nix\n# In ExecStartPre, before docker-compose up:\nlogHealthStatus = [\n  \"/run/current-system/sw/bin/sh -c '${podmanBin} ps -a --filter label=io.podman.compose.project=${projectName} --format \\\"{{.Names}}: {{.Status}}\\\" | /run/current-system/sw/bin/tee /tmp/podman-health-pre-${projectName}.log || true'\"\n];\n\n# In ExecStartPost, after docker-compose up:\nlogHealthStatusPost = [\n  \"/run/current-system/sw/bin/sh -c '${podmanBin} ps -a --filter label=io.podman.compose.project=${projectName} --format \\\"{{.Names}}: {{.Status}}\\\" | /run/current-system/sw/bin/tee /tmp/podman-health-post-${projectName}.log || true'\"\n];\n```\n\n**Benefits:**\n- Historical record of health check patterns\n- Can identify containers that frequently get stuck\n- Helps tune `start_period` and `interval` settings\n- Useful for debugging deployment issues\n\n## Testing Strategy\n\n### Testing Recommendation 1 (Stale Health Detection)\n\n**Goal:** Verify the detection script correctly identifies and removes stuck containers without breaking healthy ones.\n\n#### Phase 1: Validation in Dev Environment\n\n1. **Test Setup:**\n   ```bash\n   # On dev VM, create a test stack with intentionally broken health check\n   cat \u003e /tmp/test-health-compose.yml \u003c\u003cEOF\n   services:\n     test-slow:\n       image: nginx:alpine\n       healthcheck:\n         test: [\"CMD\", \"sleep\", \"999\"]  # Always times out\n         interval: 30s\n         timeout: 10s\n         start_period: 10s\n     test-healthy:\n       image: nginx:alpine\n       healthcheck:\n         test: [\"CMD\", \"curl\", \"-f\", \"http://localhost\"]\n         interval: 30s\n         timeout: 5s\n   EOF\n\n   podman compose -f /tmp/test-health-compose.yml up -d\n   ```\n\n2. **Wait for stuck state:**\n   ```bash\n   # Wait 2+ minutes for test-slow to get stuck in \"starting\"\n   watch 'podman ps -a --format \"{{.Names}}: {{.Status}}\"'\n   ```\n\n3. **Test detection script:**\n   ```bash\n   # Run the detection logic manually (using 90s threshold)\n   threshold=90\n   ids=$(podman ps -a --filter label=io.podman.compose.project=tmp --format \"{{.ID}}\")\n   for id in $ids; do\n     health=$(podman inspect -f \"{{.State.Health.Status}}\" $id 2\u003e/dev/null || echo \"none\")\n     started=$(podman inspect -f \"{{.State.StartedAt}}\" $id 2\u003e/dev/null)\n     age_seconds=$(( $(date +%s) - $(date -d \"$started\" +%s) ))\n     echo \"Container $id: health=$health, age=${age_seconds}s\"\n\n     if [ \"$health\" = \"starting\" ] || [ \"$health\" = \"unhealthy\" ]; then\n       if [ $age_seconds -gt $threshold ]; then\n         echo \"  → Would remove (stale, age \u003e ${threshold}s)\"\n       else\n         echo \"  → Keeping (still initializing, age \u003c ${threshold}s)\"\n       fi\n     fi\n   done\n   ```\n\n4. **Expected results:**\n   - `test-slow` (\u003e90s, \"starting\") → marked for removal\n   - `test-healthy` (\"healthy\") → not touched\n   - If `test-slow` is \u003c90s old → kept (wait longer)\n\n#### Phase 2: Safe Rollout to Production\n\n1. **Add detection to ONE stack first:**\n   ```nix\n   # In stacks/management/docker-compose.nix (small, non-critical stack)\n   # Add detectStaleHealth to ExecStartPre\n   ```\n\n2. **Deploy and monitor:**\n   ```bash\n   # On doc1\n   sudo nixos-rebuild switch --flake .#proxmox-vm\n   journalctl -u management-stack.service -f\n   ```\n\n3. **Verify behavior:**\n   - Check logs for \"Removing container...\" messages\n   - Confirm removed containers were actually stuck\n   - Verify healthy containers weren't touched\n\n4. **Gradual expansion:**\n   - Week 1: 1-2 small stacks (management, domain-monitor)\n   - Week 2: Medium stacks (paperless, mealie)\n   - Week 3: Critical stacks (immich, caddy)\n\n#### Phase 3: Rollback Plan\n\n**If detection removes wrong containers:**\n\n1. **Immediate rollback:**\n   ```bash\n   # Revert to previous generation\n   sudo nixos-rebuild switch --rollback\n   ```\n\n2. **Manual recovery:**\n   ```bash\n   # Recreate affected stack\n   cd /mnt/docker/\u003cstack-name\u003e\n   podman compose up -d\n   ```\n\n3. **Adjust threshold:**\n   ```nix\n   # Increase from 300s to 600s (10 minutes) if legitimately slow\n   if [ $age_seconds -gt 600 ]; then\n   ```\n\n**Safety guarantees:**\n- Only affects containers already in bad health states\n- Won't touch \"healthy\" or \"none\" (no health check) containers\n- Time threshold prevents premature removal\n- Worst case: Container recreated (same as manual fix)\n\n### Testing Recommendation 2 (Health Check Guidance)\n\n**Validation:**\n1. Test health check templates in sandbox\n2. Verify `start_period` covers actual initialization time\n3. Confirm health check command runs successfully\n\n### Monitoring During Rollout\n\n**What to watch:**\n```bash\n# Track health state changes\njournalctl -u '*-stack.service' | grep -E '(stale health|Removing container)'\n\n# Monitor auto-update behavior\njournalctl -u podman-auto-update.service\n\n# Check for unexpected restarts\npodman ps -a --format \"table {{.Names}}\\t{{.Status}}\\t{{.CreatedAt}}\"\n```\n\n## Implementation Priority\n\n1. **HIGH:** Stale health check detection (Recommendation 1)\n   - Solves immediate pain point (deployments hanging)\n   - Low risk with proper testing (phase 1-3 above)\n   - Quick to implement (~20 lines in podman-compose.nix)\n\n2. **MEDIUM:** Health check configuration guidance (Recommendation 2)\n   - Prevents future issues\n   - Requires documentation effort\n   - Improves long-term reliability\n\n3. **LOW:** Health check monitoring (Recommendation 5)\n   - Nice to have for debugging\n   - Can be added incrementally\n   - Not urgent (works fine without it)\n\n## Conclusion\n\nThe open question about rebuild vs auto-update behavior has been answered:\n\n1. **Container reuse IS the problem** - stale health checks cause real deployment hangs\n2. **Different scenarios already use different strategies** - system service reuses, user service recreates\n3. **The dual service architecture is correct** - each service optimized for its use case\n4. **Solution is targeted remediation** - detect and remove stale containers before reuse, not blanket --force-recreate\n\n**Next Steps:**\n1. Implement stale health check detection (Recommendation 1)\n2. Update documentation with health check best practices (Recommendation 2)\n3. Close research bead (nixosconfig-cm5) with findings\n4. Update bug bead (nixosconfig-hbz) with remediation plan\n\n## References\n\n### Official Documentation\n- [Docker Compose up command](https://docs.docker.com/reference/cli/docker/compose/up/)\n- [Docker Compose startup order](https://docs.docker.com/compose/how-tos/startup-order/)\n- [Podman auto-update documentation](https://docs.podman.io/en/latest/markdown/podman-auto-update.1.html)\n- [Red Hat: Podman auto-updates and rollbacks](https://www.redhat.com/en/blog/podman-auto-updates-rollbacks)\n\n### Community Resources\n- [Last9: Docker Compose Health Checks](https://last9.io/blog/docker-compose-health-checks/)\n- [Last9: Docker Status Unhealthy](https://last9.io/blog/docker-status-unhealthy-how-to-fix-it/)\n- [Tom Vaidyan: Health Checks Guide (2025)](https://www.tvaidyan.com/2025/02/13/health-checks-in-docker-compose-a-practical-guide/)\n- [Maciej Walkowiak: Docker Compose Waiting](https://maciejwalkowiak.com/blog/docker-compose-waiting-containers-ready/)\n- [Watchtower Usage Overview](https://containrrr.dev/watchtower/usage-overview/)\n\n### GitHub Issues and Discussions\n- [docker/compose#8351: --wait flag feature request](https://github.com/docker/compose/issues/8351)\n- [docker/compose#9600: Unnecessary recreation bug](https://github.com/docker/compose/issues/9600)\n- [docker/compose#10068: Container re-created when it shouldn't](https://github.com/docker/compose/issues/10068)\n- [podman#534: Autoupdate with podman-compose](https://github.com/containers/podman-compose/issues/534)\n- [podman#16098: Clarifying auto-update and rollback](https://github.com/containers/podman/discussions/16098)\n- [caprover#844: Container stuck in starting state](https://github.com/caprover/caprover/issues/844)\n\n### Forum Discussions\n- [Docker Forums: up --wait behavior changed](https://forums.docker.com/t/behavior-of-up-wait-changed/147699)\n- [Docker Forums: Unhealthy container does not restart](https://forums.docker.com/t/unhealthy-container-does-not-restart/105822)\n- [Docker Forums: Recreates every time (solved)](https://forums.docker.com/t/solved-docker-compose-up-recreates-every-time/134733)","status":"closed","priority":4,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T14:13:04.100655856+08:00","created_by":"abl030","updated_at":"2026-02-15T14:13:17.925554398+08:00","closed_at":"2026-02-15T14:13:17.925554398+08:00","close_reason":"Archived from docs/podman/ removal (commit 38b450e). Historical reference material.","labels":["archive","podman","research"],"dependencies":[{"issue_id":"nixosconfig-2o9.3","depends_on_id":"nixosconfig-2o9","type":"parent-child","created_at":"2026-02-15T14:13:04.103798175+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-2o9.4","title":"Research: Home Manager user service migration","description":"# Architecture and Operations Research on User-Scoped Podman Stack Units\n\n## Executive verdict\n\nVerdict: Partially. Moving your Podman stack lifecycle units from NixOS-generated user units in /etc/systemd/user to Home Manager–generated user units in ~/.config/systemd/user will materially reduce (and, in the common case, eliminate) the specific “stale ~/.config/systemd/user shadowed updated /etc/systemd/user” class of failures because ~/.config/systemd/user sits higher in the systemd user-unit search path than /etc/systemd/user, and the unit loader explicitly states that earlier directories override later ones.\n\nHowever, this migration does not inherently guarantee rebuild-time restart/reconciliation for user-scoped services because Home Manager’s activation logic will only perform systemd reload/switch operations when the user systemd manager is reachable (it checks systemctl --user is-system-running and skips if not running). This means failures still occur when the user manager is absent/non-lingering/unreachable, and some drift/override failure modes just “move” from /etc↔~/.config precedence conflicts into drop-in overrides / user-manager availability / sd-switch behavior.\n\nIn other words: Home Manager is a strong mitigation for your observed incident and improves lifecycle handling when the user manager is healthy, but it is not a universal fix for all reconciliation failures while staying in user scope.\n\n## Evidence by failure mode\n\n### Failure mode: unit file shadowing across /etc/systemd/user and ~/.config/systemd/user\n\nObserved incident (repo fact): stale unit under ~/.config/systemd/user/... shadowed the updated unit under /etc/systemd/user/..., leading to unexpected behavior after systemctl --user daemon-reload \u0026\u0026 restart.\n\nWhat systemd actually guarantees (primary sources):\n\n- The systemd.unit(5) user-unit search path lists (among others) both ~/.config/systemd/user and /etc/systemd/user, and the manual is explicit about precedence: “files found in directories listed earlier override files with the same name in directories lower in the list.”\n\nThis means that as long as a stale unit file continues to exist in ~/.config/systemd/user, it will override the unit of the same name in /etc/systemd/user, regardless of what you do to the /etc copy.\n\nWhy daemon-reload + restart didn’t “fix” it:\n\n- systemctl daemon-reload reloads the systemd manager configuration, reruns generators, reloads unit files, and recreates the dependency tree. It does not promise to change which on-disk file is selected if the load-path precedence and the set of files hasn’t changed.\n- systemctl restart promises “stop and then start” for the unit. It does not promise to flush all resources or to “discover a different unit file from a lower-precedence directory.”\n\nSo, with precedence unchanged, daemon-reload will reload the unit definition from the highest-precedence location (the stale ~/.config/... copy), and restart will restart that same loaded definition. This exactly matches your “expected new behavior did not appear” symptom and is fully consistent with documented precedence rules.\n\n### Failure mode: “restart” is not a full “reconcile” primitive\n\nTwo separate limitations matter operationally:\n\n- systemctl reload is explicitly “service-specific configuration” reload and not unit-file reload; unit-file reload requires daemon-reload.\n- systemctl restart does not necessarily flush all unit resources; if you need full resource teardown you may need stop followed by start.\n\nFor container stacks, this matters because a “restart” may not guarantee that everything you care about (environment, ExecStart semantics, dependency ordering, or other resource state) is re-derived if the unit definition you actually intended to load is not the one systemd is using, or if your unit semantics rely on full stop/start behavior.\n\n### Failure mode: shadowing can also come from higher-precedence runtime/control paths\n\nYour question focused on 4 paths, but the real top-of-path items in user scope include “control” and runtime directories, e.g. ~/.config/systemd/user.control and $XDG_RUNTIME_DIR/systemd/user.control, as well as transient and generator directories.\n\nOperational implication: if some tool writes persistent user-manager “control” overrides, or transient units exist with the same name, you can still see “I updated the file but behavior didn’t change” even when you standardize on Home Manager. This is a residual risk category to explicitly test.\n\n### Failure mode: does systemctl --user revert \u003cunit\u003e help in this specific incident?\n\nsystemctl revert is defined as “revert … unit files to their vendor versions” and removes drop-in config plus any user-configured unit file that overrides a matching vendor unit (vendor meaning “located below /usr/”). It also notes that if the unit has no vendor-supplied version, it is not removed.\n\nIn your incident, the conflict was user config (~/.config/...) overriding admin config (/etc/...), not overriding a vendor unit in /usr/lib/systemd/user or similar. Therefore, systemctl --user revert is not expected to remove the ~/.config/... unit file in order to “fall back” to /etc/systemd/user, because /etc is not “vendor” per the documented definition.\n\nWhat revert can help with in this general area is removing drop-ins and unmasking—i.e., if the “shadowing” is caused by systemctl --user edit drop-ins or masks rather than a full competing unit file.\nBut for the specific “~/.config vs /etc” override you described, it should be treated as non-solution / only partially relevant.\n\n## Home Manager impact analysis\n\n### Where Home Manager materializes systemd.user.services\n\nHome Manager’s modules/systemd.nix generates user units into XDG config files under systemd/user/... (e.g. systemd/user/\u003cunit\u003e.service) and installs WantedBy/RequiredBy links under systemd/user/\u003ctarget\u003e.wants/ etc. This is done via xdg.configFile entries with names like systemd/user/${filename}.\n\nWith standard XDG defaults, that means: ~/.config/systemd/user/*.service (and related wants/requires directories) are Home Manager’s primary materialization target for user units.\n\nHome Manager also has an explicit mechanism to expose unit files shipped by packages into $XDG_DATA_HOME/systemd/user, via its systemd.user.packages option.\nThis matters because the systemd user-unit search path includes both config-home and data-home directories (with config-home taking precedence).\n\n### How Home Manager activation reloads and (attempts to) reconcile services\n\nHome Manager defines an activation step home.activation.reloadSystemd which:\n\n- Ensures XDG_RUNTIME_DIR is set (comment says this is needed when running from the NixOS module where it is not set).\n- Checks the user manager state via systemctl --user is-system-running. It proceeds only if the result is running or degraded; otherwise it prints “User systemd daemon not running. Skipping reload.”\n- When enabled, it uses sd-switch to compute differences between “old units directory” and “new units directory” (derived from the old/new Home Manager generations) and automatically applies the necessary start/stop/reload/restart actions.\n- When systemd.user.startServices is disabled, it instead runs a script that only prints suggested systemctl commands, requiring manual application.\n\nHome Manager also introduces unit-level metadata for switching behavior (in the unit’s [Unit] section): X-Reload-Triggers, X-Restart-Triggers, and X-SwitchMethod (e.g., “reload”, “restart”, “stop-start”, “keep-old”). These are explicitly described as activation-time triggers/switch hints.\n\n### How Home Manager behaves standalone vs NixOS integration\n\nThe Home Manager project documents that it can be used standalone or “as a module within a NixOS system configuration,” where user profiles are built together with the system when running nixos-rebuild.\n\nIn NixOS integration, Home Manager creates a system-level oneshot service per configured user: home-manager-\u003cusername\u003e.service, which runs as that user (User=\u003cusername\u003e), is ordered into multi-user.target, and executes the Home Manager activation package.\n\nThat system service runs the activation script using a login shell and attempts to import session variables from the user’s systemd environment via systemctl --user show-environment, while also defaulting XDG_RUNTIME_DIR to /run/user/$UID if unset.\n\nImportant implication for your objective: Under NixOS integration, Home Manager activation is not “magically more privileged”—it still ultimately depends on being able to talk to the user systemd manager. Home Manager’s own activation code explicitly anticipates that XDG_RUNTIME_DIR could be missing under the NixOS module and patches around it, but it still skips reloading if the user manager is not reachable/running.\n\n### Would Home Manager ownership eliminate the specific shadowing class?\n\nYes, for the class you observed—if you consolidate ownership.\n\nYour incident is a cross-owner collision: NixOS-generated user units are placed under /etc/systemd/user (via environment.etc.\"systemd/user\") , but ~/.config/systemd/user has higher precedence.\n\nIf you move generation of those units into Home Manager, the authoritative units move into the highest-precedence directory (~/.config/systemd/user), which makes “stale ~/.config shadowed updated /etc” structurally unlikely—because there is no longer a need for an /etc copy at all, and Home Manager will overwrite/update the ~/.config copy each activation.\n\nBut it can “move” the failure mode:\n\n- Overrides and drop-ins created by systemctl --user edit live in user config scope and can still override what Home Manager writes. (systemctl revert only guarantees vendor reversion semantics, not “revert to Home Manager version.”)\n- Home Manager only attempts service reconciliation when the user manager is running or degraded; otherwise it will skip.\n\nSo, you eliminate one precedence collision class, but you must still manage drift/overrides and user-manager availability.\n\n## Test system blueprint\n\nThis blueprint is designed to validate both ownership models while being edge-case heavy and automation-friendly:\n\n- Model A: NixOS-generated user units in /etc/systemd/user (current design).\n- Model B: Home Manager–generated user units in ~/.config/systemd/user with systemd.user.startServices active (candidate design).\n\n### Persistent test host path\n\nGoal: a “prod-like” environment where state (including drift, stale links, and rollbacks) can accumulate.\n\nRecommended shape\n\nA dedicated VM (or spare host) with:\n\n- Your flake-based system config and Home Manager integration enabled (repo fact).\n- A dedicated test user that owns the Podman rootless stack units (so you can isolate failures).\n- Persistent storage backing (qcow2 + snapshots, or ZFS dataset) so you can force “stale generation” behavior and test rollback semantics.\n\nObservability baseline (capture every run)\n\nUse these as your invariant “before/after” checks:\n\n```bash\n# Identify which unit file systemd is actually using\nsystemctl --user show \u003cunit\u003e -p FragmentPath -p DropInPaths -p UnitFileState\n\n# Show the on-disk unit content (helps detect shadowing \u0026 diverged edits)\nsystemctl --user cat \u003cunit\u003e\n\n# Confirm unit-path precedence on *this* host (diagnose surprises)\nsystemd-analyze --user unit-paths || true\n\n# Global user-manager state\nsystemctl --user is-system-running\nsystemctl --user --failed --no-pager\n\n# Podman state\npodman ps --all\npodman images\npodman inspect \u003ccontainer\u003e --format '{{.State.Status}} {{.State.Health.Status}}' 2\u003e/dev/null || true\n\n# Journal slice around the rebuild (tune time window per run)\njournalctl --user -u \u003cunit\u003e --no-pager -b\n```\n\nThe critical invariant you want per scenario is: FragmentPath points where you think it does, and the observed running containers match the intended generation.\n\n### Fast repeatable harness path\n\nGoal: dozens of scenarios, repeatable, minimal manual setup.\n\nTwo practical approaches (choose based on your tolerance for build time vs fidelity):\n\nHarness option: NixOS VM test framework (highest rigor, heavier builds)\nUse NixOS’ VM test system (the same style as nixosTests) to boot a VM, apply two generations, and assert systemd + Podman behavior via scripted commands. This is the most “programmable scenario runner” style because the test driver can intentionally corrupt files, simulate missing runtime dirs, and run rebuild/rollback sequences deterministically.\n\nHarness option: QEMU VM with snapshot + scenario runner (fast iteration, high fidelity)\nBuild a single VM once, then run scenarios by:\n\n- Snapshot baseline.\n- Apply scenario mutations (compose/env/secret/unit drift).\n- Run nixos-rebuild switch (or direct switch-to-configuration) and record assertions.\n- Roll back snapshot.\n\nThis preserves “real” systemd and Podman behavior without re-building the world per test.\n\n### Scenario runbook definitions\n\nThe scenarios below are written to be applicable to both Model A and Model B; the key is what you declare as the “expected FragmentPath”:\n\n- Model A expected FragmentPath: /etc/systemd/user/\u003cunit\u003e (or a symlink under /etc/systemd/user).\n- Model B expected FragmentPath: ~/.config/systemd/user/\u003cunit\u003e (or a symlink there).\n\nFor each scenario, “Assertions” include both systemd and Podman state checks.\n\nS01 — Unit shadowing conflict (~/.config vs /etc)\n\nSetup\nCreate two different unit files with the same name and obvious behavioral differences:\n\n- A copy under ~/.config/systemd/user/\u003cunit\u003e.service\n- A copy under /etc/systemd/user/\u003cunit\u003e.service\n\n(You can do this by temporarily disabling your generator and writing minimal test units.)\n\nAction\n\n```bash\nsystemctl --user daemon-reload\nsystemctl --user restart \u003cunit\u003e\n```\n\nAssertions\n\n```bash\nsystemctl --user show \u003cunit\u003e -p FragmentPath\nsystemctl --user cat \u003cunit\u003e | head\n```\n\nExpected: FragmentPath resolves to the user config copy, because earlier directories override later ones in the user unit search path.\n\nPass/fail criteria\nPass if the active unit source is the higher-precedence path and behavior matches that file; fail otherwise.\n\nCleanup\nRemove the manually created unit file(s) and reload.\n\nS02 — “daemon-reload does not unshadow”\n\nSetup\nSame as S01.\n\nAction\nModify only the /etc/systemd/user copy; leave ~/.config copy untouched. Then:\n\n```bash\nsystemctl --user daemon-reload\nsystemctl --user restart \u003cunit\u003e\n```\n\nAssertions\nFragmentPath and cat still point to ~/.config copy, not the updated /etc copy, until the higher-precedence file is removed. (Pass if you observe that “reload + restart” doesn’t switch to lower-precedence file.)\n\nCleanup\nRemove test units.\n\nS03 — Compose change: image tag change\n\nSetup\nBaseline stack running from a compose file with a pinned image tag.\n\nAction\nChange tag (e.g., myimage:v1 → myimage:v2) and rebuild/switch.\n\nAssertions\n\n```bash\nsystemctl --user status \u003cunit\u003e --no-pager\npodman ps --all --format '{{.Names}}\\t{{.Image}}\\t{{.Status}}'\n```\n\nExpected: service returns to active, container image matches new tag, and restart occurred.\n\nPass/fail criteria\nPass if container restarts into new image and unit remains active; fail if container remains on old image or unit uses old ExecStart.\n\nCleanup\nRollback commit or revert tag.\n\nS04 — Compose change: service rename (forces remove/create)\n\nSetup\nCompose file with service web.\n\nAction\nRename service to web2 and rebuild/switch.\n\nAssertions\n\n```bash\npodman ps -a --format '{{.Names}}'\nsystemctl --user status \u003cunit\u003e --no-pager\n```\n\nExpected: old container removed or stopped; new named container exists; unit active.\n\nPass/fail criteria\nPass if “rename” produces correct container set without leaving orphaned (unless explicitly desired).\n\nCleanup\nRevert.\n\nS05 — Environment file missing vs removed vs malformed\n\nSetup\nUnit references an EnvironmentFile (directly or via script). Start in “present \u0026 valid” state.\n\nAction A (missing)\nDelete the env file, rebuild/switch.\n\nAssertions\n\n```bash\nsystemctl --user status \u003cunit\u003e --no-pager\njournalctl --user -u \u003cunit\u003e --no-pager -b | tail -n 80\n```\n\nExpected: service fails deterministically and logs indicate missing file / parse failure.\n\nAction B (removed from config)\nRemove reference from stack definition, rebuild/switch.\n\nAssertions\nUnit becomes active again without relying on file.\n\nPass/fail criteria\nPass if failures are deterministic and reconciliation occurs when config is corrected.\n\nCleanup\nRestore env file / revert change.\n\nS06 — Secret path changes and missing secret cases\n\nSetup\nContainer consumes a secret from a known file path.\n\nAction\nChange secret path in config and rebuild/switch; then test “missing secret” by removing file.\n\nAssertions\nsystemctl --user status shows expected success/failure; Podman container state reflects start failure when secret missing.\n\nPass/fail criteria\nPass if missing secret fails fast with clear logs and recovery works after restoring secret.\n\nCleanup\nRestore secret and rerun the switch.\n\nS07 — Manual drift: edited unit file and stale symlink\n\nSetup\nPick a unit under test and confirm its FragmentPath.\n\nAction A (edit drift)\nModify the on-disk unit file directly (or create a conflicting drop-in). Rebuild/switch.\n\nAssertions\n\n```bash\nsystemctl --user show \u003cunit\u003e -p FragmentPath -p DropInPaths\nsystemctl --user cat \u003cunit\u003e | sed -n '1,80p'\n```\n\nExpected: drift is detectable (drop-in present, content differs).\n\nAction B (stale symlink)\nReplace unit file symlink with a stale target; rebuild/switch.\n\nPass/fail criteria\nPass if your process detects and corrects drift (or at minimum alarms); fail if drift silently persists.\n\nCleanup\nRemove drift artifacts, regenerate by rebuild.\n\nS08 — Auto-update interaction (Podman)\n\nSetup\nUse a container configured for auto-update in a systemd unit context.\n\nAction\nTrigger podman auto-update (timer or manual) and then run a rebuild/switch.\n\nAssertions\nPodman auto-update is documented to restart systemd units that run containers after pulling an updated image.\nVerify:\n\n```bash\njournalctl --user -u podman-auto-update.service --no-pager -b || true\npodman images\nsystemctl --user status \u003cunit\u003e\n```\n\nPass/fail criteria\nPass if auto-update and rebuild do not “fight” into oscillation and the final state matches the target generation.\n\nCleanup\nDisable timer and revert image labels for the test.\n\nS09 — Health-check weirdness: stuck unhealthy/starting\n\nSetup\nEnable a healthcheck that can flip unhealthy.\n\nAction\nForce unhealthy state (e.g., block dependency) and rebuild/switch.\n\nAssertions\nPodman supports healthchecks and (in newer Podman) configurable healthcheck actions like restart; document claims this exists and how it behaves.\nVerify:\n\n```bash\npodman ps --all --format '{{.Names}}\\t{{.Status}}'\npodman inspect \u003ccontainer\u003e --format '{{.State.Health.Status}}'\nsystemctl --user status \u003cunit\u003e\n```\n\nPass/fail criteria\nPass if system converges (container healthy or fails clearly and stays failed). Fail if you get loops or silent unhealthy while systemd claims active.\n\nCleanup\nRestore normal health behavior.\n\nS10 — Rebuild with no material config changes\n\nSetup\nClean baseline state; no changes between two rebuilds.\n\nAction\nRun rebuild/switch twice.\n\nAssertions\nExpect no unnecessary restarts beyond what your tooling defines as needed. For Home Manager, the “switch method / triggers” system exists specifically to avoid unnecessary restarts, and restarts may occur based on diffs/triggers.\nMeasure:\n\n```bash\nsystemctl --user show \u003cunit\u003e -p ActiveEnterTimestamp -p ExecMainStartTimestamp\n```\n\nPass if timestamps don’t change (or change only when expected by your policy).\n\nCleanup\nNone.\n\nS11 — Rebuild while user manager absent/degraded/not lingering\n\nSetup\nEnsure the user manager is not running or is degraded.\n\nAction\nRun rebuild/switch.\n\nAssertions\nHome Manager explicitly skips reload if user systemd is not running.\nFor NixOS activation, rebuild output often includes “reloading user units for …”; the activation script reloads user units for users returned by loginctl list-users and starts nixos-activation.service under that user.\nValidate:\n\n```bash\nsystemctl --user is-system-running   # from inside an actual user session if possible\njournalctl -u home-manager-\u003cuser\u003e.service --no-pager -b || true\n```\n\nPass/fail criteria\nPass if behavior is predictable and you can define a recovery path (next login, linger enablement, or explicit restart). Fail if rebuild claims success but services silently remain stale.\n\nCleanup\nRestore normal user manager conditions.\n\nS12 — Rollback reconciliation (nixos-rebuild switch --rollback)\n\nSetup\nMake a change that modifies unit or container behavior (tag change, env var).\n\nAction\nRollback.\n\nAssertions\n\n```bash\npodman ps --all --format '{{.Names}}\\t{{.Image}}'\nsystemctl --user show \u003cunit\u003e -p FragmentPath -p ActiveState\n```\n\nPass if rollback converges to previous intended generation behavior (container image, env, secrets), not just the unit file contents.\n\nCleanup\nReturn to baseline generation.\n\n## Scenario matrix\n\n| ID | Area | What it stresses | Primary risk it detects | Key assertion(s) |\n|---|---|---|---|---|\n| S01 | Unit shadowing | ~/.config vs /etc precedence | Wrong unit source chosen | FragmentPath points to highest-precedence copy |\n| S02 | Reload semantics | daemon-reload under shadowing | False belief that reload “switches source” | daemon-reload doesn’t change source without removing override |\n| S03 | Compose mutation | Image tag update | Stale container image | Container image matches desired tag |\n| S04 | Compose mutation | Service rename | Orphaned/stale containers | Old container removed; new created |\n| S05 | Env robustness | Missing/removed/malformed env | Non-deterministic failures | Clear failure + deterministic recovery |\n| S06 | Secret robustness | Secret path drift | Silent wrong config | Fail-fast on missing secret |\n| S07 | Drift | Manual edits / stale links | Declarative state bypassed | Drift detectable via cat / drop-ins |\n| S08 | Auto-update | podman auto-update interaction | Restart races / oscillation | Final state matches target generation |\n| S09 | Healthcheck | unhealthy/starting edge cases | systemd says active while unhealthy | Converges (healthy) or fails clearly |\n| S10 | No-op rebuild | idempotency | Unnecessary restarts | Timestamps stable unless policy triggers |\n| S11 | User manager absent | non-linger / degraded | “Switch succeeded but nothing reconciled” | HM skip behavior; NixOS reload scope observable |\n| S12 | Rollback | reverse reconciliation | Rollback doesn’t restore runtime | Container + unit converge to prior state |\n\n## Recommendation and risks\n\n### Final architecture recommendation for phase 3\n\nRecommendation: Adopt single-owner user-unit management for the Podman stack units, and if you choose Home Manager as the owner, treat it as an operational system that must be validated under user-manager availability constraints.\n\nConcretely:\n\n- If the dominant pain is the class you observed (stale ~/.config shadowing /etc), then moving ownership into Home Manager is an effective structural mitigation, because it places the authoritative unit in the highest-precedence user path and updates it during activation.\n- If your dominant pain is “rebuild-time reconciliation must always happen even if no user manager is running,” then neither approach fully guarantees that while staying in user scope, because Home Manager explicitly skips reload when user systemd isn’t running. In that case you need an operational safeguard design (below) or reconsider scope (system services).\n\n### Decision table\n\n| Dimension | NixOS-managed user unit (/etc/systemd/user) | Home Manager-managed user unit (~/.config/systemd/user) | Hybrid ownership |\n|---|---|---|---|\n| Unit placement (primary) | Generated into /etc/systemd/user via environment.etc.\"systemd/user\" | Generated into XDG config systemd/user/... (typically ~/.config/systemd/user) | Hard to make safe; risks name collisions |\n| Susceptibility to ~/.config shadowing | High (by definition): ~/.config/systemd/user overrides /etc/systemd/user | Lower for this specific class (authoritative copy is already highest-precedence) | Highest risk: two competing sources for same unit name |\n| Rebuild-time service reconciliation | NixOS activation reloads user units for users found via loginctl list-users and starts nixos-activation.service ; does not inherently imply full lifecycle management of arbitrary user units | Home Manager runs sd-switch-based switching when user systemd is reachable; otherwise skips | Frequently undefined (double-reload / inconsistent restart logic) |\n| Drift resistance | Strong for /etc content but weak if user creates ~/.config overrides | Strong for declared content but still vulnerable to manual drop-ins/edits in ~/.config | Weakest |\n| Operational clarity (“where is truth?”) | Split truth: /etc is intended, but ~/.config can override silently | Clearer: “truth is in ~/.config”, but must manage user-manager availability | Lowest clarity |\n| Best fit for your objective | Acceptable only if you also prevent/clean shadowing | Best direct match, with explicit residual risks | Not recommended |\n\n### Operational safeguards regardless of approach\n\nThese are not “code fixes”; they are operational guardrails derived from the documented behaviors:\n\n- Shadowing detection as a first-class health signal\nAlways check FragmentPath and DropInPaths for every stack unit and fail the “switch” pipeline (or at least alert) if the path is not the expected owner path. This is the direct way to detect “stale unit shadowing” rather than discovering it via runtime behavior.\n\n- Treat “user manager unavailable” as a failed reconciliation state\nHome Manager will skip reload when user systemd isn’t running. Make it observable (logs/alerts) so “no reconcile happened” is not silent success.\n\n- Explicitly test interactions with Podman auto-update\nPodman auto-update is designed to restart systemd units that run containers after image updates. If you also restart/reconcile on rebuild, you must test for restart races and ensure the final state is deterministic (S08).\n\n- Make rollback a first-class supported operation\nIf rollback does not converge runtime state (containers/config/env), you do not have a safe declarative ops story. Run S12 regularly.\n\n### Minimal migration safety checklist\n\n- Inventory current unit names and current loaded sources\nRecord systemctl --user show \u003cunit\u003e -p FragmentPath -p DropInPaths for every stack unit.\n\n- Enforce single ownership of each unit name\nBefore migration: ensure that for each unit name, only one authoritative source exists (either /etc/systemd/user or ~/.config/systemd/user). The systemd loader explicitly treats earlier path entries as overriding.\n\n- Plan for the transitional “stale units remain” phase\nMigration must include a step that proves old unit files are not still present in higher-precedence directories.\n\n- Validate reconciliation under degraded and absent user manager states\nHome Manager will skip reload when the user daemon is not running. This must be an explicit go/no-go gate for “production-safe.”\n\n- Validate Podman-specific edge cases (auto-update, healthcheck)\nPodman auto-update expects containers to run inside systemd units and restarts those units. Ensure this doesn’t conflict with rebuild-driven restarts.\n\n## Source list with links\n\nsystemd behavior (unit paths, reload/restart/revert semantics)\n\n- systemd.unit(5) — User Unit Search Path and explicit override rule (“directories listed earlier override … lower in the list”).\n- systemctl(1) — daemon-reload definition (reruns generators, reloads unit files, recreates dependency tree).\n- systemctl(1) — restart semantics and limitations (not necessarily flushes all resources; stop+start may be needed).\n- systemctl(1) — reload vs daemon-reload distinction (reload does not reload unit file).\n- systemctl(1) — revert definition and vendor-version limitation (vendor is below /usr/; no vendor version implies not removed).\n\nNixOS user unit generation and activation behavior\n\n- NixOS module generating per-user units into /etc/systemd/user via environment.etc.\"systemd/user\".\n- NixOS activation-script module defining system.userActivationScripts and the nixos-activation user service.\n- NixOS switch-to-configuration.pl (example commit) showing “reloading user units for $name…” and use of systemctl --user daemon-reexec and starting nixos-activation.service for users returned by loginctl list-users.\n\nHome Manager user service generation and activation\n\n- Home Manager modules/systemd.nix — generation of systemd/user/... xdg config files, definition of systemd.user.startServices, and reload logic (checks is-system-running, uses sd-switch, skips if not running).\n- Home Manager project README — states NixOS module mode builds profiles with nixos-rebuild.\n- Home Manager NixOS module (nixos/default.nix) — system home-manager-\u003cuser\u003e oneshot service, runs as user, imports environment from user systemd environment, defaults XDG_RUNTIME_DIR.\n\nPodman interactions that affect systemd lifecycle/reconciliation\n\n- podman-auto-update(1) — auto-update restarts the systemd unit executing the container after pulling an updated image; requires running inside systemd units.\n- podman-generate-systemd(1) — notes deprecation and recommends Quadlet for running containers under systemd.\n- podman-systemd.unit(5) — systemd unit file options for Podman-managed containers including health-related settings.\n- Red Hat blog on Podman healthcheck actions (restart/stop/kill/none; “starting with Podman v4.3”).","status":"closed","priority":4,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T14:13:05.287956912+08:00","created_by":"abl030","updated_at":"2026-02-15T14:13:17.937758073+08:00","closed_at":"2026-02-15T14:13:17.937758073+08:00","close_reason":"Archived from docs/podman/ removal (commit 38b450e). Historical reference material.","labels":["archive","home-manager","podman","research"],"dependencies":[{"issue_id":"nixosconfig-2o9.4","depends_on_id":"nixosconfig-2o9","type":"parent-child","created_at":"2026-02-15T14:13:05.289736353+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-2o9.5","title":"Research: Podman autoupdate + compose raw image metadata","description":"Podman Auto-Update Failures With Compose-Managed Containers on NixOS\nExecutive summary and conclusion\n\nYour observed failure (locally auto-updating container \"\u003cid\u003e\": raw-image name is empty) is best explained as a Podman implementation gap/bug in the Docker-compatible (“compat”) container create API path, which is the path used by podman compose when it delegates to docker-compose / docker compose as provider.\n\nIn Podman 5.7.1, podman auto-update hard-requires that ctr.RawImageName() be non-empty; if it is empty, Podman records an error and skips the container. However, the compat API CreateContainer handler in 5.7.1 does not set SpecGenerator.RawImageName, which is the record needed for ctr.RawImageName() later. As a result, containers created through this API path can have a valid ImageName/Config.Image but still have empty RawImageName, which reliably breaks podman auto-update.\n\nThis is also explicitly reported upstream as a bug for docker-compose/API-created containers (issue #19688).\n\nConclusion: this is not intended “working as designed” behavior for the compose-managed lifecycle + Podman auto-update combination; it is a known incompatibility stemming from an incomplete implementation in the Docker-compat API create path, and it is still present in the v5.7.1 source shown below.\n\nConfidence: High for root cause (directly supported by 5.7.1 source + upstream bug report).\nConfidence: Medium for “fixed in later versions” status (I did not find evidence of a fix in available release notes, and the relevant 5.7.1 code path still exists).\nWhat exactly fails and why it fails\nAuto-update requires RawImageName and errors if it is empty\n\nIn Podman 5.7.1, auto-update builds a task list over running containers with io.containers.autoupdate set. It reads rawImageName := ctr.RawImageName() and immediately errors out if it is empty, producing the exact message you see.\n\nThis is not an incidental log string—RawImageName is used as the image reference for (a) checking digests and (b) pulling updates; the task logic stores it as task.rawImageName and uses it to parse registry references and to pull.\n\nPodman’s own spec generator data model documents the intent: RawImageName is the user-specified, unprocessed image input, and while “optional”, it is “strongly encouraged” when Image is set—specifically because workflows like auto-update need the exact original reference.\nWhy compose-managed containers tend to have empty RawImageName\n\npodman compose is not a native compose engine; it is a thin wrapper that executes an external provider (typically docker-compose / docker compose), wiring it to the local Podman socket. By default, if docker-compose is installed, it takes precedence.\n\nWhen the provider is Docker Compose, container creation goes through Podman’s Docker-compat (“compat”) API.\n\nIn Podman 5.7.1, the compat CreateContainer handler:\n\n    normalizes the image name (NormalizeToDockerHub),\n    looks up the image,\n    then creates a SpecGenerator using either the image ID or a resolved name, but never sets SpecGenerator.RawImageName.\n\nBecause SpecGenerator.NewSpecGenerator() sets only ContainerStorageConfig.Image (not RawImageName), RawImageName remains unset unless a higher layer explicitly populates it.\n\nSo you get the exact mismatch you reported:\n\n    podman inspect shows an image name in several places (e.g., normalized image name),\n    but RawImageName (the “original user input”) can still be empty,\n    and podman auto-update fails.\n\nThis is confirmed by the upstream bug report for docker-compose/API-created containers: “Looks like rawImageName is only set in CLI and play but not in API handler.”\nBug vs design vs edge-case and version behavior\nClassification\n\nBug / implementation gap (most accurate):\n\n    Auto-update requires RawImageName and fails without it (documented by 5.7.1 code).\n    The Podman compat API CreateContainer handler does not set RawImageName in 5.7.1 (documented by 5.7.1 code).\n    The SpecGenerator model strongly encourages setting RawImageName when Image is set (documented in the model comment).\n    Upstream issue #19688 identifies the exact mismatch and is labeled as a bug.\n\nNot “intended behavior”: the spec generator explicitly frames RawImageName as important for exact user input, and the issue report expects parity with CLI-created containers.\n\nEdge-case limitation (secondary framing): Podman also enforces fully-qualified references for registry-based update, because if containers are created from IDs, Podman cannot know which registry reference to check/pull. This is explicitly documented in Podman’s systemd/quadlet unit docs.\nHowever, your case is not “image ID used intentionally”; it is a missing metadata field despite having a usable image name elsewhere.\nDoes podman compose fail to populate RawImageName “by design”?\n\nIf podman compose is using Docker Compose (docker-compose) as provider (the default behavior when installed), it is essentially routing creation through the compat API handler, which in 5.7.1 visibly does not populate RawImageName.\n\nThat makes the observed behavior an implementation gap/bug in Podman’s compat API path, not a deliberate “compose must not support auto-update” design choice.\nVersions affected and fixed\n\nDirectly evidenced as affected\n\n    Podman 5.7.1: compat CreateContainer handler does not set RawImageName; auto-update errors on empty RawImageName.\n    Podman 4.6.0 is implicated by the upstream issue reproduction environment, and the reported symptom matches the same missing-field root cause.\n\nRelated precedent: fixed elsewhere\n\n    A similar class of bug existed for podman play kube: containers created by podman play kube “did not record the raw image name used to create containers,” and this was later fixed (documented in release notes).\n    This supports the interpretation that “missing RawImageName breaks auto-update” is a recognized correctness issue, not an accepted limitation.\n\nFixed for compat compose path?\n\n    I did not find evidence (in the investigated sources) that compat container create now sets RawImageName in a way that would resolve this in 5.7.1. The 5.7.1 code shown still omits it.\n    Issue #19688 is closed/locked, but the closure state alone does not demonstrate a fix; the report contains no linked PR in the captured view.\n\nConfidence: Medium that “no fix exists up through 5.7.1 and the current documented behavior you’re seeing”; High that 5.7.1 is affected.\nOfficially supported update strategies for compose-managed workloads today\nWhat Podman officially supports well: systemd/Quadlet + auto-update\n\nPodman’s systemd integration (via generator / quadlet-style unit definitions) is where Podman auto-update is most “first-class”:\n\n    Podman’s systemd unit documentation includes AutoUpdate= and explicitly ties it to podman-auto-update(1).\n    It also documents the fully-qualified image reference requirement for registry auto-update.\n\nAdditionally, Podman’s podman-auto-update documentation explicitly references configuring auto-update via quadlet.\n\nFor a homelab/single-host environment, the “Podman-native” strategy that aligns with official documentation is therefore:\n\n    Manage containers via Quadlet/systemd units (user units for rootless)\n    Set AutoUpdate=registry (or equivalent labels)\n    Run podman-auto-update.timer or invoke podman auto-update on a schedule\n\nWhat Podman officially provides for compose: a wrapper, not lifecycle semantics\n\nPodman documents that podman compose is a wrapper executing an external compose provider and passing through commands/options.\n\nTherefore, “compose-managed updates” are, in practice, the provider’s update workflow—typically:\n\n    podman compose pull\n    podman compose up -d (and sometimes --force-recreate, depending on provider semantics)\n\nThis is not a Podman-specific claim of guarantee; it follows directly from “podman compose executes another tool and passes the command/args directly.”\nPractical implication for your environment\n\nBecause the compat create handler in 5.7.1 does not set RawImageName, Podman-native auto-update is not operationally reliable for containers created via docker-compose over the Podman socket.\n\nSo, “officially recommended” in the sense of “most supported by Podman docs and implementation” is:\n\n    Use Quadlet/systemd-managed containers for auto-update, or\n    Use compose pull + redeploy as your update mechanism when you stay on compose.\n\nBest practices for rootless + systemd + compose in ops\nScheduling and observability with systemd timers\n\nSystemd timers are explicitly designed for time-based activation of services (cron-like scheduling under systemd supervision). For a homelab goal of low-toil + reliable logging, this matters because timer-triggered services:\n\n    run under a consistent unit name,\n    emit logs to journald in a centralized way (systemd-managed).\n\nPrefer Podman’s systemd/Quadlet integration for “production-like” stability\n\nPodman’s systemd unit docs emphasize that it supports both system and user units, and that unit generation is integrated into boot/daemon-reload.\n\nEnterprise guidance (RHEL documentation) explicitly positions Quadlet as having “many advantages” over generated unit files and notes Quadlet availability in recent Podman versions (starting with Podman v4.6 in that doc). This is a strong signal that “systemd-native container units” are the long-term-friendly management approach.\nCompose in production/homelab: minimize “API impedance mismatch”\n\nGiven podman compose defaults to Docker Compose if installed, and Docker Compose uses the compat API path, you should assume “Docker API semantics” apply unless proven otherwise.\n\nWhen an operational feature depends on Podman-internal metadata (like RawImageName), prefer workflows that create containers through Podman-native code paths (Quadlet/CLI) over Docker-compat code paths. This is an inference supported by the specifically missing RawImageName in the compat handler vs its use in auto-update.\nWatchtower in Podman environments\nCompatibility and maintenance status\n\nWatchtower’s documented operating model is to run as a container which must mount the Docker socket because it “needs to interact with the Docker API.”\n\nThere are long-running upstream Watchtower discussions requesting Podman support because Watchtower expects /var/run/docker.sock and Docker API behavior.\n\nMore importantly for 2026 operational decision-making: the upstream Watchtower repository was archived (read-only) on December 17, 2025, which indicates the original project is no longer actively maintained in its upstream home.\nRisk profile compared to Podman-native approaches\n\nBased on Watchtower’s requirement to mount a privileged control socket for the container runtime (Docker API socket), Watchtower inherently expands the blast radius of a compromise of the Watchtower container (general socket-mount risk). This is a security inference from the documented requirement to mount the control socket.\n\nPodman’s preferred path for automatic updates is integrated with systemd/quadlet (AutoUpdate=) and podman auto-update, which avoids introducing an additional third-party controller container.\nBottom line\n\nFor Podman environments in 2026, Watchtower is generally redundant at best and risky at worst, and the archival of the upstream repo materially increases operational risk.\n\nConfidence: High that Watchtower relies on Docker API socket mounting; High that upstream repo is archived; Medium that it is “not recommended” for Podman specifically (because some people run it against Podman’s Docker-compat socket, but you inherit both compatibility gaps and a now-archived upstream).\nRemediation options, decision matrix, and a rollout plan\nCan missing raw image metadata be repaired in place?\n\nPractically, treat this as not repairable in place.\n\nPodman’s internal container config states the container configuration “may not be changed once created” and is stored read-only in state; changes are not written back and can cause inconsistencies.\n\nSince podman auto-update uses ctr.RawImageName() and fails when empty, the remediation is to ensure new containers are created with RawImageName populated, rather than trying to mutate existing containers.\n\nConfidence: Medium-high (based on Podman internal documentation + behavior), but note this is based on upstream code comments rather than a “supported admin API” statement.\nWhat “creation paths” will populate RawImageName?\n\nFrom the upstream bug report perspective, RawImageName is “set in CLI and play but not in API handler.” In your case, this implies:\n\n    Docker-compat API path (docker-compose / docker compose provider): does not set it in 5.7.1.\n    Podman-native creation paths: are expected to set it (at least in the cases called out by upstream), and Podman historically fixed missing-raw-image-name issues in other command paths (e.g., play kube).\n\nDecision matrix\nOption\tReliability in real ops\tOperational complexity\tDowntime risk\tSecurity posture\tNotes / key tradeoffs\nStay on podman auto-update + wait for fix\tLow (for docker-compose/compat-created containers) \tLow (no changes)\tLow (but you’re not updating)\tGood (Podman-native)\tYou will keep getting “raw-image name is empty” until containers are created with RawImageName populated or Podman changes compat handler.\nRecreate containers under a path that sets required metadata\tMedium–High (if you truly move off compat create) \tMedium\tMedium (recreate events)\tGood\tRequires changing how containers are created (e.g., Quadlet or Podman-native create path). Container config is not meant to be mutated in place.\nReplace with pull/redeploy script per stack (podman compose pull \u0026\u0026 up -d)\tHigh (most predictable) \tMedium (per-stack timers/scripts)\tMedium (depends on restart strategy)\tGood\tUses compose provider semantics; avoids RawImageName entirely. Works even when compat path can’t support auto-update.\nUse Watchtower\tLow–Medium (depends on compat/socket behavior) \tMedium\tMedium\tWorse (socket-mount controller) \tUpstream is archived/read-only as of Dec 17, 2025, increasing long-term risk.\nRecommended operational path for a homelab\n\nGiven your stated priorities (“low toil, high reliability”) and the demonstrated incompatibility between auto-update and compat-created containers, the most pragmatic approach is:\n\n    Short term: move to per-stack pull + redeploy under systemd timers (option 3).\n    Medium term: migrate “important” stacks to Quadlet/systemd units with AutoUpdate=registry (Podman-native path), so you can use Podman’s supported auto-update model without compose/provider edge cases.\n    Avoid: Watchtower, unless you accept the security tradeoff and the archived-upstream risk.\n\nConfidence: High for “option 3 works around the RawImageName failure”; Medium-high for “Quadlet migration is the best-aligned long-term strategy.”\nPractical rollout plan with rollback\nPreparation phase\n\nCreate an inventory report that lets you separate:\n\n    containers that can be auto-updated today vs those that cannot, and\n    which stacks are impacted.\n\nExample checks (illustrative commands):\n\n    Identify containers with the autoupdate label\n    Ensure RawImageName is populated (this is the key failure point)\n    Confirm PODMAN_SYSTEMD_UNIT label is set where you expect\n\nThis is justified because 5.7.1 auto-update will otherwise fail at task-assembly time.\nPhase one: stabilize updates via compose redeploy\n\nFor each stack:\n\n    Create a systemd .service that runs the external compose provider command you already rely on (podman compose ...). This leverages the fact that podman compose simply passes through to the provider you have installed.\n    Create a matching .timer with an OnCalendar= schedule. Systemd timers are explicitly intended for time-based activation.\n    Implement an update routine:\n        podman compose pull\n        podman compose up -d\n        (Optionally include provider-specific flags to reduce downtime or force recreation, depending on your provider’s behavior—this is provider-defined since podman compose delegates.)\n\nRollback strategy:\n\n    If the update breaks functionality, revert the compose file to the prior image tag/digest and run podman compose up -d again.\n    Because this is compose-driven, rollback is tied to your compose configuration and image tagging policy, not to Podman auto-update state. (This is an operational inference based on compose delegation.)\n\nPhase two: migrate high-value stacks to Quadlet auto-update\n\nFor the stacks where you most want Podman-native auto-update:\n\n    Convert the stack from compose to Quadlet/systemd units (container or pod units as appropriate). Podman documents this systemd integration and AutoUpdate= in podman-systemd.unit(5).\n    Use fully-qualified image references when using AutoUpdate=registry, per the Podman systemd unit documentation.\n    Enable Podman’s update timer or schedule podman auto-update using a systemd timer (the latter is consistent with systemd’s model).\n\nRollback strategy:\n\n    Keep a known-good image tag available and ensure your systemd unit can be reverted to it; Podman auto-update also has explicit rollback logic in its implementation when Rollback is enabled (as seen in code).\n\nDecommission phase\n\nOnce you have either:\n\n    moved the stack to Quadlet, or\n    accepted compose redeploy as your update mechanism,\n\nyou should remove io.containers.autoupdate=registry from containers that are still created via the compat compose path, because it will produce persistent update errors and noise. This follows from the documented failure mode in 5.7.1.","status":"closed","priority":4,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T14:13:06.344511089+08:00","created_by":"abl030","updated_at":"2026-02-15T14:13:17.945953562+08:00","closed_at":"2026-02-15T14:13:17.945953562+08:00","close_reason":"Archived from docs/podman/ removal (commit 38b450e). Historical reference material.","labels":["archive","autoupdate","podman","research"],"dependencies":[{"issue_id":"nixosconfig-2o9.5","depends_on_id":"nixosconfig-2o9","type":"parent-child","created_at":"2026-02-15T14:13:06.346508057+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-2o9.6","title":"Research: Phase 2.5 risk analysis and solutions","description":"Risk and Solutions Analysis for Home-Managed User systemd Stack Units on NixOS\nExecutive summary\n\nThis migration changes the authority boundary for user-unit definitions from a system-owned unit directory (/etc/systemd/user) to a user-owned unit directory (~/.config/systemd/user). In systemd user mode, the user configuration directory has higher precedence than /etc/systemd/user, so a correctly-present, correctly-loaded ~/.config/systemd/user/\u003cunit\u003e.service will override any legacy definition in /etc/systemd/user. However, “correctly present” and “correctly loaded” are non-trivial once you add (a) oneshot + RemainAfterExit=true semantics, (b) autoupdate-induced restarts, (c) Home Manager’s collision guardrails, and (d) the fact that systemd can apply drop-ins from other paths even when the main fragment comes from ~/.config.\n\nThe three unresolved problems you listed map to three distinct reliability gaps:\n\n    Ambiguous restart outcomes (Issue 1) is primarily a semantic mismatch between “systemd unit state” and “container stack health/readiness,” made worse by oneshot semantics where success often means “the CLI returned,” not “the workload is healthy.” systemd explicitly notes oneshot behavior and how RemainAfterExit affects “active” state. Podman auto-update further warns that unit restart can appear successful even if the container fails shortly after, unless readiness is signaled (e.g., SDNOTIFY).\n    Drift recovery (Issue 2) is primarily a Home Manager activation collision policy problem: by design, Home Manager checks for “existing file is in the way” collisions during checkLinkTargets and fails fast, and activation scripts cannot easily “delete first” because collision checks precede the write boundary.\n    Reconciliation timing (Issue 3) is primarily an activation-triggering and daemon reload problem: systemd may continue running with a previously loaded unit until daemon-reload, and systemd exposes NeedDaemonReload to indicate the unit’s FragmentPath/SourcePath changed since last read. Home Manager also has an explicit option to start/stop/reload/enable services at activation time (including an “apply automatically” mode via sd-switch) but that still depends on activations actually running when you need them.\n\nPreferred approaches, aligned with your constraints (user scope, keep Podman auto-update + PODMAN_SYSTEMD_UNIT, minimize toil, stay declarative):\n\n    Issue 1 (restart ambiguity): keep oneshot if you want, but add a hard post-start verification gate (ExecStartPost=) that fails the unit if expected containers are not running/healthy within a bounded timeout; use explicit exit codes and journald-structured logging so automation can treat systemctl --user restart as authoritative. This leverages systemd’s documented failure behavior for ExecStartPre/ExecStartPost.\n    Issue 2 (drift recovery): enable strong auto-heal for just the unit artifacts by setting per-file force = true on the managed unit files (and any managed drop-ins) so activation overwrites local replacements instead of failing; optionally use backupFileExtension only where you truly want backups, since repeated backup collisions are a known sharp edge.\n    Issue 3 (reconcile timing): use systemd.user.startServices = \"sd-switch\" to apply service diffs automatically at activation time, and add a lightweight, non-invasive user-scope auditor timer/service that checks FragmentPath, DropInPaths, and NeedDaemonReload across all stack units and emits actionable alerts (and optionally runs systemctl --user daemon-reload). This avoids activation deadlocks by not attempting to rebuild generations from inside the user manager.\n\nPolicy defaults recommended:\n\n    Issue 1: fail-closed for “restart/apply” (for automation correctness), while ensuring failures are obviously “apply failed, stack may still be running” via logs and a separate “status” probe.\n    Issue 2: auto-heal (overwrite) for unit artifacts; block-and-alert for unexpected drop-ins.\n    Issue 3: block-and-alert on provenance violations; auto-heal only via daemon-reload and “safe restart” when explicitly enabled.\n\nDetailed risk analysis\nIssue 1: Restart visibility ambiguity with oneshot + RemainAfterExit\n\nPrimary root causes\n\nOneshot services behave like short-lived “do something then exit” jobs. systemd documents that oneshot “is similar to exec,” but the manager considers the unit “up” after the main process exits; and RemainAfterExit= is “particularly useful” for oneshot services because it keeps the unit considered active after its processes exit. This is structurally at odds with “a container stack is healthy and serving traffic,” because:\n\n    systemd is tracking the lifecycle of the compose command, not the containers’ processes;\n    podman compose up -d returns when it has submitted the desired state and detached; it does not necessarily mean all containers are healthy at application level; and\n    systemd explicitly notes that for RemainAfterExit=yes, invoking systemctl start again may take no action if the unit is already considered active, which can confuse operators who expect “start” to be an idempotent re-apply.\n\nPodman auto-update adds a second semantic layer: it “restarts containers configured for auto updates” by restarting the systemd units they run in, and it even cautions that determining restart failure is best done via SDNOTIFY-based readiness; “without that, restarting the systemd unit may succeed even if the container has failed shortly after.” Your current model restarts a stack orchestrator unit (oneshot) that then manipulates containers, which is workable but makes Podman’s “restart success” semantics especially easy to misread as “stack is good.”\n\nFailure-mode taxonomy\n\n    False-positive success after restart\n        Mechanism: systemctl --user restart stack.service returns success because ExecStart succeeded and systemd considers the unit active (exited), but one or more containers crash shortly after, or never became ready. systemd tracks the oneshot action, not the container health.\n        Blast radius: automation (CI, cron, “update pipeline”) believes the update succeeded; Podman auto-update may mark update “true” and not roll back if the systemd restart appears successful.\n        Detectability: low without explicit health checks; requires querying container status/health or building readiness gating into the unit.\n        Operator impact: “green” systemd with real stack degradation; delayed incident detection.\n\n    False-negative failure state while stack is still running\n        Mechanism: prestart checks (secrets/env) fail and return non-zero; systemd marks unit failed, even though existing containers (from previous successful run) remain running. This is particularly likely if you add strict ExecStartPre checks that fail fast. systemd behavior: any of ExecStartPre, ExecStart, or ExecStartPost failing (without “-” prefix) makes the unit failed.\n        Blast radius: monitoring mirrors systemd → pages even though service continues; auto-update rollbacks may trigger unnecessarily; operators may attempt panic remediation that actually causes an outage.\n        Detectability: medium if you explicitly log “apply failed; existing containers unchanged” and provide a separate “runtime health” probe.\n        Operator impact: confusion, wasted time, higher chance of operator-induced incidents.\n\n    No-op “start” due to RemainAfterExit\n        Mechanism: unit remains active after a prior successful run; a later “start” does nothing because systemd considers it already running (RemainAfterExit=yes “latches” active).\n        Blast radius: human operators and automation expecting “start == apply” can silently skip applying changes (especially dangerous if a “fix” playbook runs start rather than restart).\n        Detectability: high if you codify “start is ensure-latched; restart is apply,” and alert when start is used in automation.\n\n    Degraded user manager\n        Mechanism: the per-user systemd manager may not exist at boot or persist after logout unless linger is enabled. A user manager instance is created for logged-in users; user services keep running outside a session only if lingering is enabled.\n        Blast radius: auto-update timer firing in user scope may not run; restarts requested by Podman auto-update may fail because there is no user manager to receive them.\n        Detectability: medium—system-wide logs and loginctl state; but frequently mistaken for Podman failures.\n        Operator impact: “works while logged in” class of outages.\n\nIssue 2: Drift recovery gap for ~/.config/systemd/user unit artifacts\n\nPrimary root causes\n\nHome Manager is designed to be conservative about overwriting existing user files. During activation, it runs collision checks (e.g., checkLinkTargets) and errors out if a file exists where Home Manager intends to place a managed symlink. The failure pattern is well documented in real activations: “Existing file ‘…’ is in the way…” and suggestions to move/remove or use backup options.\n\nCritically, Home Manager’s own documentation emphasizes the activation DAG constraint: scripts that cause observable side effects must occur after writeBoundary, while checks like checkLinkTargets run earlier to prevent accidentally deleting user data. That means “auto-delete the drifted file during activation” is intentionally hard unless you opt into an overwrite policy.\n\nHome Manager does provide an overwrite mechanism via per-file force. Community references show the intent clearly: force will unconditionally replace the target, deleting it regardless of whether it is a file or link. MyNixOS option listings also include .force for home.file targets.\n\nFailure-mode taxonomy\n\n    Activation hard-fail due to drift\n        Mechanism: a managed unit file symlink is replaced with a local file; next activation detects collision and fails before applying any changes.\n        Blast radius: prevents rolling out updated unit definitions, container images, or timers; can also block system-level nixos-rebuild switch flows when Home Manager is used as a NixOS module, because the associated Home Manager activation service fails.\n        Detectability: high (explicit activation error).\n        Operator impact: manual cleanup required; may be remote-host hostile.\n\n    Backup-based drift handling fails due to backup collisions\n        Mechanism: backupFileExtension/-b workflow can fail if a backup target already exists (“file.old would be clobbered…”), leaving you back at a hard failure.\n        Blast radius: intermittent failures; operator toil increases because “the backup fix” isn’t stable under repeated churn.\n        Detectability: high (explicit error).\n        Operator impact: fatigue; higher chance of “just delete it” unsafe behavior.\n\n    Silent provenance break if drift prevents updates but old units continue running\n        Mechanism: activation fails, but previously-enabled units continue running with old definitions; container auto-update may keep restarting the old behavior. This can create a split-brain where declarative config says one thing; runtime keeps another.\n        Blast radius: surprises during incident response; irreproducible state.\n        Detectability: medium: requires explicit checks of FragmentPath, hashes, and Home Manager activation failure signals.\n\nIssue 3: Reconciliation timing ambiguity for no-op rebuilds and stale load state\n\nPrimary root causes\n\nThere are two distinct reconciliation layers:\n\n    Filesystem reconciliation: ensure ~/.config/systemd/user/\u003cstack\u003e.service exists and matches the Home Manager generation (not replaced, not missing).\n    systemd load reconciliation: ensure the user manager has reloaded unit metadata and is using the expected unit file; systemd exposes FragmentPath (“the unit file path this unit was read from”) and NeedDaemonReload to indicate that the file changed since last load.\n\nBecause systemd has a defined user unit load path where earlier directories override later ones, the presence/absence of the unit in ~/.config/systemd/user is decisive: if the unit disappears or is replaced incorrectly, systemd can fall back to another path (including /etc/systemd/user).\n\nHome Manager’s behavior at activation time also matters. It has an explicit option systemd.user.startServices controlling whether changed/obsolete services are automatically started/stopped after activation, including an automatic mode using sd-switch (“determines the necessary changes and automatically apply them”). If this is not enabled, “successful activation” may still require manual systemctl --user operations to fully reconcile running services.\n\nFailure-mode taxonomy\n\n    No-op rebuild does not repair drift\n        Mechanism: if a “rebuild” does not run a Home Manager activation step (or runs one that does not rewrite links), drifted files remain and invariants are not restored. This is particularly damaging because operators reasonably assume “rebuild == reconcile.”\n        Blast radius: continued sourcing of wrong unit files; drift accumulates; autoupdate can keep amplifying the wrong behavior.\n        Detectability: medium unless you explicitly audit FragmentPath and the file type (symlink vs regular) of the unit file.\n        Operator impact: seeming randomness: “sometimes rebuild fixes it, sometimes not.”\n\n    Stale loaded units and stale drop-ins\n        Mechanism: systemd can keep a loaded configuration; changes to FragmentPath/SourcePath set NeedDaemonReload=true, signaling that a reload is recommended. If you don’t reload, behavior may remain anchored to prior load state.\n        Mechanism (drop-ins): even if the main unit fragment comes from ~/.config/systemd/user, drop-ins can be sourced elsewhere; systemd exposes DropInPaths, and real systemctl show output demonstrates drop-ins coming from /etc/....\n        Blast radius: unexpected overrides (env, exec lines, dependencies) cause “it’s running but not the config I wrote.”\n        Detectability: high if you routinely inspect DropInPaths and use systemctl show -p FragmentPath.\n        Operator impact: “ghost overrides” and slow root cause analysis.\n\n    User manager not up (or unstable) at the time reconciliation is expected\n        Mechanism: user services usually run only while the user is logged in unless linger is enabled; your homelab likely expects boot-time availability.\n        Blast radius: reconciliation timers don’t fire; auto-update doesn’t fire; stack availability becomes session-dependent.\n        Detectability: medium (depends on system-level log access).\n        Operator impact: inconsistent service uptime.\n\nOption matrix\nIssue 1: Restart visibility ambiguity\nOption\tWhat changes\tSafety (false success/false fail)\tComplexity\tCompatibility with current model\tRollback burden\nKeep oneshot; improve observability only\tStandardize operator guidance (“restart, then verify”); add richer logging and a stackctl status command; do not gate success on readiness\tLow safety: still vulnerable to “restart success but stack unhealthy” because oneshot success is only command success \tLow\tHigh\tVery low\nKeep oneshot; add ExecStartPost readiness gate + strict exit codes\tAdd bounded health/readiness checks in ExecStartPost, and fail service if not running/healthy; preflight checks in ExecStartPre or wrapper; rely on systemd rule that any ExecStartPre/Start/Post failure makes unit failed \tHigh safety for automation; may introduce “unit failed but old containers still running” if preconditions fail\tMedium\tHigh (still oneshot + RemainAfterExit)\tLow\nConvert to Type=notify “latch keeper” wrapper\tKeep user-scope service, but make unit long-running and only declare READY after checks; aligns with Podman guidance that readiness matters for detecting failure \tHighest safety and best semantics; requires maintaining a small resident wrapper process\tMedium-high\tMedium (changes unit semantics; still triggers podman compose up -d)\tMedium\nIssue 2: Drift recovery gap\nOption\tWhat changes\tSafety (data loss vs availability)\tComplexity\tCompatibility\tRollback burden\nFail-closed + alert\tKeep current behavior; add drift detection and paging; require manual cleanup when conflict occurs; relies on Home Manager’s collision checks prior to write boundary \tSafe for user data, risky for availability because activation can hard-fail\tLow-medium\tHigh\tLow\nUse home-manager.backupFileExtension for conflicts\tConfigure automatic rename of conflicting files instead of failing (“move existing files by appending extension rather than exiting with an error”) \tGood availability, but risk of backup collisions (“would be clobbered”) and backup sprawl \tLow\tHigh\tLow\nStrong enforcement via per-file .force = true for unit artifacts\tMark the unit artifacts as “overwrite allowed,” letting activation repair drift by replacing the target (even if it’s a local file); community notes warn this deletes regardless of file/link \tHigh availability; intentional data loss is constrained to “owned artifacts”\tMedium\tHigh\tLow-medium\nIssue 3: Reconciliation timing ambiguity\nOption\tWhat changes\tSafety\tComplexity\tCompatibility\tRollback burden\nManual reconciliation\tDocument/require systemctl --user daemon-reload and service restarts; optionally rely on NeedDaemonReload checks only when debugging \tOperator-dependent; brittle under automation\tLow\tHigh\tVery low\nEnable systemd.user.startServices = \"sd-switch\"\tLet Home Manager automatically start/stop/reload changed services at activation time; sd-switch automatically applies necessary changes \tGood activation-time reconciliation; still depends on activation actually running\tLow-medium\tHigh\tLow\nAdd user-scope audit timer for provenance + reload health\tAdd a user *.timer + oneshot audit service that periodically checks FragmentPath, DropInPaths, NeedDaemonReload; can auto-run daemon-reload and alert on provenance violations; uses systemd properties and path-based activation patterns \tHighest robustness without rebuild recursion; requires linger to be reliable \tMedium\tHigh\tLow-medium\nFinal recommendations\nIssue 1 preferred approach: Keep oneshot, but make “restart == verified apply”\n\nRecommendation\n\nAdopt the “oneshot + verification gate” pattern: keep Type=oneshot and RemainAfterExit=true, but treat systemctl --user restart \u003cstack\u003e.service as an apply-and-verify transaction by adding:\n\n    ExecStartPre= (or wrapper-in-ExecStart) for deterministic preflight checks (secrets/env sanity, compose file presence, registry reachability, lock acquisition).\n    ExecStart= for podman compose up -d --remove-orphans.\n    ExecStartPost= for a bounded readiness check that fails if containers are not running (and optionally not healthy) within a timeout.\n\nSystemd’s semantics are explicit: ExecStartPost runs only after ExecStart is invoked successfully for oneshot (i.e., the last ExecStart= exited successfully), and failure in ExecStartPre/Start/Post causes the unit to be considered failed. This is the cleanest way to make the exit status of systemctl restart meaningful for automation, and it also improves Podman auto-update rollback correctness because Podman notes that restart success can otherwise be a false positive.\n\nPolicy choice\n\n    Fail-closed for apply/restart. A restart that cannot verify readiness should return non-zero and set the unit to failed. This is the only robust automation contract given Podman’s warning about false-positive restart success.\n    Mitigation for “stack still running but unit failed”: log explicitly and provide a dedicated stackctl runtime-status probe so operators can quickly distinguish “apply failed” from “outage.”\n\nIssue 2 preferred approach: Auto-heal drift for unit artifacts via per-file force\n\nRecommendation\n\nUse strong enforcement for the unit artifacts only: set .force = true on the Home Manager-managed files that constitute your ownership invariant (the unit file itself and any .d/ drop-ins you own). The Home Manager ecosystem explicitly recognizes .force as the mechanism to avoid the “existing file is in the way” foot-gun; it is described as unconditionally replacing the target (deleting regardless of file or link).\n\nThis choice directly eliminates the “activation deadlock” class of drift: even if the symlink is replaced by a local file, the next activation overwrites it instead of failing at checkLinkTargets. This aligns with Home Manager’s activation model where collision checks occur early, and thus overwrite intent must be declared up front.\n\nPolicy choice\n\n    Auto-heal (overwrite) for owned artifacts. Your invariant explicitly says “no effective stack unit definition should be sourced from /etc/systemd/user for migrated units,” which is incompatible with allowing manual edits to the owned unit file. Overwrite is the correct policy.\n    Keep fail-closed (no overwrite) for non-owned home files; do not globally enable destructive overwrite because it increases the chance of deleting legitimate user state.\n\nIssue 3 preferred approach: Activation-time reconciliation plus continuous provenance auditing\n\nRecommendation\n\nDo two things:\n\n    Set systemd.user.startServices = \"sd-switch\" so Home Manager activations automatically start/stop/reload systemd user services and stop obsolete services from the previous generation.\n    Add a user-scope audit timer/service that periodically validates provenance and load freshness:\n        FragmentPath must be under ~/.config/systemd/user for migrated units (as per your invariant).\n        DropInPaths must not include unexpected /etc/systemd/user/... drop-ins for migrated units (otherwise you have stale override risk).\n        NeedDaemonReload must be false, or the auditor should run systemctl --user daemon-reload and re-check. systemd explicitly defines NeedDaemonReload and FragmentPath semantics.\n\nThis avoids two fragile patterns: (a) depending on no-op rebuild behavior for drift repair, and (b) trying to rebuild Home Manager generations from inside user services (a common source of deadlocks and recursion).\n\nPolicy choice\n\n    Block-and-alert on provenance violations; auto-heal only safe reload operations. Auto-heal should be limited to daemon-reload and (optionally) reset-failed, not to rebuilding generations or deleting arbitrary files.\n\nImplementation plan\nSystemd and provenance primitives to standardize on\n\nThese diagnostics are the foundation for both enforcement and validation:\n\n    Unit provenance: FragmentPath is the unit file path the unit was read from.\n    Load freshness: NeedDaemonReload indicates the configuration file the unit is loaded from (FragmentPath/SourcePath) changed since the configuration was read, and reload is recommended.\n    Drop-ins: DropInPaths exposes where override fragments are sourced.\n    User unit load order: ~/.config/systemd/user appears before /etc/systemd/user in the user unit load path, and earlier directories override later ones; you can also print the active unit paths with systemd-analyze --user unit-paths.\n    Practical query method: systemctl show -p FragmentPath \u003cunit\u003e is a standard way to locate a unit’s source file.\n\nIssue 1 implementation: Make restart outcomes unambiguous\nPattern A: Verified-apply oneshot\n\nA concrete unit structure (illustrative; adapt paths and naming):\n\nini\n\n[Unit]\nDescription=Homelab stack: foo (rootless Podman Compose)\nAfter=default.target\n\n[Service]\nType=oneshot\nRemainAfterExit=true\n\n# 1) Preflight checks: do not modify the stack, only validate inputs and acquire a lock.\nExecStartPre=/nix/store/...-stackctl/bin/stackctl preflight foo\n\n# 2) Apply desired state.\nExecStart=/nix/store/...-stackctl/bin/stackctl apply foo\n\n# 3) Verify state is actually achieved (bounded wait for \"running\" and optionally \"healthy\").\nExecStartPost=/nix/store/...-stackctl/bin/stackctl verify foo --timeout=60s\n\n# Optional hardening:\nTimeoutStartSec=120\n\nThis design uses systemd’s documented rules that (a) ExecStartPost runs only after ExecStart succeeded for oneshot, and (b) failure in ExecStartPre/Start/Post causes unit failure.\nThe stackctl contract\n\nstackctl should be the single source of truth for exit codes and journald messaging:\n\n    stackctl preflight:\n        verification of required secret files/env files (existence, permissions),\n        verification that the compose file path exists,\n        lock acquisition (e.g., flock on a per-stack lock) to prevent concurrent operator restart vs auto-update restart.\n    stackctl apply:\n        run podman compose up -d --remove-orphans with explicit project name and deterministic file set.\n    stackctl verify:\n        query expected container set (label filter tied to compose project),\n        ensure each is running,\n        if healthchecks exist, optionally wait for healthy,\n        fail after timeout.\n\nThis directly addresses Podman’s auto-update warning about restart success not implying the workload is actually running/ready.\nPolicy knob: preserve “start is no-op” but make “restart” authoritative\n\nKeep the semantic rule: start = ensure-latched; restart = apply. systemd explicitly notes that for RemainAfterExit=yes, calling systemctl start again may take no action. That is acceptable as long as your tooling and automation always uses restart for apply and uses your verification gate.\nIssue 2 implementation: Auto-heal drift without failing activation\nPreferred: per-file overwrite for unit artifacts\n\nFor the specific unit files and drop-ins you consider “owned,” enable overwrite. The Home Manager community references show .force exists and is intended for cases where external software overwrites managed files; it will replace the target even if it is a file or link.\n\nImplementation has two common shapes:\n\n    If you manage unit files via xdg.configFile or home.file, set:\n        xdg.configFile.\"systemd/user/foo.service\".force = true; or\n        home.file.\".config/systemd/user/foo.service\".force = true;\n\nMyNixOS option listings confirm .force exists for managed files.\nBackup option as a secondary safety net\n\nIf you have a strong requirement to preserve overwritten local files, home-manager.backupFileExtension is the conservative mechanism: it moves existing conflicting files aside by appending an extension rather than failing.\n\nHowever, treat it as a secondary mechanism because backup workflows can themselves fail when a backup file already exists (“would be clobbered”), and this can reintroduce activation deadlocks and operator toil.\nIssue 3 implementation: Deterministic reconciliation without deadlocks\nActivation-time service reconciliation\n\nEnable Home Manager’s systemd service reconciliation:\n\n    systemd.user.startServices = \"sd-switch\";\n\nThis option exists specifically to “start new or changed services that are wanted by active targets” and “stop obsolete services” after activation; sd-switch is described as automatically determining and applying necessary systemd changes.\n\nThis reduces the chance of “unit file on disk changed but service not restarted.”\nContinuous provenance and freshness auditing via user timers\n\nAdd a user-scope auditor:\n\n    homelab-provenance-audit.service (oneshot)\n    homelab-provenance-audit.timer (e.g., every 5–15 minutes)\n\nUse systemd’s own properties:\n\n    FragmentPath to verify the unit is loaded from ~/.config/systemd/user.\n    DropInPaths to detect unexpected legacy drop-ins.\n    NeedDaemonReload to detect stale-loaded units and optionally trigger daemon-reload.\n\nThis auditor should not attempt to rebuild Home Manager generations (to avoid recursion), but it can safely run:\n\n    systemctl --user daemon-reload if NeedDaemonReload=yes\n    systemctl --user reset-failed \u003cunit\u003e when appropriate\n    emit structured logs and (optionally) exit non-zero to integrate with alerting pipelines\n\nOptional: path-based activation for on-disk drift signals\n\nIf you want faster detection than polling, systemd supports path-based activation using .path units. In user scope, you can monitor:\n\n    ~/.config/systemd/user/foo.service\n    ~/.config/systemd/user/foo.service.d/\n\nand trigger the auditor immediately when changes occur. This helps catch manual edits quickly and ensures daemon-reload happens promptly.\nCross-cutting: Ensure user manager availability for a homelab\n\nBecause your stacks and timers run in user scope, you must treat “user manager availability” as a hard dependency:\n\n    systemd starts separate user manager instances for logged-in users.\n    user services run only while logged in unless linger is enabled; linger causes a user manager to be created at boot and persist beyond sessions.\n\nIn NixOS terms, make linger declarative for the service account that owns the stacks (implementation detail depends on your NixOS user config pattern).\nValidation plan\nDeterministic local test matrix\n\nThe goal is to validate: (a) restart semantics, (b) drift handling, (c) reconciliation under stale user manager state, (d) Podman auto-update integration.\n\nFor each test, collect:\n\n    systemctl --user show \u003cunit\u003e -p FragmentPath -p DropInPaths -p NeedDaemonReload -p ActiveState -p SubState -p Result -p ExecMainStatus (authoritative properties; FragmentPath and NeedDaemonReload semantics are defined by systemd).\n    journalctl --user -u \u003cunit\u003e for the apply/verify logs.\n    podman auto-update --dry-run and podman auto-update output; Podman documents the UPDATED field and that it restarts the systemd unit executing the container.\n    podman ps / podman inspect fields relevant to health and labels.\n\nRestart semantics\n\n    Preflight failure\n        Induce: remove required secret/env file.\n        Execute: systemctl --user restart stack.service.\n        Pass/fail:\n            PASS: systemctl returns non-zero; unit ActiveState=failed and journal explains missing secret; containers from previous run remain present (verified via podman ps).\n            Confirm ExecStartPre/ExecStartPost failure causes unit failure (systemd documented behavior).\n\n    Apply success + verify success\n        Induce: all dependencies present.\n        Execute: systemctl --user restart stack.service.\n        Pass/fail:\n            PASS: exit code 0; ActiveState=active, SubState=exited for oneshot; verify log indicates all containers running/healthy. (Oneshot/RemainAfterExit semantics documented.)\n\n    Apply “succeeds” but verify fails\n        Induce: break a container so it immediately exits or becomes unhealthy.\n        Execute: restart.\n        Pass/fail:\n            PASS: unit fails in ExecStartPost path; automation sees failure exit; aligns with systemd rule that failing start/post fails the unit.\n\nDrift handling\n\n    Replace symlink with local file\n        Induce: overwrite ~/.config/systemd/user/stack.service with a regular file.\n        Execute: Home Manager activation (the same mechanism you use in production, e.g., via nixos-rebuild switch).\n        Pass/fail:\n            PASS (with .force=true): activation does not fail; file restored to managed form; unit FragmentPath still resolves under ~/.config/systemd/user. (FragmentPath definition is explicit.)\n            FAIL (without .force): reproduce “Existing file … is in the way” during checkLinkTargets.\n\nReconciliation timing\n\n    Stale loaded unit\n        Induce: change the unit file on disk and do not run daemon-reload.\n        Validate:\n            NeedDaemonReload=yes should appear.\n        Execute: run auditor; auditor runs systemctl --user daemon-reload.\n        Pass/fail:\n            PASS: after auditor, NeedDaemonReload=no, and subsequent restart uses the new unit behavior.\n\n    Stale drop-in\n        Induce: create a legacy drop-in in /etc/systemd/user/stack.service.d/override.conf.\n        Validate:\n            DropInPaths includes the path.\n        Pass/fail:\n            PASS: auditor alerts (and optionally fails) because drop-ins violate ownership invariant even if FragmentPath is correct.\n\nPodman auto-update integration\n\n    Dry run behavior\n        Execute: podman auto-update --dry-run --format \"{{.Image}} {{.Updated}}\".\n        Pass/fail: output shows pending when updates exist (documented).\n\n    End-to-end update triggers correct unit\n        Induce: push a new image to your controlled local registry.\n        Execute: podman auto-update.\n        Pass/fail:\n            PASS: Podman reports the expected UNIT for the container and restarts it; Podman documents that it restarts the systemd unit executing the container when an image is updated.\n\n    Rollback correctness under failed restart\n        Induce: make verification fail after update (e.g., health check fails).\n        Execute: podman auto-update with default rollback behavior.\n        Pass/fail:\n            PASS: auto-update detects failure to restart and rolls back (Podman documents rollback behavior and the caveat about readiness detection).\n\nProduction-host validation\n\nProduction patterns should emphasize signals, not hope:\n\n    Daily provenance report\n        Use auditor output to produce a daily “all stacks provenance OK” signal.\n        If any unit fails invariants:\n            FragmentPath not under ~/.config/systemd/user (violation; indicates fallback to other unit path).\n            unexpected DropInPaths under /etc/systemd/user (stale override).\n            NeedDaemonReload=yes beyond a grace period (stale load state).\n\n    Alerting requirements\n        Page on:\n            verification-gated restart failures (Issue 1), because they imply “desired state not achieved.”\n            provenance violations (Issue 3), because they imply loss of declarative authority.\n        Ticket (non-page) on:\n            repeated drift repairs (Issue 2) to identify the drift source.\n\nRollback and recovery plan\nStaged rollout sequence\n\nBecause user unit load order favors ~/.config/systemd/user over /etc/systemd/user, you can stage changes safely while keeping an escape hatch.\n\n    Stage 1: Observability first\n        Deploy the auditor service/timer in user scope.\n        Deploy stackctl status tooling (read-only).\n        No change in restart semantics yet; measure current false-positive rate.\n\n    Stage 2: Enable reconciliation (sd-switch)\n        Enable systemd.user.startServices = \"sd-switch\" so activations reconcile services automatically.\n        Validate on a canary host that activations do not cause unexpected restarts beyond what you intend.\n\n    Stage 3: Drift auto-heal for unit artifacts\n        Set .force=true only for unit artifacts.\n        Validate drift scenario: replace a unit file with local file; confirm activation does not fail.\n\n    Stage 4: Verified-apply restarts\n        Introduce ExecStartPost verification gates and deterministic exit codes.\n        Validate Podman auto-update rollback flow (your controlled registry test) again under the new gating logic.\n\nRecovery playbooks by failure mode\n\nFailure mode: unit unexpectedly sourced from /etc/systemd/user\n\n    Symptoms\n        Auditor flags FragmentPath not under ~/.config/systemd/user.\n    Immediate recovery\n        Run systemctl --user daemon-reload and re-check FragmentPath. (NeedDaemonReload/reload semantics are explicit.)\n        Ensure the file exists under ~/.config/systemd/user and is not masked.\n    Root cause isolation\n        Print effective user unit paths: systemd-analyze --user unit-paths.\n        Check for missing file, wrong name, or removed file due to drift.\n\nFailure mode: activation fails due to file collisions\n\n    Symptoms\n        Home Manager service shows “Existing file … is in the way…” during checkLinkTargets.\n    Immediate recovery\n        If .force=true is not yet deployed for the file, temporarily move the file aside manually and rerun activation.\n        If using backups, watch for backup collisions (“would be clobbered”); if encountered, delete/rename the conflicting backup and rerun.\n    Long-term fix\n        Enable .force=true for the artifact category that is expected to be immutable (unit files and owned drop-ins).\n\nFailure mode: restart succeeds but service is actually down\n\n    Symptoms\n        Containers not running/healthy, but unit shows active (exited) and restart returned 0 (pre-gating scenario). Oneshot state doesn’t imply workload health.\n    Immediate recovery\n        Manually run stackctl verify (or equivalent) and restart with systemctl --user restart.\n    Long-term fix\n        Deploy ExecStartPost readiness gating so this becomes a “restart failed” event instead of silent success.\n\nFailure mode: user manager not running (timers/services not firing)\n\n    Symptoms\n        User timers not running; stacks only run while logged in.\n    Immediate recovery\n        Log in and start user services.\n    Long-term fix\n        Enable linger for the service account so the user manager exists at boot and persists; system documentation and common guidance note this requirement for non-interactive user services.","status":"closed","priority":4,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T14:13:07.604878164+08:00","created_by":"abl030","updated_at":"2026-02-15T14:13:17.954542143+08:00","closed_at":"2026-02-15T14:13:17.954542143+08:00","close_reason":"Archived from docs/podman/ removal (commit 38b450e). Historical reference material.","labels":["archive","podman","research","risk-analysis"],"dependencies":[{"issue_id":"nixosconfig-2o9.6","depends_on_id":"nixosconfig-2o9","type":"parent-child","created_at":"2026-02-15T14:13:07.608739788+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-2o9.7","title":"Decision: Container lifecycle strategy (no deploy-time --wait)","description":"# Decision: Container Lifecycle Strategy for Rebuild vs Auto-Update\n\n**Date:** 2026-02-12\n**Status:** Implemented and evolved (Phase 1 + Phase 2 complete; ownership follow-up moved to Phase 2.5)\n**Related Beads:** nixosconfig-cm5 (research), nixosconfig-hbz (bug fix)\n**Research Document:** [container-lifecycle-analysis-2026-02.md](../research/container-lifecycle-analysis-2026-02.md)\n**Empirical Test:** [2026-02-13-compose-change-propagation-test.md](../incidents/2026-02-13-compose-change-propagation-test.md)\n**Ownership follow-up decision:** [2026-02-13-home-manager-user-unit-ownership.md](2026-02-13-home-manager-user-unit-ownership.md)\n**Implementation:** [stacks/lib/podman-compose.nix](../../../stacks/lib/podman-compose.nix)\n\n## Planned Follow-up Trial (2026-02)\n\nWe will trial a simplified reliability model to reduce control-plane coupling:\n- Keep container `healthcheck` definitions in compose.\n- Stop using `compose --wait` as a host activation gate.\n- Continue enforcing `io.containers.autoupdate=registry` + `PODMAN_SYSTEMD_UNIT` as a hard invariant.\n- Prefer failure surfacing via user service state and monitoring alerts over blocking `nixos-rebuild switch`.\n\nIntent:\n- Improve deploy robustness by decoupling host config activation from slow/stuck runtime health transitions.\n- Reduce reliance on preflight remediation scripts where possible after the trial.\n\nStatus:\n- Phase 1 implemented: deploy path no longer uses compose `--wait`.\n- Phase 2 implemented: user service is sole lifecycle owner with native `sops.secrets` wiring.\n- Existing hardening/invariant behavior documented below remains current.\n\n## Update (2026-02-13)\n\nAdditional hardening was applied after the initial rollout:\n\n- Startup timeout is globally bounded at 5 minutes (`startupTimeoutSeconds ? 300`) for both the system secrets unit and user compose unit.\n- Compose deploy path now runs without `--wait`; timeout bounds still apply to service activation/retry behavior.\n- Stale health and label mismatch checks now query both project label families:\n  - `io.podman.compose.project`\n  - `com.docker.compose.project`\n- `StartedAt` timestamp parsing is normalized before `date -d` to avoid timezone-name parsing failures.\n- Label mismatch behavior is hard-fail by design (containers are removed when `PODMAN_SYSTEMD_UNIT` does not match expected ownership).\n\n## Phase 2 Completion Update (2026-02-13)\n\nPhase 2 control-plane simplification is complete and deployed to `igpu` and `doc1`.\n\nWhat changed:\n- Removed orchestration role of `*-stack-secrets.service`.\n- Removed root-side bounce path (`runuser ... systemctl --user restart ...`).\n- User service `${stackName}.service` is now the sole stack lifecycle owner.\n- Env secret delivery now uses native system-scope `sops.secrets` with runtime-readable ownership.\n- Added one-release compatibility fallback for legacy env paths with explicit warning logs.\n- Missing secrets remain hard-fail for stack startup.\n- Deploy path remains non-blocking (`podman compose up -d --remove-orphans`, no `--wait`).\n\n## Empirical Addendum (2026-02-13, `igpu`)\n\nFacts from an explicit compose-change propagation test are captured in:\n- [2026-02-13-compose-change-propagation-test.md](../incidents/2026-02-13-compose-change-propagation-test.md)\n\nObserved:\n- A compose change produced updated unit/script artifacts in the new NixOS generation.\n- The active user manager can continue to run a stale unit from `~/.config/systemd/user` when such an override path exists.\n- In that state, service restart behavior follows the stale user-level unit definition rather than the updated `/etc/systemd/user` unit.\n\nDecision update:\n- The earlier \"dual service architecture is required\" conclusion is superseded by the Phase 2 design.\n- Current design keeps hard-fail invariant enforcement while reducing orchestration coupling.\n\n## Phase 2.5 Update (2026-02-13)\n\nPhase 2 proved the simplified user-scope lifecycle model, but the `igpu` propagation test exposed an ownership-collision class:\n\n- NixOS-generated stack user units under `/etc/systemd/user` can be shadowed by stale user-level unit files in `~/.config/systemd/user`.\n- In that condition, `daemon-reload` and `restart` continue using the higher-precedence stale user-level unit.\n\nPhase 2.5 decision:\n\n1. Keep stack lifecycle in user scope.\n2. Migrate stack unit ownership to Home Manager `systemd.user.services` so authoritative unit definitions live in user-level path.\n3. Enforce single ownership per unit name and add post-switch `FragmentPath`/`DropInPaths` checks.\n4. Treat user-manager availability as an explicit reconciliation gate (do not treat skipped user reload as silent success).\n\nSee:\n- Decision: `docs/podman/decisions/2026-02-13-home-manager-user-unit-ownership.md`\n- Plan: `docs/podman/current/phase2.5-home-manager-migration-plan.md`\n- Research: `docs/podman/research/home-manager-user-service-migration-research-2026-02.md`\n\n## Incident Addendum (2026-02-13)\n\nObserved production failure mode in the auto-update window:\n\n- `doc1` (`proxmox-vm`) at ~`00:06` AWST: `52` errors\n- `igpu` at ~`00:14` AWST: `13` errors\n- Common error: `no PODMAN_SYSTEMD_UNIT label found`\n\nDiagnosis summary:\n\n- Affected containers were labeled `io.containers.autoupdate=registry`.\n- Those same containers lacked `PODMAN_SYSTEMD_UNIT`.\n- Result: `podman auto-update` had no systemd restart target and failed per-container.\n\nDecision reinforcement:\n\n- Treat label pairing as a strict invariant, enforced in preflight:\n  - `io.containers.autoupdate=registry` =\u003e `PODMAN_SYSTEMD_UNIT` required.\n- Violation is a hard startup failure for the stack.\n- Goal: fail fast at deploy/start time, not later during timer-based auto-update.\n\n## Context\n\nDuring Phase 1 migration from `podman-compose` to `podman compose`, we encountered stale container health check issues that caused deployments to hang indefinitely. This raised questions about whether our dual service architecture was correct and whether we should use different strategies for rebuild vs auto-update scenarios.\n\n## Questions Answered\n\n1. **Does container reuse with `--wait` cause stale health checks?**\n   → YES - confirmed via research, documentation, and production experience\n\n2. **Should rebuild and auto-update use different strategies?**\n   → They ALREADY DO - dual services are correctly optimized for their use cases\n\n3. **Is the dual service architecture necessary?**\n   → YES - each service serves a distinct purpose with appropriate optimizations\n\n4. **Should we use `--force-recreate` to avoid stale containers?**\n   → NO - defeats the purpose of incremental rebuilds; use targeted detection instead\n\n## Decision (Historical Snapshot, Later Evolved by Phase 2)\n\n**Keep current dual service architecture with targeted stale health detection.**\n\n### Rationale\n\n**Finding #1: Dual Services Are Correct By Design**\n\n```\nSystem Service (\u003cstack\u003e-stack.service):\n  - Triggered by: nixos-rebuild switch\n  - Purpose: Apply config changes incrementally\n  - Strategy: Smart container reuse (fast, only restart what changed)\n  - Optimization: Preserves containers when config unchanged\n\nUser Service (\u003cstack\u003e.service):\n  - Triggered by: podman auto-update → systemd restart\n  - Purpose: Pull new images, deploy updates\n  - Strategy: Full recreation (systemd ExecStop → ExecStart lifecycle)\n  - Optimization: Fresh containers with new images (Watchtower-style)\n```\n\n**Finding #2: User Services Already Recreate Containers**\n\nUser services don't need `--force-recreate` because systemd's service lifecycle already provides full recreation:\n1. Systemd runs ExecStop (stops containers)\n2. Then runs ExecStart (creates fresh containers)\n3. Result: Clean slate every auto-update\n\nThis discovery eliminated the need to change user service behavior.\n\n**Finding #3: Stale Health is a Targeted Problem**\n\nThe issue only occurs during rebuild when:\n- Container has stuck health check from previous run\n- Config is unchanged (so docker-compose reuses container)\n- `--wait` blocks on stale health status\n- No new health checks are scheduled\n\n**Finding #4: Targeted Remediation Beats Blanket Workaround**\n\nUsing `--force-recreate` on system service would:\n- ❌ Restart ALL containers on every rebuild (slow)\n- ❌ Cause unnecessary downtime\n- ❌ Defeat the purpose of incremental config changes\n- ❌ Waste time recreating healthy containers\n\nTargeted stale health detection:\n- ✅ Only removes broken containers\n- ✅ Preserves fast path for healthy containers\n- ✅ Automatic remediation (no manual intervention)\n- ✅ Low overhead (quick inspect check)\n\n## Implementation\n\n**Status:** ✅ Completed (initial: e194187; hardened: f922af4, 003e8b1)\n\n### Stale Health Detection\n\n**Location:** `stacks/lib/podman-compose.nix` (`healthCheckTimeout`, `startupTimeoutSeconds`, `detectStaleHealthScript`, `recreateIfLabelMismatchScript`)\n\n```nix\n# Add parameter to mkSystemdService function:\nhealthCheckTimeout ? 90  # Default 90 seconds, configurable per-stack\n\ndetectStaleHealth = [\n  ''\n    /run/current-system/sw/bin/sh -c '\n      ids=$(\n        {\n          ${podmanBin} ps -a --filter label=io.podman.compose.project=${projectName} --format \"{{.ID}}\"\n          ${podmanBin} ps -a --filter label=com.docker.compose.project=${projectName} --format \"{{.ID}}\"\n        } | /run/current-system/sw/bin/awk 'NF' | /run/current-system/sw/bin/sort -u\n      )\n      for id in $ids; do\n        health=$(${podmanBin} inspect -f \"{{.State.Health.Status}}\" $id 2\u003e/dev/null || echo \"none\")\n        started=$(${podmanBin} inspect -f \"{{.State.StartedAt}}\" $id 2\u003e/dev/null)\n\n        # Only remove if unhealthy/starting AND running for \u003ethreshold\n        if [ \"$health\" = \"starting\" ] || [ \"$health\" = \"unhealthy\" ]; then\n          started_clean=$(echo \"$started\" | /run/current-system/sw/bin/awk '{print $1, $2, $3}')\n          age_seconds=$(( $(date +%s) - $(date -d \"$started_clean\" +%s) ))\n          if [ $age_seconds -gt ${toString healthCheckTimeout} ]; then\n            echo \"Removing container $id with stale health ($health) - running for ${age_seconds}s (threshold: ${toString healthCheckTimeout}s)\"\n            ${podmanBin} rm -f $id\n          else\n            echo \"Container $id is $health but only ${age_seconds}s old - allowing more time (threshold: ${toString healthCheckTimeout}s)\"\n          fi\n        fi\n      done\n    '\n  ''\n];\n```\n\n**Edge Case Protection:**\n- **Default 90 seconds:** Covers most services (2-3x typical 30-45s startup time)\n- **Rapid rebuilds safe:** Won't get stuck in multi-minute loops during development\n- **Configurable per-stack:** Slow services can override (e.g., `healthCheckTimeout = 300` for database migrations)\n- **Formula:** Set to 2-3x expected startup time for the slowest container in the stack\n\nAdd to `mkExecStartPre` call:\n```nix\nExecStartPre = mkExecStartPre envFiles (podPrune ++ detectStaleHealth ++ recreateIfLabelMismatch ++ preStart);\n```\n\n### Verification Steps\n\n**Phase 1: Dev Environment Validation**\n\n1. **Create test stack with intentional failure:**\n   ```bash\n   # On dev VM\n   cat \u003e /tmp/test-health-compose.yml \u003c\u003cEOF\n   services:\n     test-slow:\n       image: nginx:alpine\n       healthcheck:\n         test: [\"CMD\", \"sleep\", \"999\"]  # Always times out\n         interval: 30s\n         timeout: 10s\n         start_period: 10s\n     test-healthy:\n       image: nginx:alpine\n       healthcheck:\n         test: [\"CMD\", \"curl\", \"-f\", \"http://localhost\"]\n         interval: 30s\n         timeout: 5s\n   EOF\n   podman compose -f /tmp/test-health-compose.yml up -d\n   ```\n\n2. **Wait for stuck state (2+ minutes):**\n   ```bash\n   watch 'podman ps -a --format \"{{.Names}}: {{.Status}}\"'\n   # test-slow should show \"Up X minutes (health: starting)\"\n   # After 90+ seconds, detection script should remove it\n   ```\n\n3. **Run detection script manually:** Verify it identifies stuck container but NOT the healthy one\n\n4. **Test in NixOS stack:** Add test stack to doc1, trigger rebuild, verify auto-remediation\n\n**Phase 2: Production Validation**\n\n1. Deploy to doc1, monitor first rebuild\n2. Check journalctl for detection messages\n3. Verify no false positives (healthy containers removed)\n4. Deploy to igpu during migration (expect same issues, verify auto-remediation)\n\n**Success Criteria:**\n- ✅ Containers stuck \u003e90s are removed\n- ✅ Containers \u003c90s in \"starting\" state are NOT removed (unless overridden per-stack)\n- ✅ Healthy containers are never touched\n- ✅ Clear logging shows what was removed and why\n- ✅ Rapid rebuilds don't get stuck in multi-minute loops\n\n## Alternative Approaches Considered\n\n### Option A: --force-recreate on System Service\n**Decision:** REJECTED\n\nWould solve stale health issue but:\n- Defeats purpose of incremental rebuilds\n- Causes unnecessary downtime\n- Slower deployments (2-5s per container × 19 stacks)\n- Restarts healthy containers for no reason\n\n### Option B: --force-recreate on User Service\n**Decision:** NOT NEEDED\n\nUser services already recreate via systemd lifecycle (ExecStop → ExecStart). Adding the flag would be redundant.\n\n### Option C: Separate compose files for rebuild vs update\n**Decision:** REJECTED\n\nWould add complexity without solving the root cause. Both scenarios need the same container configuration.\n\n### Option D: Remove --wait flag\n**Decision:** REJECTED\n\nWould lose critical benefits:\n- Can't detect deployment failures\n- Auto-update can't detect rollbacks\n- Services might depend on broken stacks\n- No reliable success/failure indication\n\n## Lessons from Watchtower\n\nWatchtower's always-recreate approach worked reliably for years. However:\n- Watchtower only handles auto-update scenario (not rebuild)\n- Our user services already implement Watchtower-style recreation\n- System services need different optimization (incremental, not full recreation)\n- The dual service architecture gives us the best of both worlds\n\n## Success Criteria\n\n1. ✅ Rebuild deployments don't hang on stale health checks\n2. ✅ Healthy containers are reused (fast path preserved)\n3. ✅ Auto-update continues to work reliably\n4. ✅ No manual intervention needed for stuck containers\n5. ✅ Clear logging when stale containers are removed\n\n## Test Matrix (Invariant Enforcement)\n\nAll tests run on both hosts: `doc1` and `igpu`.\n\n1. **Baseline audit (pre-change snapshot)**\n- Record current auto-update label pairing:\n  - `podman ps -a --format '{{.ID}}' | xargs -r podman inspect | jq -r '.[] | select(.Config.Labels[\"io.containers.autoupdate\"]==\"registry\") | \"\\(.Name) \\(.Config.Labels[\"PODMAN_SYSTEMD_UNIT\"] // \"MISSING\")\"'`\n- Expected: no `MISSING` lines after rollout.\n\n2. **Negative test (must hard-fail)**\n- Create one controlled mismatch in a test stack/container:\n  - `io.containers.autoupdate=registry` set\n  - `PODMAN_SYSTEMD_UNIT` absent\n- Start stack service.\n- Expected: service fails in `ExecStartPre` with explicit mismatch output.\n\n3. **Positive test (must pass)**\n- Add valid `PODMAN_SYSTEMD_UNIT=\u003cstack\u003e.service` for same test.\n- Start stack service again.\n- Expected: preflight passes and service reaches active/exited success state.\n\n4. **Auto-update dry run validation**\n- Run `podman auto-update --dry-run`.\n- Expected: no `no PODMAN_SYSTEMD_UNIT label found` errors.\n\n5. **Timer-path validation**\n- Let scheduled `podman-auto-update.service` run once (or trigger manually in equivalent path).\n- Expected: no missing-label errors in `journalctl -u podman-auto-update.service`.\n\n6. **Regression guard for compose label families**\n- Ensure preflight coverage includes both:\n  - `io.podman.compose.project`\n  - `com.docker.compose.project`\n- Expected: mismatch detection works regardless of which project label family the container has.\n\n## References\n\n- Full research: [container-lifecycle-analysis-2026-02.md](../research/container-lifecycle-analysis-2026-02.md)\n- Research bead: `bd show nixosconfig-cm5`\n- Bug bead: `bd show nixosconfig-hbz`\n- CLAUDE.md: Container Stack Management section\n\n## Next Steps\n\n1. Implement `detectStaleHealth` in `stacks/lib/podman-compose.nix`\n2. Test on doc1 with controlled scenario\n3. Deploy to production, monitor for issues\n4. Apply to igpu during migration\n5. Document health check best practices in stack templates (Recommendation 2)\n6. Consider health check monitoring (Recommendation 5, low priority)","status":"closed","priority":4,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T14:13:08.866525574+08:00","created_by":"abl030","updated_at":"2026-02-15T14:13:17.964209418+08:00","closed_at":"2026-02-15T14:13:17.964209418+08:00","close_reason":"Archived from docs/podman/ removal (commit 38b450e). Historical reference material.","labels":["archive","decision","podman"],"dependencies":[{"issue_id":"nixosconfig-2o9.7","depends_on_id":"nixosconfig-2o9","type":"parent-child","created_at":"2026-02-15T14:13:08.869265057+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-2o9.8","title":"Decision: Home Manager user unit ownership model","description":"# Decision: Home Manager Ownership for Stack User Units\n\n**Date:** 2026-02-13  \n**Status:** Accepted (implementation in code complete; rollout pending)  \n**Scope:** Stack lifecycle user units (`${stackName}.service`)  \n**Primary research:** [home-manager-user-service-migration-research-2026-02.md](../research/home-manager-user-service-migration-research-2026-02.md)  \n**Related incident:** [2026-02-13-compose-change-propagation-test.md](../incidents/2026-02-13-compose-change-propagation-test.md)\n\n## Context\n\n`igpu` propagation testing showed a reproducible failure class where stale unit files under `~/.config/systemd/user` override updated stack units generated by NixOS under `/etc/systemd/user`.\n\nThis is consistent with user unit search path precedence and explains why `systemctl --user daemon-reload \u0026\u0026 systemctl --user restart \u003cunit\u003e` can continue executing stale definitions when a higher-precedence unit file remains present.\n\n## Decision\n\n1. Keep stack lifecycle in user scope (no move to system service ownership).\n2. Migrate stack unit ownership from NixOS `/etc/systemd/user` generation to Home Manager `systemd.user.services` generation.\n3. Enforce single ownership for each stack unit name (no simultaneous definitions across `/etc/systemd/user` and `~/.config/systemd/user`).\n4. Add post-switch ownership assertions using:\n   - `systemctl --user show \u003cunit\u003e -p FragmentPath -p DropInPaths`\n5. Treat \"user manager unavailable/unreachable\" as a reconciliation failure signal that must be surfaced explicitly.\n\n## Rationale\n\n1. This migration materially mitigates the exact ownership-collision failure observed in production-style testing.\n2. It keeps lifecycle control in user scope, preserving current Podman auto-update model and labeling invariants.\n3. It does not assume Home Manager is a universal cure; user-manager availability and drop-in drift remain explicit residual risks and are included in Phase 2.5 test gates.\n\n## Consequences\n\n### Positive\n\n1. Eliminates the most likely `/etc` vs `~/.config` shadowing collision for stack unit definitions.\n2. Clarifies stack-unit source of truth.\n3. Aligns unit ownership with user-scoped control plane behavior.\n\n### Residual Risks\n\n1. Home Manager skips service switching if user systemd manager is not running/reachable.\n2. User-level drop-ins and transient/control paths can still alter effective unit behavior.\n3. Migration must include cleanup of old ownership artifacts to avoid transitional conflicts.\n\n## Implementation Link\n\n- Active implementation plan: `docs/podman/current/phase2.5-home-manager-migration-plan.md`","status":"closed","priority":4,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T14:13:10.104563265+08:00","created_by":"abl030","updated_at":"2026-02-15T14:13:17.972337761+08:00","closed_at":"2026-02-15T14:13:17.972337761+08:00","close_reason":"Archived from docs/podman/ removal (commit 38b450e). Historical reference material.","labels":["archive","decision","home-manager","podman"],"dependencies":[{"issue_id":"nixosconfig-2o9.8","depends_on_id":"nixosconfig-2o9","type":"parent-child","created_at":"2026-02-15T14:13:10.107798816+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-2ws","title":"Reference: Standard Kuma Health Endpoints","description":"Reference documentation for standard Uptime Kuma health check endpoints.\n\n## Standard Kuma Health Endpoints\n\n- Immich: `/api/server/ping`\n- Plex: `/identity`\n- Mealie: `/api/app/about`\n- Jellyfin: `/System/Info/Public`\n- Smokeping: `/smokeping/smokeping.cgi`\n- Others: keep `/` unless a documented unauthenticated endpoint exists.","status":"closed","priority":4,"issue_type":"chore","created_at":"2026-02-11T20:35:37.504515938+08:00","updated_at":"2026-02-11T20:36:04.145275329+08:00","closed_at":"2026-02-11T20:36:04.145275329+08:00","close_reason":"Reference doc — content moved from CLAUDE.md","labels":["kuma","monitoring","reference"]}
{"id":"nixosconfig-3fs","title":"pin ha-mcp fastmcp\u003c3 — revert when upstream fixes #645","description":"ha-mcp 6.7.0 crashes silently with fastmcp 3.0.0 (show_cli_banner renamed to show_server_banner, _tool_manager removed). Pinned fastmcp\u003c3 in scripts/mcp-homeassistant.sh. Upstream issues: #644, #645, #648. Revert the pin once ha-mcp releases a version that supports fastmcp\u003e=3.","notes":"## MCP Resilience Analysis\n\n**Nix-managed (pinned, safe):** loki, pfsense, unifi, lidarr, slskd, vinsight — installed via Home Manager, deps locked by nix.\n\n**uvx runtime-resolved (vulnerable to upstream breakage):** ha-mcp (pinned fastmcp\u003c3 as workaround), mcp-nixos, prometheus-mcp-server, beads-mcp.\n\n**Failure mode:** uvx resolves latest compatible version at install/cache-refresh time. If an upstream dep ships a breaking change without proper version bounds (like fastmcp 3.0 did), the MCP server crashes silently — Claude Code just shows 'no tools found' with no hint why.\n\n**Mitigation:** Pin deps in wrapper scripts when breakage occurs. Don't over-engineer — version pins are sufficient for third-party MCPs.","status":"closed","priority":3,"issue_type":"bug","owner":"abl030@gmail.com","created_at":"2026-02-19T18:39:31.748210579+08:00","created_by":"abl030","updated_at":"2026-02-19T19:53:48.037737644+08:00","closed_at":"2026-02-19T19:51:06.249175136+08:00","close_reason":"Fixed by pinning fastmcp\u003c3 in wrapper script. Committed as 62cbe66. Nixification tracked in nixosconfig-auy.","dependencies":[{"issue_id":"nixosconfig-3fs","depends_on_id":"nixosconfig-fah","type":"blocks","created_at":"2026-02-19T18:39:37.39221002+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-3mg","title":"Phase 3: Simplify auto-update wrapper (podman)","description":"Clean up auto-update wrapper now that auto-update works natively with user services.\n\n## Changes\n\n1. **Rewrite auto-update script:**\n   - Use podman auto-update --format \"{{.Unit}} {{.Image}} {{.Updated}}\"\n   - Parse for failed or rolled_back entries\n   - Send Gotify on failure with specifics\n   - Exit 0 on success, 1 on failure\n\n2. **Remove workarounds:**\n   - 30s sleep\n   - Dry-run detection\n   - Double-execution logic\n\n3. **Improve timer:**\n   - Add RandomizedDelaySec=900\n   - Add AccuracySec=1us\n\n## Files\n\n- modules/nixos/homelab/containers/default.nix (auto-update script)\n\n## Reference\n\ndocs/podman-compose-failures.md Part 5, Phase 3","status":"open","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-12T20:18:09.277366636+08:00","created_by":"abl030","updated_at":"2026-02-12T20:18:09.277366636+08:00","dependencies":[{"issue_id":"nixosconfig-3mg","depends_on_id":"nixosconfig-5dy","type":"blocks","created_at":"2026-02-12T20:18:27.263684627+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-465","title":"Investigate secret leakage risk in episodic-memory archives","description":"Episodic memory archives full conversation transcripts including MCP tool outputs. If UniFi/pfSense MCP returns WiFi passwords, firewall rules with IPs, or other secrets, they get stored in the SQLite DB. Investigate: (1) Does episodic-memory have any redaction/filtering capability? (2) Is ~/.claude/episodic-memory/ properly excluded from git? (3) Could search results surface secrets that then get written to tracked files (beads notes, MEMORY.md)? (4) Consider adding a pre-archive filter or configuring sensitive MCP tools to not archive outputs.","notes":"Syncthing now syncs conversation-archive which contains full conversation JSONL + summaries. These may contain secrets mentioned in chat. Consider .stignore patterns or scrubbing.","status":"closed","priority":1,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T13:13:24.134047058+08:00","created_by":"abl030","updated_at":"2026-02-22T14:25:49.041973655+08:00","closed_at":"2026-02-11T20:23:53.723571753+08:00"}
{"id":"nixosconfig-4o3","title":"Submit MCPs to awesome-mcp-servers lists","description":"Submit PRs to community directories for visibility: (1) punkpeye/awesome-mcp-servers — PR to README.md under Cloud Platforms/Developer Tools (2) appcypher/awesome-mcp-servers (3) mcpservers.org submit form. Lead with the AI-driven testing methodology and 99% first-attempt success rate.","status":"open","priority":3,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T20:01:42.917324995+08:00","created_by":"abl030","updated_at":"2026-02-15T20:01:42.917324995+08:00","dependencies":[{"issue_id":"nixosconfig-4o3","depends_on_id":"nixosconfig-7ik","type":"blocks","created_at":"2026-02-15T20:01:49.116987964+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-4o3","depends_on_id":"nixosconfig-e8i","type":"blocks","created_at":"2026-02-15T20:01:49.249225782+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-4r7","title":"pfSense reliability \u0026 observability","description":"Tracking all pfSense hardening, monitoring, and reliability improvements. pfSense is the network's single point of failure — DNS, firewall, routing all depend on it. This epic covers: syslog reliability, DNS cache warming, service watchdogs, and future observability improvements.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-17T06:38:07.11216503+08:00","updated_at":"2026-02-17T06:38:07.11216503+08:00"}
{"id":"nixosconfig-4ts","title":"Add episodic-memory MCP server to fleet","description":"Wire episodic-memory as MCP server in .mcp.json. Exposes episodic_memory_search (vector/text/hybrid) and episodic_memory_show (conversation reader). Add SessionStart hook for conversation sync.","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T12:21:27.768127085+08:00","created_by":"abl030","updated_at":"2026-02-22T14:25:49.042798947+08:00","closed_at":"2026-02-11T20:42:49.934143399+08:00","dependencies":[{"issue_id":"nixosconfig-4ts","depends_on_id":"nixosconfig-0sb","type":"parent-child","created_at":"2026-02-11T12:30:35.184597712+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-4ts","depends_on_id":"nixosconfig-9fa","type":"blocks","created_at":"2026-02-11T12:30:35.560768569+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-5dg","title":"Phase 2: Verification checklist","description":"Post-deployment verification for Phase 2 split services:\n\nBoot/Startup:\n- [ ] Linger ensures user session exists before system services run\n- [ ] System secrets service waits for user@.service\n- [ ] User service starts after podman.socket\n- [ ] All env files decrypted to /run/user/\u003cuid\u003e/secrets/\n- [ ] All containers start successfully\n\nnixos-rebuild switch:\n- [ ] Changing compose file triggers system service restart\n- [ ] Changing SOPS file triggers system service restart\n- [ ] System service bounces user service correctly\n- [ ] Only changed stack restarts (per-stack granularity)\n- [ ] Stale health detection runs before reuse\n- [ ] No journal spam from orphaned timers\n- [ ] Stack operations ~2-3s faster (removed redundant pruning)\n\npodman auto-update:\n- [ ] Auto-update finds containers (io.containers.autoupdate=registry)\n- [ ] Auto-update restarts correct unit (PODMAN_SYSTEMD_UNIT=.service)\n- [ ] User service restart succeeds (env files exist)\n- [ ] Stale health detection runs in user service\n- [ ] Health checks pass / rollback works\n- [ ] Gotify notifications work\n\nManual operations:\n- [ ] systemctl --user restart .service works\n- [ ] systemctl restart -secrets.service works\n- [ ] systemctl status shows correct states\n- [ ] journalctl -u shows logs for both services\n- [ ] podman ps shows containers owned by correct project\n\nService naming:\n- [ ] Old services removed (-stack.service, podman-compose@*.service)\n- [ ] New services present (-secrets.service, .service)\n- [ ] Monitoring updated for new names\n- [ ] No orphaned systemd units\n\nCleanup:\n- [ ] No redundant container pruning (verify with strace/journal)\n- [ ] No pod pruning (obsolete with docker-compose backend)\n- [ ] Timer cleanup still works (orphaned health check timers)\n- [ ] systemctl --user reset-failed works\n\nTest on first deployment (single stack), then verify on full rollout.","notes":"Phase 2 implementation complete (commit d395da2). Ready for deployment testing.\n\n**Deployment Procedure:**\n\n1. Deploy to proxmox-vm:\n   ```bash\n   nixos-rebuild switch --flake github:abl030/nixosconfig#proxmox-vm --target-host proxmox-vm\n   ```\n\n2. Verify service creation:\n   - System secrets services exist: `systemctl list-units | grep -- \"-secrets\"`\n   - User compose services exist: `systemctl --user list-units | grep -- \"-stack\"`\n   - Old services removed: No `{stackName}-stack` or `podman-compose@*` services\n\n3. Verify one stack (e.g., immich):\n   ```bash\n   # Check secrets service\n   systemctl status immich-stack-secrets\n\n   # Check user service\n   systemctl --user status immich-stack\n\n   # Verify containers running\n   podman ps | grep immich\n\n   # Check logs for issues\n   journalctl -u immich-stack-secrets -n 50\n   journalctl --user -u immich-stack -n 50\n   ```\n\n4. Test trigger paths:\n   - nixos-rebuild: Change one stack's SOPS file, rebuild, verify restart\n   - Auto-update: Manually trigger `podman auto-update`, verify correct service restart\n\n5. Deploy to igpu (5 stacks):\n   ```bash\n   nixos-rebuild switch --flake github:abl030/nixosconfig#igpu --target-host igpu\n   ```\n\n6. Monitor health:\n   - Check Uptime Kuma for service availability\n   - Check Loki logs for errors: `{host=\"proxmox-vm\"} |= \"error\"`\n   - Verify no orphaned health check timer spam\n\n**Success Criteria:**\n- All containers start successfully\n- No service activation failures\n- Cleanup operations ~2-3s faster (verify via journal timestamps)\n- Auto-update restarts correct unit (user service)\n- No orphaned timers or stuck health checks","status":"open","priority":1,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-12T21:34:46.267322633+08:00","created_by":"abl030","updated_at":"2026-02-12T21:49:33.736128269+08:00","dependencies":[{"issue_id":"nixosconfig-5dg","depends_on_id":"nixosconfig-5dy","type":"blocks","created_at":"2026-02-12T21:35:12.736050893+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-5dy","title":"Phase 2: Split services by privilege (podman)","description":"Separate SOPS decryption from compose lifecycle for proper privilege separation.\n\n## Architectural Decisions (2026-02-12)\n\n1. **Multi-file secrets**: Preserve separation - decrypt each envFile to separate path in /run/user/\u003cuid\u003e/secrets/. Pass multiple --env-file args to compose. No merging.\n\n2. **SOPS sharing**: Verified no stacks share SOPS files (24 use unique encEnv, 1 uses encAcmeEnv). No thundering herd risk.\n\n3. **Cleanup script**: SIMPLIFY to only handle orphaned health check timers. Remove redundant container prune (global timer handles it) and obsolete pod prune (docker-compose doesn't create pods). See Appendix F.\n\n4. **Restart triggers**: System service always reruns decrypt + bounce (simpler than conditional logic).\n\n## Changes Required\n\n1. **Rewrite mkService to generate TWO services per stack:**\n\n   System service (${stackName}-secrets.service):\n   - Type=oneshot; RemainAfterExit=true\n   - Runs as root for SOPS decryption using /var/lib/sops-nix/key.txt\n   - ExecStart: Decrypt ALL envFiles to separate paths (preserve multi-file structure)\n   - ExecStartPost: chmod/chown all files, then bounce user service via runuser\n   - Depends on user@\u003cuid\u003e.service (After + Requires)\n   - restartTriggers: ALL SOPS files + compose file (trigger proxy)\n   \n   User service (${stackName}.service):\n   - Type=oneshot; RemainAfterExit=true  \n   - ExecStartPre: Verify ALL env files exist (retry loop 30s each)\n   - ExecStart: podman compose with multiple --env-file args (no merging)\n   - After/Wants: podman.socket\n   - Environment: PODMAN_SYSTEMD_UNIT=${stackName}.service\n   - ExecStartPost/StopPost: stackCleanupSimplified (timer cleanup only)\n   - restartIfChanged=false (system service bounces it)\n\n2. **Simplify stackCleanup script:**\n   REMOVE: sleep 2, container prune (redundant), pod prune (obsolete)\n   KEEP: orphaned health check timer cleanup, systemctl reset-failed\n   BENEFIT: ~2-3s faster per operation, 38 redundant scans/day eliminated\n\n3. **Service bounce pattern (researched):**\n   +/run/current-system/sw/bin/runuser -u ${user} -- sh -c 'export XDG_RUNTIME_DIR=/run/user/\u003cuid\u003e; systemctl --user restart ${stackName}.service'\n\n4. **Testing:**\n   - Stacks come up on boot (linger → user session → services)\n   - podman auto-update restarts user service correctly\n   - Per-stack granularity (changing one stack only restarts that one)\n   - Multi-file env files all decrypted and passed to compose\n   - No journal spam from orphaned timers\n   - Operations ~2-3s faster (removed redundant pruning)\n\n## Files\n\n- stacks/lib/podman-compose.nix\n- modules/nixos/homelab/containers/default.nix\n\n## Reference\n\ndocs/podman-compose-failures.md Part 5, Phase 2\nAppendix C (cross-scope service management)\nAppendix D (socket scope)\nAppendix F (cleanup script necessity)","notes":"## Additional Architectural Decision (2026-02-12)\n\n5. **Stale health detection placement**: Run in BOTH services for complete coverage:\n   - System service ExecStartPre: handles nixos-rebuild path (runs before bounce)\n   - User service ExecStartPre: handles auto-update path (runs when auto-update restarts user service directly)\n   - Rationale: Auto-update bypasses system service, so detection must be in user service too. Slight redundancy on nixos-rebuild acceptable for defensive coverage.\n\n## Service Naming\n- System: ${stackName}-secrets.service (e.g., immich-secrets.service)\n- User: ${stackName}.service (e.g., immich.service)\n- PODMAN_SYSTEMD_UNIT=${stackName}.service (points to user service)","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-12T20:17:59.152324492+08:00","created_by":"abl030","updated_at":"2026-02-12T21:49:18.736054081+08:00","closed_at":"2026-02-12T21:49:18.736054081+08:00","close_reason":"Phase 2 implementation complete. All 23 mkService-generated stacks now use dual-service architecture: system secrets service (root, SOPS decryption) + user compose service (rootless, compose lifecycle). Cleanup script optimized (~2-3s faster). Service naming: {stackName}-secrets (system), {stackName} (user). Commit d395da2 pushed. Ready for deployment verification (nixosconfig-5dg).","dependencies":[{"issue_id":"nixosconfig-5dy","depends_on_id":"nixosconfig-rni","type":"blocks","created_at":"2026-02-12T20:35:46.990359964+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-5tx","title":"Quality gate: run at feature boundaries not every commit","description":"Decision: Don't run 'check' after every small change — it's slow and kills momentum. Run 'check' when a feature feels complete. Run 'check --full' before pushing. 'nix fmt' is cheap and fine anytime. Trust the code while iterating; validate when landing.","status":"closed","priority":4,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T20:13:43.957879438+08:00","created_by":"abl030","updated_at":"2026-02-11T20:13:49.352560049+08:00","closed_at":"2026-02-11T20:13:49.35256836+08:00"}
{"id":"nixosconfig-6bn","title":"Reference: Special Host Configurations (doc1/igpu/framework)","description":"Reference documentation for special host configurations.\n\n## doc1 (Main Services VM)\n- Serves as Nix cache server (nixcache.ablz.au)\n- Runs GitHub Actions runner (proxmox-bastion)\n- Hosts 20+ Docker services\n- MTU 1400 for Tailscale compatibility\n\n## igpu (Media Transcoding)\n- AMD 9950X iGPU passthrough\n- Latest kernel for GPU support\n- Increased inotify watches (2,097,152)\n- Vendor-reset DKMS module on Proxmox host\n\n## framework (Laptop)\n- Sleep-then-hibernate configuration\n- Fingerprint reader support\n- Power management optimizations","status":"closed","priority":4,"issue_type":"chore","created_at":"2026-02-11T20:35:56.81379339+08:00","updated_at":"2026-02-11T20:36:05.575531427+08:00","closed_at":"2026-02-11T20:36:05.575531427+08:00","close_reason":"Reference doc — content moved from CLAUDE.md","labels":["hosts","reference"]}
{"id":"nixosconfig-6u1","title":"Package loki-mcp for PyPI + MCP Registry","description":"Same pattern as pfsense-mcp: add loki_mcp/ package dir, __init__.py, __main__.py, copy generated/server.py, update pyproject.toml with full metadata + console_scripts, create server.json (io.github.abl030/loki-mcp), add mcp-name comment to README, build + publish.","status":"open","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T20:01:42.792415591+08:00","created_by":"abl030","updated_at":"2026-02-15T20:01:42.792415591+08:00","dependencies":[{"issue_id":"nixosconfig-6u1","depends_on_id":"nixosconfig-7ik","type":"blocks","created_at":"2026-02-15T20:01:49.003326587+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-7ik","title":"Publish MCP servers to PyPI + official MCP Registry","description":"Publish all our MCP servers (pfsense, unifi, loki, lidarr) to PyPI and register them on the official MCP Registry (registry.modelcontextprotocol.io). Each server needs: (1) pfsense_mcp/ package dir with __init__, __main__, bundled server.py (2) pyproject.toml with full PyPI metadata + console_scripts entry point (3) server.json for MCP registry with io.github.abl030/ namespace (4) mcp-name HTML comment in README for PyPI validation (5) uv build + uv publish to PyPI (6) mcp-publisher login github + mcp-publisher publish to registry. Also submit PRs to awesome-mcp-servers lists and mcpservers.org for visibility.","status":"open","priority":2,"issue_type":"feature","owner":"abl030@gmail.com","created_at":"2026-02-15T20:01:29.531486845+08:00","created_by":"abl030","updated_at":"2026-02-15T20:01:29.531486845+08:00"}
{"id":"nixosconfig-8ds","title":"Phase 4: Clean up compose files and verify backend (podman)","description":"Final cleanup tasks for podman compose migration.\n\n## Tasks\n\n1. **Remove deprecated version key:**\n   - Find all compose files with version: \"3.8\"\n   - Remove the key (docker-compose warns it's obsolete)\n\n2. **Add missing health checks:**\n   - Audit all containers for health checks\n   - Add health checks where missing\n   - Improves --wait reliability\n\n3. **Ensure fully-qualified images:**\n   - All images must be fully-qualified (required for auto-update registry policy)\n   - Format: registry.example.com/org/image:tag\n\n4. **Remove orphaned compose files:**\n   - Clean up compose files for disabled/removed stacks\n\n5. **Verify database backend:**\n   - Check if using BoltDB or SQLite\n   - Migrate to SQLite if needed (ahead of Podman 6.0)\n   - Command: podman info --format \"{{.Store.GraphDriverName}}\"\n\n## Files\n\n- stacks/**/docker-compose.yml (all compose files)\n\n## Reference\n\ndocs/podman-compose-failures.md Part 5, Phase 4","status":"open","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-12T20:18:20.688923695+08:00","created_by":"abl030","updated_at":"2026-02-12T20:18:20.688923695+08:00","dependencies":[{"issue_id":"nixosconfig-8ds","depends_on_id":"nixosconfig-3mg","type":"blocks","created_at":"2026-02-12T20:18:27.283469231+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-8i8","title":"Phase 3: Semantic Memory (compressed learnings)","description":"Deploy compressed learning/observation layer. Currently evaluating claude-mem (BLOCKED by critical process leak #1010) or alternatives. This space is evolving fast. See docs/agentic-memory-options-comparison.md.","status":"closed","priority":3,"issue_type":"epic","owner":"abl030@gmail.com","created_at":"2026-02-11T12:24:02.692711371+08:00","created_by":"abl030","updated_at":"2026-02-22T14:25:49.043590619+08:00","closed_at":"2026-02-11T20:42:44.647872221+08:00","dependencies":[{"issue_id":"nixosconfig-8i8","depends_on_id":"nixosconfig-sep","type":"parent-child","created_at":"2026-02-11T12:30:27.243980785+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-8i8","depends_on_id":"nixosconfig-0sb","type":"blocks","created_at":"2026-02-11T12:30:35.803936857+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-8qx","title":"Build NZBGet MCP server","description":"MCP server for NZBGet Usenet download client. Needed for download visibility and sending NZBs directly.\n\nAPI docs: https://nzbget.com/docs/api/ (JSON-RPC)\nHost: nzbget.ablz.au:443 (HTTPS, behind reverse proxy)\n\nTools needed: get_status, list_downloads, get_history, add_nzb_url.\n\nSee docs/music-pipeline-postmortem.md for context — NZBGet path mapping gap caused manual file copying.\nParent epic: nixosconfig-z95 (music pipeline)","status":"open","priority":3,"issue_type":"feature","owner":"abl030@gmail.com","created_at":"2026-02-15T18:58:39.302090395+08:00","created_by":"abl030","updated_at":"2026-02-15T18:58:39.302090395+08:00"}
{"id":"nixosconfig-94f","title":"Package unifi-mcp for PyPI + MCP Registry","description":"Same pattern as pfsense-mcp: add unifi_mcp/ package dir, __init__.py, __main__.py, copy generated/server.py, update pyproject.toml with full metadata + console_scripts, create server.json (io.github.abl030/unifi-mcp), add mcp-name comment to README, build + publish.","status":"open","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T20:01:42.729078256+08:00","created_by":"abl030","updated_at":"2026-02-15T20:01:42.729078256+08:00","dependencies":[{"issue_id":"nixosconfig-94f","depends_on_id":"nixosconfig-7ik","type":"blocks","created_at":"2026-02-15T20:01:48.943111955+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-9fa","title":"Package episodic-memory for NixOS","description":"Node.js app with native modules (better-sqlite3, sqlite-vec). Needs: buildNpmPackage or similar, handle native compilation, pre-cache HuggingFace ONNX model (all-MiniLM-L6-v2, ~22MB) in Nix store. Storage at ~/.config/superpowers/conversation-archive/ + conversation-index/db.sqlite.","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T12:21:25.946956776+08:00","created_by":"abl030","updated_at":"2026-02-22T14:25:49.044264393+08:00","closed_at":"2026-02-11T20:42:49.939676284+08:00","dependencies":[{"issue_id":"nixosconfig-9fa","depends_on_id":"nixosconfig-0sb","type":"parent-child","created_at":"2026-02-11T12:30:35.062256799+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-9fa","depends_on_id":"nixosconfig-bp0","type":"blocks","created_at":"2026-02-11T12:30:35.435434976+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-9lr","title":"Set up Syncthing to sync episodic-memory DB across fleet","description":"Use Syncthing to share ~/.claude/episodic-memory/ SQLite database across all fleet hosts. This gives fleet-wide conversation memory without cloud sync. Considerations: (1) SQLite + Syncthing conflict handling (use Syncthing's conflict resolution or WAL mode), (2) Which hosts to include, (3) One-way vs bidirectional sync.","status":"closed","priority":2,"issue_type":"feature","owner":"abl030@gmail.com","created_at":"2026-02-11T13:27:59.550923507+08:00","created_by":"abl030","updated_at":"2026-02-11T14:27:19.225599253+08:00","closed_at":"2026-02-11T14:27:19.225599253+08:00","close_reason":"Fully declarative Syncthing module deployed. WSL and doc1 connected and syncing episodic-memory folder. Module at modules/nixos/services/syncthing/default.nix, SOPS keys in secrets/hosts/*/syncthing-{cert,key}.pem, device IDs in hosts.nix."}
{"id":"nixosconfig-a12","title":"Framework s2idle lid-open race condition analysis (Feb 2026)","notes":"## Incident: Feb 18 2026 - Framework laptop appeared to crash during hibernate\n\n### What happened\nUser opened lid, screen flashed briefly, went dark. User assumed crash and hard powered off.\nIt was NOT a crash - the system re-suspended within 1 second of resuming.\n\n### Root cause: s2idle ACPI SCI re-arm race condition\n\nThe lid-open ACPI event (IRQ 9) arrived during an active timer wake cycle. The kernel's s2idle\nwake discrimination logic (`acpi_s2idle_wake()` in `drivers/acpi/x86/s2idle.c`) dismissed the\nlid event as spurious, re-armed the ACPI SCI, and went back to sleep for ~1 second. A different\ninterrupt (IRQ 7) then woke the system fully, but by then the lid-open event had been consumed.\n\nlogind never received a \"Lid opened\" input event, so it correctly re-evaluated\nHandleLidSwitch=suspend-then-hibernate with lid-state=closed, and re-triggered suspend.\n\n### Evidence from journalctl -b -1\n\n```\n# The triple-wake sequence showing the race:\nTimekeeping suspended for 133.243 seconds\nPM: Triggering wakeup from IRQ 9      ← likely the lid-open ACPI event\nACPI: PM: Rearming ACPI SCI for wakeup ← kernel dismissed it, went back to sleep\nPM: Triggering wakeup from IRQ 9      ← another ACPI wake\nACPI: PM: Rearming ACPI SCI for wakeup ← dismissed again\nTimekeeping suspended for 0.969 seconds\nPM: Triggering wakeup from IRQ 7      ← something else finally woke it\nACPI: PM: Wakeup unrelated to ACPI SCI\nPM: resume from suspend-to-idle       ← full resume, but lid event was lost\n\n# logind saw no lid-open, re-triggered:\n20:02:43 Operation 'suspend-then-hibernate' finished.\n20:02:44 Suspending, then hibernating...     ← NO \"Lid opened\" between these\n```\n\nCompare to EVERY successful resume in boot -1 which has \"Lid opened.\" logged.\n\n### Current boot showed PM: Image not found (code -22)\nBecause the user hard powered off during the re-suspend/hibernate, there was no valid\nhibernate image. This is expected, not a separate bug.\n\n### Also observed (not directly related but useful context)\n\n1. **BAT1 battery estimation broken**: `BAT1: Failed to update battery discharge rate, ignoring: Numerical result out of range` - appears on multiple wakes, may affect hibernate timing decisions.\n\n2. **HibernateOnACPower=no behavior**: When on AC, HibernateDelaySec countdown never starts. System sits in s2idle indefinitely, waking every 3h for battery checks. This is by-design but means the laptop never hibernates when plugged in (Feb 18: 7 wake cycles over 18 hours, never hibernated).\n\n3. **Feb 15 freeze failures**: `Failed to freeze unit 'user.slice': Connection timed out` followed by `Failed to put system to sleep. System resumed again: Device or resource busy` - happened twice, logind retried and succeeded on third attempt.\n\n4. **Feb 17 amdgpu errors during hibernate**: vmalloc failure in dc_state_create_copy, flip_done timeouts, NFS server timeouts causing 34-second device freeze. Hibernate still succeeded.\n\n5. **Old pstore crash (Jun 2025, kernel 6.15.2)**: btusb_suspend NULL deref during runtime PM - different bug, different kernel, kept in /var/lib/systemd/pstore/1750333820/.\n\n### Config reference\n- sleep config: modules/nixos/services/framework/sleep-then-hibernate.nix\n- hibernate fixes: modules/nixos/services/framework/hibernate-fix.nix\n- host config: hosts/framework/configuration.nix\n- Kernel: 6.19.0, BIOS 03.18, systemd 258.3\n\n### If this happens again\n1. DON'T hard power off - just close and reopen the lid\n2. Check journalctl -b -1 for the triple-wake pattern (IRQ 9 re-arm then IRQ 7)\n3. Absence of \"Lid opened\" in logind logs confirms it's this race, not a crash\n4. To debug further: `echo 'file s2idle.c +p' \u003e /sys/kernel/debug/dynamic_debug/control` (needs root) to trace the wake discrimination logic\n5. AMD PMC debug: /sys/kernel/debug/amd_pmc/s0ix_stats (needs root)","status":"closed","priority":4,"issue_type":"bug","owner":"abl030@gmail.com","created_at":"2026-02-18T20:31:10.438480757+08:00","created_by":"abl030","updated_at":"2026-02-18T20:31:49.454278823+08:00","closed_at":"2026-02-18T20:31:49.454278823+08:00","close_reason":"Investigation complete - documented as reference for future occurrences"}
{"id":"nixosconfig-af3","title":"Phase 1: Beads (procedural memory)","description":"Package bd binary, configure hooks, deploy fleet-wide. COMPLETED. See docs/beads-rollout-plan.md.","status":"closed","priority":1,"issue_type":"epic","owner":"abl030@gmail.com","created_at":"2026-02-11T12:20:29.44879412+08:00","created_by":"abl030","updated_at":"2026-02-11T12:21:11.635349337+08:00","closed_at":"2026-02-11T12:21:11.635349337+08:00","close_reason":"All subtasks complete. Beads deployed fleet-wide, verified on WSL.","dependencies":[{"issue_id":"nixosconfig-af3","depends_on_id":"nixosconfig-sep","type":"parent-child","created_at":"2026-02-11T12:20:50.815468808+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-auy","title":"Nixify ha-mcp package to avoid uvx version drift","description":"ha-mcp 6.7.0 broke silently when fastmcp 3.0.0 was released (show_cli_banner renamed to show_server_banner). Currently pinned fastmcp\u003c3 in wrapper script as workaround. Should package ha-mcp as a nix derivation with pinned deps like we do for pfsense-mcp, unifi-mcp, lidarr-mcp, slskd-mcp, vinsight-mcp, and loki-mcp. This eliminates uvx runtime resolution entirely. Also consider nixifying the other uvx-based MCPs (mcp-nixos, prometheus-mcp-server, beads-mcp) for the same reason. Revert the fastmcp\u003c3 pin once this is done.","status":"closed","priority":3,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-19T19:50:46.22186+08:00","created_by":"abl030","updated_at":"2026-02-19T19:53:31.711113763+08:00","closed_at":"2026-02-19T19:53:31.711113763+08:00","close_reason":"Decided against nixifying ha-mcp. The fastmcp\u003c3 pin in the wrapper is sufficient. Full nixification adds complexity (74 transitive deps) for marginal gain over a version pin. All MCPs we own are already nix-managed. Third-party ones just need version pins when they break.","dependencies":[{"issue_id":"nixosconfig-auy","depends_on_id":"nixosconfig-fah","type":"blocks","created_at":"2026-02-19T19:50:52.215373476+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-auy","depends_on_id":"nixosconfig-3fs","type":"blocks","created_at":"2026-02-19T19:50:55.44415969+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-bp0","title":"Track upstream episodic-memory bug fixes","description":"Monitor github.com/obra/episodic-memory for fixes to: #47 (MCP stdout corruption from console.log in embeddings.ts), #53 (orphaned MCP server processes). These are blockers for deployment. Check periodically.","notes":"Workaround: forked to abl030/episodic-memory with PRs #56 and #51 merged. No longer blocking deployment. Upstream tracking moved to nixosconfig-0sb.2 (P4 backlog).","status":"closed","priority":4,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T12:21:22.271859083+08:00","created_by":"abl030","updated_at":"2026-02-22T14:25:49.044987767+08:00","closed_at":"2026-02-11T20:42:43.518242385+08:00","dependencies":[{"issue_id":"nixosconfig-bp0","depends_on_id":"nixosconfig-0sb","type":"parent-child","created_at":"2026-02-11T12:30:34.938031048+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-bv0","title":"Reference: Hash-Based Drift Detection","description":"Reference documentation for hash-based drift detection.\n\n## Hash-Based Drift Detection\n\nNixOS's deterministic builds mean identical `system.build.toplevel` hashes guarantee identical systems. This repo uses hash comparison to verify refactors produce no unintended changes.\n\n### Workflow\n\n```bash\n# Capture current hashes as baseline (done automatically by nightly CI)\n./scripts/hash-capture.sh\n\n# Run full quality gate + drift detection (slow)\ncheck --full --drift\n\n# After making changes, compare against baseline\n./scripts/hash-compare.sh\n\n# Quick summary only (no nix-diff details)\n./scripts/hash-compare.sh --summary\n\n# Check specific host\n./scripts/hash-compare.sh framework\n```\n\n### Interpreting Results\n\n- **MATCH**: Hash unchanged - pure refactor, no functional changes\n- **DRIFT**: Hash differs - configuration changed, nix-diff shows what\n\nThe compare script runs through ALL hosts and reports ALL drift (doesn't bail on first issue).\n\n### When Hashes Change\n\nIf `hash-compare.sh` shows drift:\n1. Review the nix-diff output to understand what changed\n2. If intentional: run `./scripts/hash-capture.sh` to update baselines\n3. If unintentional: investigate and fix the regression\n\nBaselines are automatically updated by the nightly `rolling_flake_update.sh` after successful builds.","status":"closed","priority":4,"issue_type":"chore","created_at":"2026-02-11T20:35:25.753621874+08:00","updated_at":"2026-02-11T20:36:02.752093159+08:00","closed_at":"2026-02-11T20:36:02.752093159+08:00","close_reason":"Reference doc — content moved from CLAUDE.md","labels":["drift-detection","reference"]}
{"id":"nixosconfig-c96","title":"Garage Door: Shelly Gen 4 + tilt sensor integration","description":"Install and integrate garage door automation:\n- Shelly Gen 4 relay to control garage door opener\n- Tilt sensor for open/closed state detection\n- Home Assistant integration (cover entity with open/close/state)\n- Automations: auto-close timer, notifications, dashboard card","status":"open","priority":2,"issue_type":"epic","owner":"abl030@gmail.com","created_at":"2026-02-21T10:32:46.229666645+08:00","created_by":"abl030","updated_at":"2026-02-21T10:32:46.229666645+08:00","labels":["home-assistant","homelab"]}
{"id":"nixosconfig-c96.1","title":"Install tilt sensor on garage door","description":"Physical installation of the tilt sensor on the garage door panel. Pair/add to Home Assistant (ZHA or Zigbee2MQTT depending on protocol). Verify open/closed state reporting is reliable.","notes":"Sensor: Third Reality garage door tilt sensor (Zigbee)\n\n## Progress\n- Sensor: Third Reality garage door tilt sensor (Zigbee)\n- Paired via Zigbee2MQTT (SLZB-06P7 coordinator)\n- Original device ID: 0xffffb40e0601d2b5\n- Renamed to: garage_door_tilt\n- Entities created:\n  - binary_sensor.garage_door_tilt_contact (door open/closed)\n  - binary_sensor.garage_door_tilt_battery_low (battery status)\n- Pairing method: mqtt.publish to zigbee2mqtt/bridge/request/permit_join\n\n## Sensor Behaviour\n- Very sensitive tilt detection — reports 'open' even at ~10% open\n- Binary only (open/closed), no percentage tracking\n- Good for security/notification use — won't miss a partially open door\n- Mounted and tested: open/closed transitions confirmed working\n\n## DIP Switches\n- 4-level adjustable sensitivity via DIP switches on the device\n- DIP switch also controls audible beep on/off\n- Useful for tuning sensitivity if getting false triggers or missed events","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-21T10:32:52.30974447+08:00","created_by":"abl030","updated_at":"2026-02-21T11:18:07.073318686+08:00","closed_at":"2026-02-21T10:52:01.171458957+08:00","close_reason":"Tilt sensor installed, paired via Z2M, entities working, overview dashboard deployed","labels":["home-assistant"],"dependencies":[{"issue_id":"nixosconfig-c96.1","depends_on_id":"nixosconfig-c96","type":"parent-child","created_at":"2026-02-21T10:32:52.312094477+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-c96.2","title":"Install and wire Shelly Gen 4 to garage door opener","description":"Physical installation of Shelly Gen 4 relay. Wire to garage door opener motor (dry contact relay to trigger open/close). Connect to WiFi and add to Home Assistant via Shelly integration. Verify relay toggles the door.","notes":"## Hardware Change: Shelly → MHCOZY ZG-001\n\n### Why\n- Shelly 1 Gen 4 ANZ is 240V AC only — no DC input support\n- Don't want live 240V hanging in the garage door opener\n- Merlin MT100EVO has 30V DC accessory output — need a DC-powered relay\n\n### New Device: MHCOZY ZG-001\n- 1 Channel Zigbee 3.0 Smart Relay Switch\n- Model: ZG-001 (Amazon AU ASIN: B08X218VMR)\n- Power: DC 5-32V (will run off Merlin's 30V DC accessory output)\n- Dry contact relay with NO/NC/COM terminals\n- Inching mode (momentary pulse, adjustable, default 1s) — set to 0.4-1.0s for MT100EVO\n- Selflock mode also available (not needed here)\n- Built-in 433MHz RF receiver (bonus, not required)\n- Confirmed working with Zigbee2MQTT and Home Assistant\n- Pairs to existing SLZB-06P7 coordinator\n- Status: ORDERED, waiting for delivery\n\n### Shelly 1 Gen 4 ANZ\n- Returning — AC only, not suitable for this use case\n\n---\n\n## MT100EVO Terminal Block (confirmed)\n\n| Terminal | Label | Wire | Function |\n|----------|-------|------|----------|\n| **1** | Push Button | Red (+ve) | Dry contact input for wired wall button |\n| **2** | Ground | White (-ve) | Common ground (shared with IR beam input) |\n| **0** | E-Serial | — | Security+ 2.0 protocol (not used) |\n\n- Ships with green test button bridging T1 and T2\n- Shorting T1-T2 momentarily toggles door: open → close → stop\n- **30V DC accessory output** (up to 50mA) — enough to power the MHCOZY\n- **IMPORTANT**: 30V DC output is disabled in 'Low Standby Mode' — must turn that setting OFF in opener menu\n- **Pulse duration: 0.4–1.0 seconds** — longer (~2s) causes double-command interpretation (door lurches then stops)\n\n---\n\n## Wiring Plan: MHCOZY ZG-001 → Merlin MT100EVO\n\n### Power\n- DC+ and DC- from Merlin's 30V DC accessory output to MHCOZY power input\n- Within MHCOZY's 5-32V DC range\n\n### Relay Output\n- COM and NO to MT100EVO Terminal 1 and Terminal 2\n- Wired in parallel with existing wall button — both continue to work\n\n### MHCOZY Configuration\n- Set to inching mode (momentary) ~0.5-1.0 second pulse\n- Pair via Z2M (Zigbee 3.0)\n\n### Materials Needed\n- Twin-core signal wire (bell wire / 0.5mm) for relay output to T1/T2\n- Short wire for DC power from Merlin accessory terminals\n- Screwdriver, wire strippers\n\n---\n\n## HA Integration Plan (for when relay arrives)\n\nTemplate cover entity combining relay + tilt sensor:\n- Control: switch entity from MHCOZY via Z2M\n- State: binary_sensor.garage_door_tilt_contact (already working)\n- Conditions prevent toggle when already in desired state\n- Automations: notify if open \u003e10min, auto-close at night\n\n---\n\n## References\n- HA Community: https://community.home-assistant.io/t/mhcozy-zigbee-dry-contact-relay-for-lights-garage-opener-etc/419210\n- Amazon AU: https://www.amazon.com.au/MHCOZY-Adjustable-Self-Flock-Momentary-SmartThings/dp/B08X218VMR\n- SmartHomeScene review: https://smarthomescene.com/reviews/zigbee-dry-contact-relay-review/\n- MT100EVO terminals: https://community.garadget.com/t/merlin-mt100evo-cyclone-pro-mt120evo/4194\n- MT100EVO manual: https://garagedooropenerremotes.com.au/wp-content/uploads/2017/08/MT100EVO-Tiltmaster-manual-114D4626G.pdf","status":"open","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-21T10:32:55.147990496+08:00","created_by":"abl030","updated_at":"2026-02-21T11:13:02.825349634+08:00","labels":["home-assistant"],"dependencies":[{"issue_id":"nixosconfig-c96.2","depends_on_id":"nixosconfig-c96","type":"parent-child","created_at":"2026-02-21T10:32:55.149735302+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-c96.3","title":"Configure HA cover entity combining Shelly + tilt sensor","description":"Create a template cover entity in Home Assistant that combines the Shelly relay (for control) and tilt sensor (for state). Should expose open/close/stop actions and report current position (open/closed).","status":"open","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-21T10:33:00.742524454+08:00","created_by":"abl030","updated_at":"2026-02-21T10:33:00.742524454+08:00","labels":["home-assistant"],"dependencies":[{"issue_id":"nixosconfig-c96.3","depends_on_id":"nixosconfig-c96","type":"parent-child","created_at":"2026-02-21T10:33:00.744654697+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-c96.3","depends_on_id":"nixosconfig-c96.1","type":"blocks","created_at":"2026-02-21T10:33:00.747853239+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-c96.3","depends_on_id":"nixosconfig-c96.2","type":"blocks","created_at":"2026-02-21T10:33:00.751590339+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-c96.4","title":"Garage door automations and dashboard","description":"Set up automations:\n- Auto-close after N minutes if left open\n- Notification when door opens/closes\n- Dashboard card showing state with open/close button\n- Optional: night-time check automation (notify if open at bedtime)","status":"open","priority":3,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-21T10:33:02.301429774+08:00","created_by":"abl030","updated_at":"2026-02-21T10:33:02.301429774+08:00","labels":["home-assistant"],"dependencies":[{"issue_id":"nixosconfig-c96.4","depends_on_id":"nixosconfig-c96","type":"parent-child","created_at":"2026-02-21T10:33:02.302827251+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-c96.4","depends_on_id":"nixosconfig-c96.3","type":"blocks","created_at":"2026-02-21T10:33:05.312787336+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-cm5","title":"Research: Container lifecycle in rebuild vs auto-update scenarios","description":"## Problem Statement\n\nCurrent implementation treats rebuild and auto-update the same way, but they have different requirements:\n\n### Rebuild Time (nixos-rebuild switch)\n**Goal:** Apply config changes, restart only what changed\n**Current behavior:** `docker-compose up -d --wait` reuses containers if config matches\n**Desired:** Incremental updates, don't restart unchanged containers\n**Issue:** Stale health checks from reused containers\n\n### Update Time (podman auto-update)\n**Goal:** Pull new images, recreate all containers with latest versions\n**Current behavior:** Unclear - have both system service (podman auto-update) AND user services (podman-compose@*)\n**Desired:** Watchtower-style - always recreate, fresh containers\n**Issue:** Dual service architecture is confusing\n\n## Questions to Research\n\n1. **What does podman auto-update actually do?**\n   - Does it use docker-compose at all?\n   - Or does it directly manage containers via podman API?\n   - How does it interact with compose-managed containers?\n\n2. **What is the dual service architecture?**\n   - System service: `\u003cstack\u003e-stack.service` (what we interact with)\n   - User service: `podman-compose@\u003cproject\u003e.service` (for auto-update?)\n   - Why both? What's the relationship?\n\n3. **How should --wait and --force-recreate be used?**\n   - Rebuild: `up -d --wait` (reuse, only restart changed)\n   - Update: `up -d --wait --force-recreate` (fresh everything)\n   - Or should auto-update not use compose at all?\n\n4. **What's the right architecture?**\n   - Option A: Compose for everything, different flags for rebuild vs update\n   - Option B: Compose for rebuild, podman auto-update for updates (separate paths)\n   - Option C: Something else entirely?\n\n## Research Plan\n\n1. Read `modules/nixos/homelab/containers/default.nix` auto-update implementation\n2. Read `stacks/lib/podman-compose.nix` dual service setup\n3. Check podman auto-update documentation\n4. Trace through what actually happens during:\n   - nixos-rebuild switch (which services restart, why)\n   - systemctl start podman-auto-update (what gets recreated)\n5. Compare to Watchtower's model for lessons learned\n\n## Success Criteria\n\nClear answer to:\n- When should containers be recreated vs reused?\n- How should rebuild differ from update?\n- Should we keep dual services or consolidate?\n- What flags belong where?\n\n## Related Issues\n\n- nixosconfig-hbz: Stale container reuse issue","notes":"# Research Complete - Decision Made\n\n## Summary\n\nResearch definitively answered the open questions about rebuild vs auto-update behavior. See full analysis: `docs/research/container-lifecycle-analysis.md`\n\n## Key Findings\n\n1. **Container reuse DOES cause stale health checks** ✅\n   - Confirmed via Docker docs, GitHub issues, community forums, and our production experience\n   - When docker-compose reuses a container with stuck health state, --wait blocks indefinitely\n   \n2. **Dual service architecture is CORRECT** ✅\n   - System service: Optimized for rebuild (smart reuse, fast incremental changes)\n   - User service: Optimized for auto-update (full recreation via systemd lifecycle)\n   - Each already uses the right strategy for its use case\n\n3. **User services already recreate containers** ✅ (Key insight!)\n   - Systemd runs full service cycle: ExecStop → ExecStart\n   - This already provides Watchtower-style fresh deployment\n   - No need for --force-recreate flag\n\n4. **Solution: Targeted remediation, not blanket workaround** ✅\n   - Add stale health detection in ExecStartPre (system service only)\n   - Remove containers in \"starting\" or \"unhealthy\" state before reuse\n   - Preserves fast path for healthy containers\n   - Low overhead, automatic remediation\n\n## Decision\n\n**APPROVED:** Implement Recommendation 1 from research doc\n- Add `detectStaleHealth` check to `stacks/lib/podman-compose.nix`\n- Runs before `recreateIfLabelMismatch` in ExecStartPre\n- Detects and removes containers with stuck health checks\n- No changes to user services (already working correctly)\n- Do NOT add --force-recreate (defeats incremental rebuild purpose)\n\n## Related Work\n\n- Update nixosconfig-hbz (stale container bug) with remediation plan\n- Document health check best practices in stack templates\n- Update CLAUDE.md with findings (DONE)\n\n## Rating: 9/10\n\nExcellent research, correct conclusions, implementation-ready recommendations.","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-12T17:24:44.344752827+08:00","created_by":"abl030","updated_at":"2026-02-12T18:03:51.396290413+08:00","closed_at":"2026-02-12T18:03:51.396294152+08:00"}
{"id":"nixosconfig-cnz","title":"Build custom slskd MCP server","description":"Need an MCP server for slskd (Soulseek client) to avoid podman exec / raw API calls.\n\nShould expose:\n- Download transfer status (active, completed, failed)\n- Search Soulseek network\n- Browse peer shares\n- Connection status\n- Download/upload statistics\n- Share management\n\nslskd has a full REST API at /api/v1/. Currently we're hitting it via podman exec wget which is terrible.\nParent epic: nixosconfig-z95 (music pipeline)","notes":"Scaffold complete at https://github.com/abl030/slskd-mcp — OpenAPI spec (70 paths, 93 ops) pulled from temp container with SLSKD_SWAGGER=true. Generator skeleton matches lidarr-mcp pattern. Sprint 1 (generator core) is next.","status":"in_progress","priority":2,"issue_type":"feature","owner":"abl030@gmail.com","created_at":"2026-02-15T17:59:42.57521834+08:00","created_by":"abl030","updated_at":"2026-02-15T21:34:12.01118246+08:00"}
{"id":"nixosconfig-d7u","title":"Migrate Claude Code to official/community NixOS module","description":"Manual npm install of episodic-memory plugin dependencies irreparably broke Claude Code. Current custom HM module (modules/home-manager/services/claude-code.nix) is fragile.\n\nPlan:\n1. Research: is there an official or community NixOS/HM module for Claude Code? Evaluate options.\n2. Migrate from custom module to the chosen module.\n3. Plumb in episodic-memory plugin/MCP but leave it DISABLED (no credits right now).\n4. Rethink MCP server placement: repo-level (.mcp.json) vs home-directory level (~/.claude/).\n5. Strip out arr and soulseek MCPs entirely — don't re-add them.\n\nKey lesson: manual npm install inside the Nix store broke everything. The new approach must handle plugin deps properly.","notes":"2026-02-11: Closed — keeping custom module, not migrating. See earlier notes for full rationale.","status":"closed","priority":0,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T19:59:21.147037821+08:00","created_by":"abl030","updated_at":"2026-02-11T20:16:58.622978381+08:00","closed_at":"2026-02-11T20:16:58.622985505+08:00"}
{"id":"nixosconfig-da9","title":"Handle orphaned episodic-memory processes","description":"MCP servers accumulate when sessions end abnormally (upstream #53). Need systemd user timer or cleanup script to reap orphans. May not be needed if #53 gets fixed upstream.","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T12:21:30.268775782+08:00","created_by":"abl030","updated_at":"2026-02-22T14:25:49.045687691+08:00","closed_at":"2026-02-11T20:42:49.929225543+08:00","dependencies":[{"issue_id":"nixosconfig-da9","depends_on_id":"nixosconfig-0sb","type":"parent-child","created_at":"2026-02-11T12:30:35.311665735+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-da9","depends_on_id":"nixosconfig-9fa","type":"blocks","created_at":"2026-02-11T12:30:35.685200115+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-e7h","title":"Phase 2: Split container services by privilege","description":"Split each container stack into two services:\n\nSystem service (-secrets.service):\n- Runs as root for SOPS decryption\n- Decrypts env files to /run/user/\u003cuid\u003e/secrets/\n- Creates directories, fixes ownership\n- Bounces user service via runuser\n- Acts as restart trigger proxy (restartTriggers on compose + SOPS files)\n- Includes stale health detection in ExecStartPre\n\nUser service (.service):\n- Runs in user scope (rootless)\n- Owns complete compose lifecycle\n- Includes stale health detection in ExecStartPre (for auto-update path)\n- Uses simplified cleanup script (timer cleanup only)\n- restartIfChanged=false (system service triggers)\n- PODMAN_SYSTEMD_UNIT points here for auto-update\n\nChanges to mkService in stacks/lib/podman-compose.nix:\n- Generate two services instead of one\n- Update PODMAN_SYSTEMD_UNIT label\n- Implement runuser bounce pattern (+prefix for root)\n- Simplify cleanup per Appendix F\n\nExclude domain-monitor (defer to separate bead)\n\nArchitectural decisions from docs/podman-compose-failures.md Phase 2:\n- Multi-file secrets: preserve separation\n- SOPS sharing: no thundering herd (verified unique)\n- Cleanup: timer cleanup only (remove redundant prune)\n- Restart triggers: always rerun decrypt + bounce (simpler)\n\nReferences:\n- docs/podman-compose-failures.md Phase 2\n- Appendix C: Cross-scope service management\n- Appendix F: Cleanup script simplification","status":"closed","priority":1,"issue_type":"feature","owner":"abl030@gmail.com","created_at":"2026-02-12T21:34:29.250438425+08:00","created_by":"abl030","updated_at":"2026-02-12T21:35:08.948681691+08:00","closed_at":"2026-02-12T21:35:08.948681691+08:00","close_reason":"Duplicate of existing nixosconfig-5dy which already has comprehensive Phase 2 plan"}
{"id":"nixosconfig-e8i","title":"Publish pfsense-mcp to PyPI","description":"PyPI packaging already done (pfsense_mcp/ package, pyproject.toml, server.json, mcp-name tag). Wheel builds at dist/. Steps remaining: (1) Create PyPI account + API token (2) uv publish dist/* (3) Verify pip install pfsense-mcp works (4) mcp-publisher login github (5) mcp-publisher publish (6) Verify on registry.modelcontextprotocol.io","status":"open","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T20:01:42.656616215+08:00","created_by":"abl030","updated_at":"2026-02-15T20:01:42.656616215+08:00","dependencies":[{"issue_id":"nixosconfig-e8i","depends_on_id":"nixosconfig-7ik","type":"blocks","created_at":"2026-02-15T20:01:48.878565971+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-fah","title":"Reference: Home Assistant \u0026 Music Assistant Integration","description":"Reference documentation for Home Assistant MCP integration.\n\n## Home Assistant\n\nUses [ha-mcp](https://github.com/homeassistant-ai/ha-mcp) for smart home control. Usage notes:\n- All tools are **deferred** — use `ToolSearch` with query `+homeassistant` to load them before calling.\n- Tool names follow `ha_*` convention (e.g., `ha_call_service`, `ha_search_entities`, `ha_get_state`).\n- Use `ha_search_entities` to find entities by name/area/domain.\n- Device targeting uses `entity_id` for service calls.\n- Supports fuzzy entity matching and media playback via Music Assistant.\n- Auth: `HA_TOKEN` in sops-encrypted `secrets/mcp/homeassistant.env`.\n\n## Music Assistant Playback (Preferred Method)\n\nAlways search first to get exact URIs, then play with the URI. This avoids fuzzy matching errors (e.g., playing \"Mr. Peanut\" track instead of \"Peanut\" album).\n\n```python\n# 1. Get Music Assistant config_entry_id (one-time lookup)\nha_get_integration(query=\"music_assistant\")  # Returns entry_id\n\n# 2. Search for the album/track\nha_call_service(\"music_assistant\", \"search\", return_response=True, data={\n    \"config_entry_id\": \"01K3AS5H08FV1C1AAKEDAFDMB5\",\n    \"name\": \"Peanut\",\n    \"artist\": \"Otto Benson\",\n    \"media_type\": [\"album\"],\n    \"limit\": 5\n})\n# Returns: {\"albums\": [{\"uri\": \"spotify--xxx://album/123\", \"name\": \"Peanut\", ...}]}\n\n# 3. Play with exact URI\nha_call_service(\"music_assistant\", \"play_media\",\n    entity_id=\"media_player.kitchen_home_2\",\n    data={\"media_id\": \"spotify--xxx://album/123\", \"media_type\": \"album\"})\n```\n\n## Volume Control Quirk (Google Cast / Music Assistant)\n\n- Use `volume_set` with explicit `volume_level` (0.0-1.0) — avoid `volume_up`/`volume_down`.\n- Wait until playback is stable before changing volume — mid-transition volume changes can stop playback.\n- If volume change stops playback, resume with `media_player.media_play` then retry.","status":"open","priority":4,"issue_type":"chore","created_at":"2026-02-11T20:35:53.017013613+08:00","updated_at":"2026-02-11T20:37:45.173223803+08:00","labels":["home-assistant","mcp","reference"],"comments":[{"id":1,"issue_id":"nixosconfig-fah","author":"abl030","text":"Still need to implement HA/Music Assistant integration","created_at":"2026-02-11T12:37:45Z"}]}
{"id":"nixosconfig-h3i","title":"Re-enable vinsight-mcp package after nix-netrc deploys","description":"The nix-netrc secret was updated with a classic GitHub PAT (replacing a fine-grained PAT that couldn't fetch private repo archives). The secret will deploy to all hosts via tonight's rolling update. Once deployed, uncomment pkgs.vinsight-mcp in modules/home-manager/services/claude-code.nix line 207 and commit. The next rolling update will then build with vinsight-mcp enabled. Root cause: fine-grained PATs return 404 on github.com/archive URLs (only work with api.github.com). Classic PATs work on both.","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-18T21:19:06.496390225+08:00","created_by":"abl030","updated_at":"2026-02-19T18:11:35.048498189+08:00","closed_at":"2026-02-19T07:40:39.001967922+08:00"}
{"id":"nixosconfig-h4j","title":"Initialize beads on nixosconfig repo","description":"Run bd init, bd doctor --fix, configure sync-branch for multi-clone workflow, install git hooks.","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T12:20:43.55317738+08:00","created_by":"abl030","updated_at":"2026-02-11T12:21:01.830323323+08:00","closed_at":"2026-02-11T12:21:01.830323323+08:00","close_reason":"Deployed and verified on WSL. See docs/beads-rollout-plan.md.","dependencies":[{"issue_id":"nixosconfig-h4j","depends_on_id":"nixosconfig-af3","type":"parent-child","created_at":"2026-02-11T12:20:52.468773353+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-h4j","depends_on_id":"nixosconfig-htg","type":"blocks","created_at":"2026-02-11T12:20:55.048127951+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-h4j","depends_on_id":"nixosconfig-tij","type":"blocks","created_at":"2026-02-11T12:20:55.082149386+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-h4j","depends_on_id":"nixosconfig-ion","type":"blocks","created_at":"2026-02-11T12:20:55.11831503+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-hbz","title":"Stale container reuse issue with podman compose --wait","description":"## Issue\n\nDocker-compose with --wait flag can get stuck indefinitely waiting for containers with stale health check status. This happens when:\n\n1. A container starts and fails its initial health check\n2. The container remains in \"starting\" state with failed health log entry\n3. Service is restarted (nixos-rebuild, manual restart, etc)\n4. Docker-compose finds existing container and tries to reuse it\n5. --wait blocks waiting for health check to pass\n6. Container's health check system doesn't retry (only has the initial failed attempt)\n7. Deployment hangs forever\n\n## Root Cause\n\nDocker-compose's container reuse logic + podman's health check not re-running after initial failure = deadlock situation.\n\n## Symptoms\n\n- Service stuck in \"activating (start)\" state during nixos-rebuild\n- journalctl shows \"Container \u003cname\u003e Waiting\" as last message\n- podman inspect shows health log with single failed entry from minutes/hours ago\n- Container status shows \"Up X minutes (starting)\"\n\n## Solution\n\nRemove stale containers before redeploying:\n\n```bash\n# Find containers stuck in starting state\npodman ps -a --format \"table {{.Names}}\\t{{.Status}}\" | grep \"starting\"\n\n# Check health log (should have multiple attempts, not just one old failure)\npodman inspect \u003cname\u003e --format '{{json .State.Health.Log}}' | jq\n\n# Remove to force fresh creation\npodman rm -f \u003cname\u003e\n\n# Restart service\nsudo systemctl restart \u003cstack-name\u003e\n```\n\n## Prevention\n\nCurrently none - this is inherent to docker-compose's reuse behavior. Options:\n\n1. Use --force-recreate flag (defeats fast restart purpose)\n2. Ensure cleanup runs between deployments (unreliable)\n3. Accept manual intervention when it happens (current approach)\n\n## Status\n\nOngoing risk - not just migration artifact. Can happen anytime container health degrades and service restarts before cleanup.","notes":"# Remediation Plan Approved\n\nBased on research (nixosconfig-cm5), we have a clear solution to the stale container reuse issue.\n\n## Root Cause (Confirmed)\n\nContainer reuse during nixos-rebuild when health checks are in bad state:\n1. Container has failed/stuck health check from previous run\n2. Config unchanged, so docker-compose reuses container\n3. --wait blocks waiting for health to become \"healthy\"\n4. Health check state never changes (no new checks scheduled)\n5. Deployment hangs indefinitely\n\n## Solution (Ready to Implement)\n\nAdd pre-start detection in `stacks/lib/podman-compose.nix`:\n\n```nix\ndetectStaleHealth = [\n  \"/run/current-system/sw/bin/sh -c 'ids=$(${podmanBin} ps -a --filter label=io.podman.compose.project=${projectName} --format \\\"{{.ID}}\\\"); for id in $ids; do health=$(${podmanBin} inspect -f \\\"{{.State.Health.Status}}\\\" $id 2\u003e/dev/null || echo \\\"none\\\"); if [ \\\"$health\\\" = \\\"starting\\\" ] || [ \\\"$health\\\" = \\\"unhealthy\\\" ]; then echo \\\"Removing container $id with stale health: $health\\\" \u003e\u00262; ${podmanBin} rm -f $id; fi; done'\"\n];\n```\n\nAdd to ExecStartPre (line ~217, before recreateIfLabelMismatch).\n\n## Benefits\n\n- Prevents indefinite hangs during rebuild\n- Maintains fast reuse for healthy containers  \n- Automatic remediation (no manual intervention)\n- Low overhead (quick inspect check)\n- Logs when containers are removed for visibility\n\n## Testing Plan\n\n1. Create test scenario with stuck health check\n2. Verify detection removes stale container\n3. Verify fresh container created successfully\n4. Verify healthy containers NOT removed\n5. Deploy to doc1, monitor for issues\n6. Deploy to igpu during migration\n\n## Implementation Priority\n\nHIGH - Solves immediate production pain point (deployments hanging)\n\nSee full analysis: `docs/research/container-lifecycle-analysis.md`","status":"closed","priority":2,"issue_type":"bug","owner":"abl030@gmail.com","created_at":"2026-02-12T16:52:06.05363207+08:00","created_by":"abl030","updated_at":"2026-02-12T18:26:54.234152284+08:00","closed_at":"2026-02-12T18:26:54.234152284+08:00","close_reason":"Implemented stale health detection in commit e194187. System now automatically removes containers stuck in starting/unhealthy state for \u003e90s before redeployment, preventing indefinite hangs."}
{"id":"nixosconfig-htg","title":"Add beads to Claude Code fleet packages","description":"Add pkgs.beads to homelab.claudeCode packages in claude-code.nix. Deploys bd binary to all hosts with claudeCode enabled.","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T12:20:40.425485279+08:00","created_by":"abl030","updated_at":"2026-02-11T12:21:01.821176636+08:00","closed_at":"2026-02-11T12:21:01.821176636+08:00","close_reason":"Deployed and verified on WSL. See docs/beads-rollout-plan.md.","dependencies":[{"issue_id":"nixosconfig-htg","depends_on_id":"nixosconfig-af3","type":"parent-child","created_at":"2026-02-11T12:20:52.360048324+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-htg","depends_on_id":"nixosconfig-vzl","type":"blocks","created_at":"2026-02-11T12:20:54.921124885+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-ion","title":"Add beads-mcp to .mcp.json","description":"Add beads-mcp server entry using uvx pattern. Provides MCP tools for structured beads access.","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T12:20:42.453117364+08:00","created_by":"abl030","updated_at":"2026-02-11T12:21:01.827484528+08:00","closed_at":"2026-02-11T12:21:01.827484528+08:00","close_reason":"Deployed and verified on WSL. See docs/beads-rollout-plan.md.","dependencies":[{"issue_id":"nixosconfig-ion","depends_on_id":"nixosconfig-af3","type":"parent-child","created_at":"2026-02-11T12:20:52.43008575+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-ion","depends_on_id":"nixosconfig-vzl","type":"blocks","created_at":"2026-02-11T12:20:55.013324396+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-jdw","title":"Self-host MusicBrainz mirror + LRCLIB lyrics + switch Lidarr to nightly","description":"Lidarr's official metadata server (`api.lidarr.audio`) has been broken since 2025-05-21 (MusicBrainz schema change corrupted their DB, never rebuilt). LRCLIB public API is also painfully slow. Self-hosting both solves reliability AND makes everything instant.\n\n## The Problem\n\n### Lidarr Metadata Server\n- `api.lidarr.audio` broken for 9+ months — artist searches fail, album art patchy\n- Root cause: MusicBrainz schema change on 2025-05-21 corrupted the Lidarr team's database\n- Workaround: `lidarr:\u003cMBID\u003e` prefix for searches, but the whole UI experience is degraded\n- The **software** isn't broken, their **hosted instance** is. Self-hosting the same software with a fresh DB works perfectly.\n\n### LRCLIB\n- Public API at lrclib.net works but is very slow\n- Tagging agent and any lyrics consumers are bottlenecked by it\n\n## The Solution\n\n### 1. MusicBrainz Mirror + Lidarr Metadata Server (LMD)\n- **Guide**: https://github.com/blampe/hearring-aid/blob/main/docs/self-hosted-mirror-setup.md\n- **6 containers**: PostgreSQL, RabbitMQ, Solr, MusicBrainz, Redis, LMD (`blampe/lidarr.metadata`)\n- **Resources**: ~100GB disk, 8GB RAM (doc1 has 18GB available, needs disk expansion)\n- **Replication**: Auto-replicates from upstream MusicBrainz, weekly Solr re-index cron\n- **Data flow**: Lidarr → LMD (:5001) → local PostgreSQL + Solr + Fanart/Spotify/Last.fm APIs\n- **API keys needed**: Fanart.tv, Spotify, Last.fm, MusicBrainz replication token from metabrainz.org\n- **Community standard**: de facto solution used by multiple people, not some random project\n- Also benefits: tagging agent can use local MusicBrainz API (:5000) instead of rate-limited public API\n\n### 2. LRCLIB Self-Hosted\n- **Repo**: https://github.com/tranxuanthang/lrclib\n- **Stack**: Single Rust binary + SQLite — dead simple\n- **Resources**: ~19GB for the database dump\n- **Container**: `podman run -d -v lrclib-data:/data -p 3300:3300 lrclib-rs:latest`\n- **TODO**: Investigate replication/update strategy for the lyrics database (periodic re-download of dump? incremental updates?)\n\n### 3. Lidarr Nightly (Plugins Branch)\n- Plugins branch merged into nightly as of ~Jan 2026\n- Reddit: https://www.reddit.com/r/Lidarr/comments/1qglq27/the_plugin_branch_will_be_no_more/\n- Image swap: `lscr.io/linuxserver/lidarr:latest` → `lscr.io/linuxserver/lidarr:nightly`\n- Needed for Tubifarry plugin (can set custom metadata server endpoint)\n- **ONE-WAY database migration** — take backup before switching\n- Plugins are \"soon\" reaching master, then we can switch back\n\n## Consumers to hook up\n- **Lidarr** → point at local LMD (:5001) for metadata, via Tubifarry plugin\n- **Tagging agent** (Sonnet) → point at local MusicBrainz API (:5000) for MB lookups + local LRCLIB (:3300) for lyrics\n- **Any future lyrics consumers** → local LRCLIB (:3300)\n\n## Infrastructure\n- **Host**: doc1 (proxmox-vm) — 30GB RAM, 18GB available\n- **Disk**: needs expansion, currently 51GB free on local disk, need ~150GB more for MB mirror + LRCLIB\n- **Current Lidarr**: `lscr.io/linuxserver/lidarr:latest` in `stacks/music/docker-compose.yml`, config at `/mnt/docker/music/lidarr`\n- **Music root**: `/mnt/data/Media/Music/AI/`\n\n## Implementation order\n1. Expand doc1 disk on Proxmox\n2. Deploy MusicBrainz mirror (initial DB fetch ~1hr, Solr indexing ~several hrs)\n3. Deploy LRCLIB (download dump, start container)\n4. Switch Lidarr to nightly (backup first!)\n5. Install Tubifarry, point at local LMD\n6. Update tagging agent CLAUDE.md to use local endpoints\n7. Verify everything works end-to-end\n\n## References\n- hearring-aid guide: https://github.com/blampe/hearring-aid/blob/main/docs/self-hosted-mirror-setup.md\n- LRCLIB source: https://github.com/tranxuanthang/lrclib\n- Official LidarrAPI.Metadata: https://github.com/Lidarr/LidarrAPI.Metadata\n- LidMeta (lighter alternative, pre-alpha, NOT recommended): https://github.com/davedean/lidmeta\n- Community pool at api.musicinfo.pro (fallback option)\n- Reddit thread on plugins merge: https://www.reddit.com/r/Lidarr/comments/1qglq27/the_plugin_branch_will_be_no_more/\n- GitHub issue on broken metadata: https://github.com/Lidarr/Lidarr/issues/5498","notes":"Implementation plan written to docs/musicbrainz-mirror-plan.md. Key decision: musicbrainz-docker requires build step (Solr custom cores) so can't use our standard podman.mkService. Using thin NixOS systemd wrapper around cloned musicbrainz-docker project. Compose overrides (postgres-settings, memory-settings, volume-settings, lmd-settings) added to local/compose/ on doc1. LMD included as compose override, not separate stack. Open questions in plan doc.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-02-21T21:10:06.219373203+08:00","updated_at":"2026-02-22T15:09:16.156845051+08:00","closed_at":"2026-02-22T15:09:16.156845051+08:00","close_reason":"Stack implemented: flake input, extraComposeFiles lib tweak, 4 overrides, lmd+lrclib, weekly reindex timer, lidarr→nightly. Disk expanded 250G→400G. Secrets placeholder created. Manual init (createdb.sh, sir reindex, replication token) still needed.","labels":["infrastructure","lidarr","music"]}
{"id":"nixosconfig-kju","title":"Enable bd compact (memory decay)","description":"Requires Anthropic API key in sops. Summarizes closed issues to preserve decisions while freeing context. Low priority until issue volume warrants it.","status":"closed","priority":4,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T12:21:05.626959253+08:00","created_by":"abl030","updated_at":"2026-02-22T14:25:49.046432894+08:00","closed_at":"2026-02-11T20:42:49.94587169+08:00","dependencies":[{"issue_id":"nixosconfig-kju","depends_on_id":"nixosconfig-sep","type":"parent-child","created_at":"2026-02-11T12:30:36.053727089+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-kju","depends_on_id":"nixosconfig-25q","type":"blocks","created_at":"2026-02-11T20:25:49.168522225+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-kpc","title":"Package lidarr-mcp for PyPI + MCP Registry","description":"Depends on lidarr-mcp generator being complete (nixosconfig-v3y). Same pattern: add lidarr_mcp/ package dir, update pyproject.toml, server.json, mcp-name tag, build + publish. Lower priority since the server isn't built yet.","status":"open","priority":3,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T20:01:42.849311985+08:00","created_by":"abl030","updated_at":"2026-02-15T20:01:42.849311985+08:00","dependencies":[{"issue_id":"nixosconfig-kpc","depends_on_id":"nixosconfig-7ik","type":"blocks","created_at":"2026-02-15T20:01:49.059654884+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-kpc","depends_on_id":"nixosconfig-v3y","type":"blocks","created_at":"2026-02-15T20:01:49.178813767+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-ldz","title":"Set up Seeed XIAO Smart IR Mate for Daikin AC control","description":"## Seeed XIAO Smart IR Mate — Daikin AC Control\n\n### Hardware\n- **Device**: Seeed XIAO Smart IR Mate (ESP32-C3, IR TX/RX, USB-C powered)\n- **AC Remote**: Daikin ARC466A40 → protocol TBD (DAIKIN 280-bit or DAIKIN312 312-bit)\n- **MAC**: 3c:dc:75:bf:23:74\n- **Static IP**: 192.168.1.39 (pfSense DHCP static mapping on LAN)\n\n### ESPHome Config\n- **File**: `ha/esphome/daikin-ir.yaml` (tracked in git)\n- **Secrets**: `ha/esphome/secrets.yaml` (gitignored)\n- **Framework**: ESP-IDF\n- **GPIO3**: IR transmitter (carrier duty 50%)\n- **GPIO4**: IR receiver (inverted, idle 25ms)\n- **API**: No encryption\n- **OTA**: Password protected\n\n### HA Integration\n- **Entity**: `climate.daikin_ir_controller_aircon` (currently non-functional)\n- Added via ESPHome auto-discovery\n\n### BLOCKER: ARC466A40 protocol unknown\n- ESPHome `platform: daikin` did not work — may be wrong protocol OR was masked by RMT symbol memory bug\n- Full research: `docs/daikin-arc466-esphome-research.md`\n- ARC466 family spans two protocols: DAIKIN (280-bit) and DAIKIN312 (312-bit)\n- A40 variant not explicitly listed for either\n\n### Current status\n- Config set to `dump: all` on IR receiver to identify protocol\n- Need to flash OTA, then press physical remote at device to capture protocol ID\n- If DAIKIN (280-bit): built-in `platform: daikin` should work (re-test without RMT bug)\n- If DAIKIN312: need custom component via mistic100/ESPHome-IRremoteESP8266 fork\n\n### Key Learnings\n- ESP32-C3 RMT symbol memory (96 total) is shared; LED strip + IR TX + IR RX exceeds budget — removed LED\n- `idle` max is 32767us with ESP-IDF RMT; Seeed reference (65500us) only works with Arduino\n- NeoPixelBus is Arduino-only; use esp32_rmt_led_strip with ESP-IDF\n- Flash with `sudo` needed until `dialout` group added to epimetheus (nix config change made, needs rebuild)\n\n### Remaining\n- [ ] Flash `dump: all` config and identify protocol variant\n- [ ] Re-test `platform: daikin` if protocol is DAIKIN 280-bit\n- [ ] Build custom component if protocol is DAIKIN312\n- [ ] Update aircon automations once working\n- [ ] Rebuild epimetheus for dialout group","notes":"## IT WORKS\n\n`platform: daikin` confirmed working with ARC466A40. The protocol was always correct — previous failure was RMT symbol memory exhaustion from the LED strip.\n\nTested: cool 21°C ON (beep), OFF (beep). Both responded immediately.\n\n### Remaining\n- [ ] Remove `dump: all` from receiver (noisy, not needed in production)\n- [ ] Update existing aircon automations to use climate entity\n- [ ] Rebuild epimetheus for dialout group\n- [ ] Consider re-adding API encryption","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-24T16:56:59.179934009+08:00","created_by":"abl030","updated_at":"2026-02-24T18:03:54.528931448+08:00","closed_at":"2026-02-24T18:03:46.037890867+08:00"}
{"id":"nixosconfig-mbs","title":"syslogd watchdog and restart","description":"syslogd on pfSense was found dead (stopped since ~Aug 2025). This meant no pfSense logs were reaching Loki despite the syslog forwarding config being correct. The Alloy syslog receiver on igpu (192.168.1.33:1514) was listening fine — syslogd just wasn't running to send anything.\\n\\nRoot cause: unknown — syslogd silently died and FreeBSD didn't restart it. The default rc.d start uses -s (secure/no-remote); pfSense needs its own restart method (pfSsh.php playback svc restart syslogd) which adds -c -c flags.\\n\\nFix applied:\\n1. Restarted syslogd via pfSense service manager — logs immediately started flowing to Loki under {host=\\\"pfsense\\\"}\\n2. Installed pfSense-pkg-Service_Watchdog (v1.8.7_4)\\n3. Added syslogd to watchdog with notify=true — auto-restarts if it dies again","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-17T06:38:18.222912599+08:00","updated_at":"2026-02-17T06:38:42.200048946+08:00","closed_at":"2026-02-17T06:38:42.200048946+08:00","close_reason":"Installed Service_Watchdog package, added syslogd with notify=true. Restarted syslogd — logs now flowing to Loki under {host=\\\"pfsense\\\"}.","labels":["observability","pfsense"],"dependencies":[{"issue_id":"nixosconfig-mbs","depends_on_id":"nixosconfig-4r7","type":"parent-child","created_at":"2026-02-17T06:38:35.826452978+08:00","created_by":"daemon"}]}
{"id":"nixosconfig-mof","title":"Reference: Secrets Management (Sops-nix)","description":"Reference documentation for secrets management with Sops-nix.\n\n## Secrets Management\n\nUses **Sops-nix** with **Age** encryption:\n- Secrets encrypted against SSH host keys (converted to Age keys)\n- Bootstrap paradox: Host keys decrypt master user key on boot\n- Configuration: `secrets/.sops.yaml`\n- When adding a new host:\n  1. Get SSH host key from `/etc/ssh/ssh_host_ed25519_key.pub`\n  2. Convert to Age key: `cat /etc/ssh/ssh_host_ed25519_key.pub | ssh-to-age`\n  3. Add Age key to `secrets/.sops.yaml`\n  4. Re-encrypt: `sops updatekeys --yes \u003cfile\u003e` for each secret file\n\n## Secrets Commands\n\n```bash\n# Edit encrypted file\nsops secrets/path/to/file.env\n\n# Add new host to secrets\ncd secrets\nfind . -type f \\( -name \"*.env\" -o -name \"*.yaml\" -o -name \"ssh_key_*\" \\) | \\\n  while read file; do sops updatekeys --yes \"$file\"; done\n```","status":"closed","priority":4,"issue_type":"chore","created_at":"2026-02-11T20:35:16.285649333+08:00","updated_at":"2026-02-11T20:36:02.028451454+08:00","closed_at":"2026-02-11T20:36:02.028451454+08:00","close_reason":"Reference doc — content moved from CLAUDE.md","labels":["reference","secrets"]}
{"id":"nixosconfig-rni","title":"Phase 1.5: Migrate podman socket to user scope","description":"Move podman API service from system scope with User= to native user scope with socket activation.\n\n## Why Migrate\n\nOfficial podman docs recommend user services for rootless sockets. System services with User= have known issues:\n- No session context\n- Manual environment setup required\n- sd_notify rejection\n- Mixed system/user journal logging\n\n## Changes Required\n\n1. Remove system service from modules/nixos/homelab/containers/default.nix\n2. Enable native user socket/service (NixOS already ships these)\n3. Test socket activation and existing stack connectivity\n\n## Low Risk\n\nSocket path stays /run/user/1000/podman/podman.sock - no impact on existing stacks.\n\n## Reference\n\ndocs/podman-compose-failures.md Part 5, Phase 1.5\nAppendix D (socket scope research)","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-12T20:34:57.743175567+08:00","created_by":"abl030","updated_at":"2026-02-12T21:35:26.608996205+08:00","closed_at":"2026-02-12T21:35:26.608996205+08:00","close_reason":"Phase 1.5 complete - deployed to both proxmox-vm (19 stacks) and igpu (5 stacks). User socket activation working correctly."}
{"id":"nixosconfig-s2u","title":"DNS cache warming cron job on pfSense","description":"nixos-upgrade on proxmox-vm failed because nginx couldn't resolve cache.nixos.org during config reload. Unbound's DNS cache can go cold for domains not queried frequently enough.","design":"Installed pfSense-pkg-Cron (v0.3.8_6) and created a cron job (id 15) running every 5 minutes as root. Uses /usr/bin/drill (FreeBSD native) to query 5 nix-critical domains against the local resolver (127.0.0.1): cache.nixos.org, github.com, api.github.com, nixosconfig.cachix.org, hyprland.cachix.org. Commands separated by ; so all domains get queried even if one fails.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-17T06:26:12.255390858+08:00","updated_at":"2026-02-17T06:26:19.897161904+08:00","closed_at":"2026-02-17T06:26:19.897161904+08:00","close_reason":"Installed pfSense-pkg-Cron and created cron job (id 15) warming DNS cache every 5 minutes for 5 nix-critical domains.","dependencies":[{"issue_id":"nixosconfig-s2u","depends_on_id":"nixosconfig-4r7","type":"parent-child","created_at":"2026-02-17T06:38:37.323472582+08:00","created_by":"daemon"}]}
{"id":"nixosconfig-sbb","title":"Research: podman-compose failures and migration to podman compose","status":"closed","priority":1,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-12T15:05:30.617782595+08:00","created_by":"abl030","updated_at":"2026-02-12T15:06:09.198289094+08:00","closed_at":"2026-02-12T15:06:09.198289094+08:00","close_reason":"Using docs/ instead"}
{"id":"nixosconfig-sep","title":"Three-layer agentic memory system","description":"Cross-session memory system for AI agents. Four conceptual layers:\n\n1. **Procedural (how-to)**: Patterns, runbooks, workflows, common commands. Lives in CLAUDE.md — loaded every session automatically.\n2. **Declarative (facts/decisions)**: Task tracking, decision records, rationale. Lives in beads — retrieved on demand via bd prime/show.\n3. **Episodic (raw history)**: Conversation archives for context recovery. Paused — episodic-memory plugin, blocked on npm dep packaging.\n4. **Semantic (compressed learnings)**: LLM-distilled knowledge from closed beads and conversations. Not started — depends on episodic layer.\n\nPhase 1 (beads as declarative memory) is live. CLAUDE.md as procedural memory predates this epic and works well. MEMORY.md is a thin bridge: critical patterns + pointers to beads.","notes":"PAUSED — gated on credits (revisit next week). All sub-tasks consolidated here. When resuming: package episodic-memory for Nix, deploy to fleet, handle orphaned processes, track upstream fixes, enable bd compact, migrate beads overlay to flake input, then build semantic layer.","status":"closed","priority":4,"issue_type":"epic","owner":"abl030@gmail.com","created_at":"2026-02-11T12:20:24.034306477+08:00","created_by":"abl030","updated_at":"2026-02-22T14:25:49.047262376+08:00","closed_at":"2026-02-15T17:07:26.995237025+08:00"}
{"id":"nixosconfig-shn","title":"Podman compose migration (4 phases)","description":"Complete migration from podman-compose to podman compose with architectural improvements.\n\n## Status\n\n**Phase 1: Replace podman-compose with podman compose** ✅ COMPLETE (2026-02-12)\n- Swapped compose tool to use podman compose (docker-compose backend)\n- Added --wait flag for reliable deployments\n- Implemented stale health detection (90s threshold, configurable)\n- Deployed to doc1 and igpu\n- All 24 stacks migrated successfully\n\n**Phase 2: Split Services by Privilege** - NOT STARTED\n- Separate SOPS decryption from compose lifecycle\n- System service for secrets, user service for compose\n- Core architectural change for privilege separation\n\n**Phase 3: Simplify Auto-Update** - NOT STARTED\n- Clean up auto-update wrapper\n- Remove workarounds\n\n**Phase 4: Clean Up** - NOT STARTED\n- Remove version: \"3.8\" from compose files\n- Add missing health checks\n- Verify database backend\n\n## References\n\n- Migration plan: docs/podman-compose-failures.md\n- Research: docs/research/container-lifecycle-analysis.md\n- Decision: docs/decisions/2026-02-12-container-lifecycle-strategy.md","status":"open","priority":2,"issue_type":"epic","owner":"abl030@gmail.com","created_at":"2026-02-12T20:17:43.322030342+08:00","created_by":"abl030","updated_at":"2026-02-12T20:17:43.322030342+08:00"}
{"id":"nixosconfig-sts","title":"Loki \\\"no pfSense logs\\\" alert","description":"Belt-and-suspenders: create a Loki alert rule that fires if no logs arrive from {host=\\\"pfsense\\\"} within a 15-minute window. This catches cases where syslogd dies AND Service Watchdog fails to restart it, or if the network path to igpu:1514 breaks.\\n\\nDepends on the Loki alerting pipeline being set up (ruler or Grafana alerts).","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-17T06:38:22.10660363+08:00","updated_at":"2026-02-17T06:38:22.10660363+08:00","labels":["backlog","observability","pfsense"],"dependencies":[{"issue_id":"nixosconfig-sts","depends_on_id":"nixosconfig-4r7","type":"parent-child","created_at":"2026-02-17T06:38:36.80723582+08:00","created_by":"daemon"}]}
{"id":"nixosconfig-tij","title":"Configure SessionStart/PreCompact hooks","description":"Add bd prime (SessionStart) and bd sync (PreCompact) hooks via homelab.claudeCode.settings in base.nix. Guarded with .beads directory check.","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T12:20:41.426970383+08:00","created_by":"abl030","updated_at":"2026-02-11T12:21:01.824818522+08:00","closed_at":"2026-02-11T12:21:01.824818522+08:00","close_reason":"Deployed and verified on WSL. See docs/beads-rollout-plan.md.","dependencies":[{"issue_id":"nixosconfig-tij","depends_on_id":"nixosconfig-af3","type":"parent-child","created_at":"2026-02-11T12:20:52.393463559+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-tij","depends_on_id":"nixosconfig-vzl","type":"blocks","created_at":"2026-02-11T12:20:54.969577444+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-tw9","title":"Plexamp Linux audio broken since 4.12.4 — pinned to 4.12.3","description":"## Problem\nPlexamp 4.12.4+ on Linux has broken audio playback — seekbar advances 1 second then resets to 0, no sound output. Known upstream bug affecting all Linux distros (Debian, Fedora, Manjaro, NixOS, etc).\n\n## Root Cause\nUnknown. Plex dev (elan) acknowledged Jan 2026 saying \"a fair bit of stuff changed under the hood\" but no fix shipped. Affects both Flatpak and AppImage. PipeWire and PulseAudio both affected.\n\n## Upstream Tracking\n- Forum: https://forums.plex.tv/t/plexamp-flatpak-appimage-does-not-start-playback-until-audio-device-switched/929631\n- GitHub: https://github.com/flathub/com.plexamp.Plexamp/issues/270\n\n## Current Workaround\nPinned Plexamp to 4.12.3 via overlay in nix/overlay.nix. Remove the overlay when upstream ships a fix.\n\n## Known Workarounds (others)\n- Switch audio device in settings and switch back (every launch)\n- --disable-background-timer-throttling flag (changes init timing, masks the bug)\n- Downgrade to 4.12.3 (what we did)\n\n## Action Items\n- [ ] Monitor upstream forum thread for a fix\n- [ ] When fix ships, remove the pin overlay and test","status":"open","priority":2,"issue_type":"bug","owner":"abl030@gmail.com","created_at":"2026-02-23T17:08:08.143501433+08:00","created_by":"abl030","updated_at":"2026-02-23T17:08:08.143501433+08:00"}
{"id":"nixosconfig-usj","title":"Reference: VM Automation","description":"Reference documentation for VM automation workflow.\n\n## VM Definitions\n\n`vms/definitions.nix` contains:\n- **imported**: Pre-existing VMs (readonly=true) - documented but not managed by automation\n- **managed**: VMs provisioned and managed through automation\n- **template**: Base template (VMID 9002) for cloning\n\n## Proxmox Operations\n\n**CRITICAL**: Always use `vms/proxmox-ops.sh` wrapper script, NEVER run Proxmox commands directly via SSH.\n\nThe wrapper protects production VMs (104, 109, 110) from accidental modification by checking the readonly flag before ANY destructive operation.\n\n## Provisioning Workflow\n\n1. Define VM in `vms/definitions.nix` under `managed`\n2. Create host configuration in `hosts/{name}/`:\n   - `configuration.nix` (NixOS config)\n   - `disko.nix` (disk partitioning)\n   - `hardware-configuration.nix` (hardware detection)\n   - `home.nix` (Home Manager config)\n3. Add placeholder entry to `hosts.nix` with temporary publicKey (default temp password hash: `temp123`)\n4. Run: `nix run .#provision-vm \u003cvm-name\u003e`\n5. After provisioning, run: `nix run .#post-provision-vm \u003cvm-name\u003e \u003cIP\u003e \u003cVMID\u003e`\n6. Deploy with secrets: `nixos-rebuild switch --flake .#\u003cvm-name\u003e --target-host \u003cvm-name\u003e`\n\nPost-provision expects SOPS identity; it mirrors the `dc` lookup order (env vars, age key files, host key, user key).\n\n## VM Operations Commands\n\n```bash\n# List all VMs\n./vms/proxmox-ops.sh list\n\n# Get VM status\n./vms/proxmox-ops.sh status \u003cvmid\u003e\n\n# Start/stop VM (protected - checks readonly flag)\n./vms/proxmox-ops.sh start \u003cvmid\u003e\n./vms/proxmox-ops.sh stop \u003cvmid\u003e\n\n# Provision new VM\nnix run .#provision-vm \u003cvm-name\u003e\n\n# Post-provision (fleet integration)\nnix run .#post-provision-vm \u003cvm-name\u003e \u003cIP\u003e \u003cVMID\u003e\n\n# Get next available VMID\n./vms/proxmox-ops.sh next-vmid\n```","status":"closed","priority":4,"issue_type":"chore","created_at":"2026-02-11T20:35:10.539403722+08:00","updated_at":"2026-02-11T20:36:01.424805823+08:00","closed_at":"2026-02-11T20:36:01.424805823+08:00","close_reason":"Reference doc — content moved from CLAUDE.md","labels":["reference","vm"]}
{"id":"nixosconfig-v3y","title":"Build custom arr MCP server","description":"The existing mcp-arr-server npm package is low quality:\n- Missing @modelcontextprotocol/sdk from dependencies (packaging bug)\n- No 'add artist' tool — had to fall back to raw API calls\n- No 'monitor album/artist' tool\n- No 'add root folder' tool\n- Limited to read-only-ish operations\n\nBuild a custom MCP server for the *arr suite (starting with Lidarr) that covers the full workflow:\n- Search \u0026 add artists\n- List/monitor/unmonitor albums\n- Trigger searches\n- Manage root folders \u0026 quality profiles\n- View queue \u0026 history\n- Full CRUD on all Lidarr entities\n\nUse the same pattern as our other MCP wrappers. Can expand to Sonarr/Radarr later.\nParent epic: nixosconfig-z95 (music pipeline)","notes":"UNICODE HYPHEN BUG: Lidarr's release parser splits on ASCII hyphen (U+002D) but MusicBrainz stores artist names with non-breaking hyphen (U+2011). This means Lidarr can't match its OWN artists against NZB release names. Upstream bug, not fixable by us. Our MCP must: (1) normalize Unicode hyphens to ASCII in all comparisons, (2) match by MusicBrainz ID not parsed name when grabbing releases, (3) use ASCII names for directories. See docs/music-pipeline-postmortem.md for full context.","status":"open","priority":2,"issue_type":"feature","owner":"abl030@gmail.com","created_at":"2026-02-15T17:56:02.073238223+08:00","created_by":"abl030","updated_at":"2026-02-15T19:06:10.829247377+08:00"}
{"id":"nixosconfig-vwd","title":"Soularr Xavier matching diagnosis: peer availability + track count mismatch","description":"## Problem\nSoularr repeatedly fails to download MP3 320 of xaviersobased - Xavier despite correct matches existing on soulseek.\n\n## Root Causes Found (3 issues, all fixed)\n\n### Issue 1: No user fallback on download failure (FIXED)\nWhen downloads from the matched user all error out (e.g. claire419 — user is Away, transfers rejected), soularr gave up on the entire album. Never tried the next matching user.\n\n### Issue 2: Search cache filetype narrowing excludes valid users (FIXED)\nverify_filetype(\"mp3 320\") requires bitRate=320 in search metadata. Many soulseek clients don't report bitrate → files cached under generic \"mp3\" only. Combined with cutoff_unmet logic restricting to [\"mp3 320\"], most valid users were invisible.\n\nDiagnostic evidence: out of 16 search result users, only 3 had \"mp3 320\" in cache. The other 13 (potentially including fiqstro) were skipped because their search results lacked bitrate metadata.\n\n### Issue 3: minimum_match_ratio=0.8 too strict (OPEN)\nBonteKraai had 20 correct MP3 320 tracks but scored 0.5-0.73 due to filename format mismatch. The check_ratio truncation works for standard \"01. artist - title.mp3\" but not all naming conventions. Consider reducing to 0.6.\n\n## Changes Made\nFile: /tmp/soularr/soularr.py (volume-mounted into soularr container on doc1)\n\n1. skip_users parameter threading through try_enqueue → try_multi_enqueue → find_download\n2. monitor_downloads.delete_album() tries fallback before failing (max 3 users)\n3. try_enqueue/try_multi_enqueue merge file_dirs from specific AND generic base filetype\n4. Diagnostic logging in check_for_match and album_match (keep for verification)\n\n## Verification\n- \"Trying next user for Xavier (skipping: claire419)\" confirmed in Loki logs\n- Filetype broadening deployed, awaiting next run with more users in pool\n- match_ratio reduction still TODO","notes":"## Patch Plumbing\n\n### Current patch (cutoff_unmet quality skip + user fallback + filetype broadening)\n- Source: /tmp/soularr/soularr.py (git clone of mrusse/soularr + local edits)\n- Deployed via: scp to doc1:/mnt/docker/music/soularr/soularr.py\n- Volume mount in stacks/music/docker-compose.yml line 97:\n  ${DATA_ROOT}/music/soularr/soularr.py:/app/soularr.py:ro\n- Container restart picks up changes (no nix rebuild needed for py changes)\n\n### Deployment workflow\n1. Edit /tmp/soularr/soularr.py on WSL\n2. scp to doc1:/mnt/docker/music/soularr/soularr.py\n3. ssh doc1 \"podman restart soularr\" (or systemctl restart music.service)\n\n## Implemented Fixes (2026-02-17)\n\n### Fix 1: User fallback on download failure (DEPLOYED)\n- Added skip_users parameter to try_enqueue(), try_multi_enqueue(), find_download()\n- monitor_downloads.delete_album() now attempts fallback before failing:\n  1. Extracts failed username from download files\n  2. Adds to per-album skip_users set\n  3. Clears grab_list entry, calls find_download(album, grab_list, skip_users=skip_users)\n  4. If new user found: continues monitoring; if not: truly fails\n  5. Max 3 user attempts cap prevents infinite loops\n- Verified working: \"Trying next user for Xavier (skipping: claire419)\" observed in logs\n\n### Fix 2: Search cache filetype broadening (DEPLOYED)\n- ROOT CAUSE FOUND: verify_filetype(\"mp3 320\") requires bitRate=320 in search metadata\n  Many soulseek clients don't report bitrate → files cached under generic \"mp3\" only\n- For cutoff_unmet, filetypes_to_try is restricted to [\"mp3 320\"] → users with generic\n  \"mp3\" cache entries are NEVER checked, even though their files are actually 320kbps\n- Fix: try_enqueue and try_multi_enqueue now merge file_dirs from both specific\n  (\"mp3 320\") AND generic base (\"mp3\") before matching\n- This means users like fiqstro (whose search results lack bitrate metadata) are now\n  included in the matching pool\n\n### Fix 3: Diagnostic logging (DEPLOYED, keep for now)\n- check_for_match: logs track_num, dir_count, dir_filetype, dir_files per user\n- album_match: logs per-track failure with best_ratio and minimum_match_ratio\n- Revealed minimum_match_ratio is 0.8 — very strict for some naming conventions\n  (e.g. BonteKraai had 20 correct tracks but scored 0.5-0.73)\n\n## Open Issues\n\n### minimum_match_ratio too strict at 0.8\n- BonteKraai: 20 correct mp3 320 tracks, all scored 0.5-0.73 due to filename format\n  (folder: [2026-01-30] Xavier — unusual naming convention)\n- The check_ratio truncation works well for \"01. artist - title.mp3\" format but\n  not for all naming conventions\n- Consider reducing to 0.6 or 0.7","status":"open","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-17T08:52:25.817453175+08:00","created_by":"abl030","updated_at":"2026-02-17T09:32:53.004614592+08:00"}
{"id":"nixosconfig-vzl","title":"Package bd binary in NixOS overlay","description":"fetchurl pre-built binary from GitHub Releases into nix/overlay.nix. Temporary until upstream flake builds (Go \u003e= 1.25.6 blocker).","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-11T12:20:37.381492224+08:00","created_by":"abl030","updated_at":"2026-02-11T12:21:01.818838877+08:00","closed_at":"2026-02-11T12:21:01.818838877+08:00","close_reason":"Deployed and verified on WSL. See docs/beads-rollout-plan.md.","dependencies":[{"issue_id":"nixosconfig-vzl","depends_on_id":"nixosconfig-af3","type":"parent-child","created_at":"2026-02-11T12:20:52.32530434+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-vzl","depends_on_id":"nixosconfig-0yk","type":"blocks","created_at":"2026-02-11T12:20:54.872882245+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-z95","title":"Music Automation: Soulseek → Lidarr → Plex pipeline","description":"Enable Claude Code to fulfill requests like \"get me this album\" by searching Soulseek, downloading via slskd, tagging via Lidarr, and serving via Plex.\n\nArchitecture: Option C (Hybrid) — Soularr daemon for bulk/background, direct MCP for immediate requests.\nHost: doc1 (proxmox-vm). Music library: /mnt/data/music/ai.\nQuality default: 320kbps MP3 (overridable per request).\n\nFull plan originally in docs/music-automation-plan.md (moved here).","notes":"# Music Automation Plan: Soulseek → Lidarr → Plex Pipeline\n\n**Goal**: Enable Claude Code to fulfill requests like \"get me this album\" by searching Soulseek, downloading, tagging, and making available in Plex.\n\n**Date**: 2026-02-05\n**Status**: Infrastructure code complete, manual setup remaining\n\n---\n\n## Decisions Made\n\n| Question | Decision |\n|----------|----------|\n| **Host for slskd** | doc1 (main services VM) |\n| **Host for Lidarr** | doc1 (already deployed, unused) |\n| **Lidarr state** | Fresh start — nuke existing config |\n| **Music library path** | `/mnt/data/music/ai` (new subfolder for AI-sourced) |\n| **Soulseek credentials** | Existing account, store in sops |\n| **Secrets storage** | sops-encrypted (consistent with repo pattern) |\n| **Architecture** | Option C: Hybrid (Soularr daemon + direct MCP for immediate) |\n| **Default quality** | 320kbps MP3 (overridable per request) |\n| **Duplicate handling** | Skip and warn if exists in library |\n| **Plex scanning** | Auto-scan enabled (no action needed) |\n| **Notifications** | Gotify on download complete |\n| **slskd MCP approach** | Try existing SoulseekMCP, generate from OpenAPI if gaps |\n| **MCP server location** | Local (current pattern) |\n\n---\n\n## Proposed Architecture\n\n```\nUser: \"Get me Whiskeytown - Strangers Almanac\"\n                    │\n                    ▼\n            ┌───────────────┐\n            │  Claude Code  │\n            └───────┬───────┘\n                    │\n        ┌───────────┼───────────┐\n        ▼           ▼           ▼\n   SoulseekMCP   Lidarr MCP   Plex MCP\n        │           │           │\n        ▼           ▼           ▼\n      slskd      Lidarr       Plex\n        │           │           │\n        └─────┬─────┘           │\n              ▼                 │\n        Download Folder         │\n              │                 │\n              └────► Lidarr ────┘\n                   (import/tag)\n                        │\n                        ▼\n                  Music Library\n                        │\n                        ▼\n                      Plex\n```\n\n### Option A: Full MCP Control (Maximum Flexibility)\n\nClaude controls each step directly via MCP servers:\n1. Search Soulseek via SoulseekMCP\n2. Download via SoulseekMCP\n3. Trigger Lidarr import via Lidarr MCP\n4. Trigger Plex scan via Plex MCP\n\n**Pros**: Full control, can handle edge cases, immediate feedback\n**Cons**: More MCP servers to maintain, Claude orchestrates everything\n\n### Option B: Soularr Daemon (Most Hands-Off)\n\nClaude only adds to Lidarr's wanted list; Soularr daemon handles the rest:\n1. Add album to Lidarr wanted list via Lidarr MCP\n2. Soularr (daemon) monitors wanted list every 5 min\n3. Soularr searches slskd and triggers download\n4. Lidarr auto-imports from download folder\n5. Plex auto-scans library\n\n**Pros**: Simple, robust, handles retries/failures\n**Cons**: Not immediate (5 min polling), less visibility\n\n### Option C: Hybrid\n\nClaude can do both — use Soularr for bulk/background, direct MCP for immediate requests.\n\n---\n\n## Components Required\n\n### New Docker Services\n\n| Service | Image | Purpose | Port |\n|---------|-------|---------|------|\n| slskd | `slskd/slskd` | Soulseek client with REST API | 5030 (web), 5031 (API) |\n| soularr | `mrusse/soularr` | Lidarr ↔ slskd bridge | None (daemon) |\n\n### Existing Services to Configure\n\n| Service | Current State | Changes Needed |\n|---------|---------------|----------------|\n| Lidarr | Deployed on doc1, unused | Fresh config: root folder `/mnt/data/music/ai`, Soularr as download bridge |\n| Plex | Working, auto-scan enabled | Add `/mnt/data/music/ai` to music library |\n\n### MCP Servers to Add\n\n| MCP Server | Source | Purpose |\n|------------|--------|---------|\n| SoulseekMCP | `@jotraynor/SoulseekMCP` | Direct Soulseek search/download |\n| mcp-arr-server | `npm: mcp-arr-server` | Lidarr management |\n| plex-mcp-server | `npm: plex-mcp` | Plex library scan/search |\n\n---\n\n## Implementation Status\n\n### Completed (2026-02-05)\n\n| Item | File | Notes |\n|------|------|-------|\n| Docker compose for slskd | `stacks/music/docker-compose.yml` | Added slskd service with volumes, ports 5030/5031 |\n| Docker compose for soularr | `stacks/music/docker-compose.yml` | Daemon that bridges Lidarr wanted list → slskd |\n| Firewall ports | `stacks/music/docker-compose.nix` | Added 5030, 5031 to firewallPorts |\n| MCP wrapper: arr | `scripts/mcp-arr.sh` | Sources secrets, runs `npx -y mcp-arr-server` |\n| MCP wrapper: soulseek | `scripts/mcp-soulseek.sh` | Sources secrets, runs soulseek MCP |\n| MCP config | `.mcp.json` | Added `arr` and `soulseek` entries |\n| NixOS MCP module | `modules/nixos/services/mcp.nix` | Added arr/soulseek options with sops integration |\n| Secrets: arr-mcp.env | `secrets/arr-mcp.env` | sops-encrypted, has placeholder API key |\n| Secrets: soulseek-mcp.env | `secrets/soulseek-mcp.env` | sops-encrypted, has placeholder credentials |\n| Secrets: music.env | `secrets/music.env` | Updated with SOULSEEK_USERNAME/PASSWORD placeholders |\n| Soularr config template | `stacks/music/soularr/config.yaml` | Template config, needs Lidarr API key |\n| Quality gate | - | `check` passes |\n\n**Note**: The MCP module is defined but NOT enabled in any host config yet. Nothing will run until you explicitly enable it.\n\n### Remaining Manual Steps\n\n```bash\n# 1. Fill in your real Soulseek credentials\nsops secrets/soulseek-mcp.env\n# Change:\n#   SOULSEEK_USERNAME=your_actual_username\n#   SOULSEEK_PASSWORD=your_actual_password\n\nsops secrets/music.env\n# Change the SOULSEEK_USERNAME and SOULSEEK_PASSWORD lines\n\n# 2. Enable the MCP module in proxmox-vm host config\n#    Edit hosts/proxmox-vm/configuration.nix and add:\n#\n#    homelab.mcp = {\n#      enable = true;\n#      arr.enable = true;\n#      soulseek.enable = true;\n#    };\n\n# 3. Create the AI music directory on doc1\nssh proxmox-vm 'mkdir -p /mnt/data/music/ai'\n\n# 4. Deploy to doc1\nnixos-rebuild switch --flake .#proxmox-vm --target-host proxmox-vm\n\n# 5. Fresh Lidarr setup\n#    - Browse to http://doc1:8686\n#    - Complete initial setup wizard\n#    - Settings → Media Management → Root Folder: /mnt/data/music/ai\n#    - Settings → General → Copy the API Key\n\n# 6. Update arr secrets with Lidarr API key\nsops secrets/arr-mcp.env\n# Change LIDARR_API_KEY to the key from Lidarr\n\n# 7. Copy soularr config to doc1 and update it\nscp stacks/music/soularr/config.yaml proxmox-vm:/mnt/data/music/soularr/\nssh proxmox-vm 'nano /mnt/data/music/soularr/config.yaml'\n# Update the lidarr.api_key field with your Lidarr API key\n\n# 8. Restart the music stack on doc1\nssh proxmox-vm 'systemctl --user restart podman-compose@music'\n\n# 9. Verify services are running\nssh proxmox-vm 'podman ps | grep -E \"slskd|soularr|lidarr\"'\n\n# 10. Test slskd web UI\n#     Browse to http://doc1:5030\n#     Should show slskd interface, check Settings for Soulseek connection status\n\n# 11. Add Plex library (if not already done)\n#     In Plex, add /mnt/data/music/ai as a Music library source\n```\n\n### Testing the Pipeline\n\nOnce setup is complete:\n\n1. **Test Lidarr MCP**: Restart Claude Code, then ask \"search for artist Lucinda Williams in Lidarr\"\n2. **Test slskd connection**: Check http://doc1:5030 shows \"Connected\" to Soulseek\n3. **Test Soularr**: Add an album to Lidarr's wanted list, wait 5 min, check if Soularr triggers a search\n4. **End-to-end**: Ask Claude \"get me Whiskeytown Strangers Almanac\" and watch it flow through\n\n---\n\n## Open Items\n\n### Still To Determine\n\n- [x] Exact path for music library: `/mnt/data/music/ai`\n- [ ] Lidarr API key (will get after fresh config)\n- [ ] Plex token (for MCP server if needed — may not be required if auto-scan works)\n- [ ] Network: verify slskd can reach Soulseek network from doc1\n- [x] Lidarr naming format preference: use defaults\n\n### Notes\n\n- Lidarr wasn't actually flakey — just never properly configured\n- Music Assistant should automatically see new files via Plex (auto-scan enabled)\n- Quality is overridable per request, 320kbps default\n\n---\n\n## Implementation Phases\n\n### Phase 1: Infrastructure [CODE COMPLETE]\n- [x] Docker compose for slskd\n- [x] Docker compose for soularr\n- [x] Firewall ports configured\n- [ ] Deploy to doc1\n- [ ] Configure Soulseek credentials (fill in sops placeholders)\n- [ ] Test slskd web UI and API manually\n- [ ] Verify network connectivity (Soulseek servers reachable)\n\n### Phase 2: Lidarr Setup [PENDING]\n- [ ] Fresh Lidarr configuration (nuke existing)\n- [ ] Set root folder to `/mnt/data/music/ai`\n- [ ] Configure import settings (tagging, naming)\n- [ ] Get API key from Settings → General\n- [ ] Test manual import of a downloaded album\n\n### Phase 3: Bridge Setup [CODE COMPLETE]\n- [x] Soularr container added to docker-compose\n- [x] Soularr config template created\n- [ ] Copy config to doc1 and add Lidarr API key\n- [ ] Test: add album to Lidarr, verify Soularr picks it up\n\n### Phase 4: MCP Integration [CODE COMPLETE]\n- [x] Add mcp-arr-server to .mcp.json\n- [x] Add Soulseek MCP to .mcp.json\n- [x] MCP wrapper scripts created\n- [x] NixOS module extended for arr/soulseek secrets\n- [x] Sops secrets created (with placeholders)\n- [ ] Fill in real credentials\n- [ ] Test from Claude Code: search, download, import flow\n\n### Phase 5: Polish [PENDING]\n- [ ] Update CLAUDE.md with music automation workflow\n- [ ] Add Plex library path if not present\n- [ ] Optional: Gotify notifications on download complete\n- [ ] Optional: Quality filters in Soularr/slskd config\n\n---\n\n## Example MCP Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"arr\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-arr-server\"],\n      \"env\": {\n        \"LIDARR_URL\": \"http://lidarr.local:8686\",\n        \"LIDARR_API_KEY\": \"${LIDARR_API_KEY}\"\n      }\n    },\n    \"soulseek\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/SoulseekMCP/dist/index.js\"],\n      \"env\": {\n        \"SOULSEEK_USERNAME\": \"${SOULSEEK_USER}\",\n        \"SOULSEEK_PASSWORD\": \"${SOULSEEK_PASS}\",\n        \"DOWNLOAD_PATH\": \"/downloads/music\"\n      }\n    },\n    \"plex\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-plex\"],\n      \"env\": {\n        \"PLEX_URL\": \"http://plex.local:32400\",\n        \"PLEX_TOKEN\": \"${PLEX_TOKEN}\"\n      }\n    }\n  }\n}\n```\n\n---\n\n## Example Docker Compose Additions\n\n```yaml\nservices:\n  slskd:\n    image: slskd/slskd:latest\n    container_name: slskd\n    environment:\n      - SLSKD_REMOTE_CONFIGURATION=true\n      - SLSKD_SHARED_DIR=/music\n      - SLSKD_DOWNLOADS_DIR=/downloads\n    volumes:\n      - ./slskd/config:/app\n      - /path/to/music:/music:ro      # Share with Soulseek network\n      - /path/to/downloads:/downloads  # Download location\n    ports:\n      - \"5030:5030\"  # Web UI\n      - \"5031:5031\"  # API\n    restart: unless-stopped\n\n  soularr:\n    image: mrusse/soularr:latest\n    container_name: soularr\n    environment:\n      - ANTHROPIC_API_KEY=not_needed   # Only if using AI features\n    volumes:\n      - ./soularr/config:/config\n    depends_on:\n      - slskd\n    restart: unless-stopped\n```\n\n---\n\n## Risk Considerations\n\n1. **Soulseek availability**: Files may not be available, need graceful handling\n2. **Quality inconsistency**: Different uploaders = different quality/tagging\n3. **Legal considerations**: Soulseek operates in a gray area depending on jurisdiction\n4. **Network stability**: Soulseek peers can disconnect mid-download\n5. **Storage**: Music libraries grow; plan for capacity\n\n---\n\n## Next Steps\n\n1. **Fill in Soulseek credentials** in `secrets/soulseek-mcp.env` and `secrets/music.env`\n2. **Enable MCP module** in `hosts/proxmox-vm/configuration.nix` (see Remaining Manual Steps)\n3. **Deploy to doc1**: `nixos-rebuild switch --flake .#proxmox-vm --target-host proxmox-vm`\n4. **Fresh Lidarr setup** at http://doc1:8686, get API key\n5. **Update Lidarr API key** in `secrets/arr-mcp.env` and soularr config on doc1\n6. **Test the pipeline** end-to-end\n\n---\n\n## References\n\n- [slskd GitHub](https://github.com/slskd/slskd)\n- [slskd API Docs](https://github.com/slskd/slskd/blob/master/docs/api.md)\n- [Soularr](https://soularr.net)\n- [Soularr GitHub](https://github.com/mrusse/soularr)\n- [SoulseekMCP](https://glama.ai/mcp/servers/@jotraynor/SoulseekMCP)\n- [mcp-arr-server](https://www.npmjs.com/package/mcp-arr-server)\n- [Lidarr](https://lidarr.audio/)\n- [Lidarr API](https://lidarr.audio/docs/api/)","status":"in_progress","priority":2,"issue_type":"epic","owner":"abl030@gmail.com","created_at":"2026-02-15T14:18:47.806943555+08:00","created_by":"abl030","updated_at":"2026-02-15T17:10:32.354430256+08:00","labels":["homelab","infrastructure","music"]}
{"id":"nixosconfig-z95.1","title":"Review music plan, existing code, and infra state","description":"Before any deployment, review the full plan together:\n\n1. Audit existing code:\n   - stacks/music/docker-compose.yml (slskd + soularr services)\n   - stacks/music/docker-compose.nix (firewall ports, nix integration)\n   - stacks/music/soularr/config.yaml (template config)\n   - scripts/mcp-arr.sh, scripts/mcp-soulseek.sh (MCP wrappers)\n   - modules/nixos/services/mcp.nix (arr/soulseek options)\n   - .mcp.json entries for arr + soulseek\n\n2. Check infra state on doc1:\n   - Is Lidarr already running? What state is it in?\n   - Does /mnt/data/music/ exist? What's in it?\n   - Are slskd/soularr containers deployed or just defined?\n   - Network: can doc1 reach Soulseek network?\n\n3. Validate decisions still make sense:\n   - Option C (hybrid) still the right call?\n   - MCP server choices still current/maintained?\n   - Any new alternatives since Feb 5?\n\n4. Discuss and confirm the deployment order before proceeding.","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T14:19:00.416132178+08:00","created_by":"abl030","updated_at":"2026-02-15T17:17:14.791084718+08:00","closed_at":"2026-02-15T17:17:14.791084718+08:00","close_reason":"Review complete. Issues found: soularr image name wrong (mrusse08 not mrusse), SoulseekMCP wrapper broken (@anthropic doesn't exist), MEMORY.md conflict, root folder mismatch. Decisions: use /mnt/data/music/ai, hybrid MCP, user has soulseek creds.","labels":["music","review"],"dependencies":[{"issue_id":"nixosconfig-z95.1","depends_on_id":"nixosconfig-z95","type":"parent-child","created_at":"2026-02-15T14:19:00.419211604+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-z95.2","title":"Deploy slskd + soularr to doc1 and fill sops credentials","description":"Phase 1: Get the infrastructure running on doc1.\n\n- Fill real Soulseek credentials in secrets/soulseek-mcp.env and secrets/music.env\n- Enable homelab.mcp module in hosts/proxmox-vm/configuration.nix\n- Create /mnt/data/music/ai on doc1\n- Deploy: nixos-rebuild switch --flake .#proxmox-vm --target-host proxmox-vm\n- Verify slskd web UI at http://doc1:5030\n- Verify Soulseek network connectivity","status":"closed","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T14:19:03.783163622+08:00","created_by":"abl030","updated_at":"2026-02-15T17:33:10.143061868+08:00","closed_at":"2026-02-15T17:33:10.143061868+08:00","close_reason":"All services deployed on doc1: slskd (healthy), soularr (running, no wanted), lidarr (healthy). Root folder /mnt/data/music/ai created. Soularr config.ini with API key in place. Remaining: fill soulseek creds in sops, configure Lidarr root folder to new path, add to Plex.","labels":["deploy","music"],"dependencies":[{"issue_id":"nixosconfig-z95.2","depends_on_id":"nixosconfig-z95","type":"parent-child","created_at":"2026-02-15T14:19:03.788151772+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-z95.2","depends_on_id":"nixosconfig-z95.1","type":"blocks","created_at":"2026-02-15T14:19:16.259780458+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-z95.3","title":"Fresh Lidarr setup and soularr bridge configuration","description":"Phase 2+3: Configure Lidarr and wire up soularr.\n\n- Fresh Lidarr config at http://doc1:8686\n- Set root folder to /mnt/data/music/ai\n- Configure import settings (tagging, naming)\n- Grab API key from Settings → General\n- Update secrets/arr-mcp.env with Lidarr API key\n- Copy soularr config to doc1 with Lidarr API key\n- Test: add album to Lidarr wanted list, verify soularr picks it up","status":"open","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T14:19:06.873115948+08:00","created_by":"abl030","updated_at":"2026-02-15T14:19:06.873115948+08:00","labels":["lidarr","music"],"dependencies":[{"issue_id":"nixosconfig-z95.3","depends_on_id":"nixosconfig-z95","type":"parent-child","created_at":"2026-02-15T14:19:06.875172535+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-z95.3","depends_on_id":"nixosconfig-z95.2","type":"blocks","created_at":"2026-02-15T14:19:16.407295223+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-z95.4","title":"Test MCP integration and end-to-end pipeline","description":"Phase 4+5: MCP servers working and full pipeline validated.\n\n- Fill real credentials in sops secrets\n- Restart Claude Code to pick up MCP servers\n- Test Lidarr MCP: search for an artist\n- Test Soulseek MCP: search for an album\n- End-to-end: \"get me [album]\" → search → download → import → Plex\n- Add Plex library path for /mnt/data/music/ai if not present\n- Optional: Gotify notification on download complete\n- Update CLAUDE.md with music automation workflow","status":"open","priority":2,"issue_type":"task","owner":"abl030@gmail.com","created_at":"2026-02-15T14:19:10.09225553+08:00","created_by":"abl030","updated_at":"2026-02-15T14:19:10.09225553+08:00","labels":["mcp","music","testing"],"dependencies":[{"issue_id":"nixosconfig-z95.4","depends_on_id":"nixosconfig-z95","type":"parent-child","created_at":"2026-02-15T14:19:10.093667591+08:00","created_by":"abl030"},{"issue_id":"nixosconfig-z95.4","depends_on_id":"nixosconfig-z95.3","type":"blocks","created_at":"2026-02-15T14:19:16.494467896+08:00","created_by":"abl030"}]}
{"id":"nixosconfig-znn","title":"Migrate to official programs.claude-code HM module + add plugin support","description":"DISCOVERY: Official Home Manager module exists at nix-community/home-manager/modules/programs/claude-code.nix with proper options-based config BUT no plugin support.\n\nCURRENT STATE:\nWe maintain homelab.claudeCode custom module with:\n- Plugin installation/management\n- Node.js bundling and PATH setup\n- Writable cache directory for npm install\n- Native module symlink patching\n\nOFFICIAL MODULE PROVIDES:\n- programs.claude-code.settings (JSON schema validated)\n- programs.claude-code.{agents,commands,hooks,skills,rules}\n- programs.claude-code.mcpServers (MCP integration)\n- Proper option system with validation\n- Maintained by nix-community\n\nMIGRATION PLAN:\n1. Switch base from homelab.claudeCode to programs.claude-code\n2. Move settings/hooks/skills to official options\n3. Extend with programs.claude-code.plugins option for:\n   - Plugin source/version declaration\n   - Writable cache management\n   - Node.js dependency handling\n   - Native module compilation\n4. Keep plugin extension local OR upstream to home-manager\n\nDECISION POINT:\n- Local extension: Faster, keeps plugin complexity out of HM\n- Upstream PR: Proper solution, benefits community, slower\n\nREFERENCES:\n- Official module: https://github.com/nix-community/home-manager/blob/master/modules/programs/claude-code.nix\n- Maintainer: lib.maintainers.khaneliman\n- Related: sadjow/claude-code-nix, MachsteNix/claude-code-nix\n\nBLOCKS:\n- Full episodic-memory integration (nixosconfig-0sb)\n- Clean MCP server deployment (nixosconfig-4ts)","status":"open","priority":2,"issue_type":"feature","owner":"abl030@gmail.com","created_at":"2026-02-11T16:43:45.757186024+08:00","created_by":"abl030","updated_at":"2026-02-11T16:43:45.757186024+08:00"}
